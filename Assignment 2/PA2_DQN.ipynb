{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF5Jf6mAA1vU"
      },
      "source": [
        "# <center> CS6700: Reinforcement Learning\n",
        "# <center> Programming Assignment 2: DQN and AC\n",
        "## <center> Submitted By:\n",
        "## <center> Gautham Govind A: EE19B022\n",
        "## <center> Vishnu Vinod : CS19B048 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8YufnOhB3CY"
      },
      "source": [
        "This notebook focuses on the first set of tasks based on Deep Q-Networks (DQNs). The key objective is to train DQNs on 3 different classic control environments, benchamark their performance and use hyperparameter tuning to optimize their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCQeRpcQDRHu"
      },
      "source": [
        "We start with importing necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA2BRb6HDltW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "#from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "import tensorflow_probability as tfp\n",
        "from scipy.special import softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkdta_RrHVep"
      },
      "source": [
        "Since we are training a neural network, using a GPU can speed up the training process. So we use one if available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qV6lHMuHdDQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-coz0QYYEFlc"
      },
      "source": [
        "The two key components of the DQN approach are:\n",
        "- Q-Network\n",
        "- Replay memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ5TTuZeJABZ"
      },
      "source": [
        "### Q-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJyXxv56JDFi"
      },
      "source": [
        "The Q-Network learns the action values for each state. It takes a particular state as input and outputs the action values corresponding to each action. The hyperparameters associated with the Q-Network are:\n",
        "- Architecture: The neural network architecture itself can be thought of as a hyperaparameter.\n",
        "- Learning rate: The learning rate of the network is also a hyperparameter.\n",
        "\n",
        "Note that since the network architecture itself may change, we need to write separate class definitions for each architecture. For now, the architecture given in the tutorial is used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx_XNYecLG4I"
      },
      "outputs": [],
      "source": [
        "class QNetwork1(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork1, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    # Forward pass of the network\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roLy8Mab5Tt2"
      },
      "outputs": [],
      "source": [
        "class QNetwork2(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork2, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.fc4 = nn.Linear(64, action_size)\n",
        "\n",
        "    # Forward pass of the network\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return self.fc4(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPU2890t0VsV"
      },
      "outputs": [],
      "source": [
        "class QNetwork3(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork3, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 128)\n",
        "        self.fc4 = nn.Linear(128, action_size)\n",
        "\n",
        "    # Forward pass of the network\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        return self.fc4(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEOh4njcFM54"
      },
      "source": [
        "### Replay Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7sOBcAvGRV9"
      },
      "source": [
        "The replay memory stores past experiences and samples them instead of using new experiences directly so as to break correlation. We have two hyperparameters associated with the replay memory:\n",
        "- Buffer size: This represents the maximum number of past experiences that are stored in the replay buffer.\n",
        "- Batch size: This represents the number of experiences that are sampled from the buffer for each training iteration.\n",
        "\n",
        "Note that since we do not have any \"architecture\" as such as we had in the case of Q-Network, a single class definition for replay memory will suffice; hyperparameters can be changed by initializing the class with the required values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHWfPrgtG3Sz"
      },
      "outputs": [],
      "source": [
        "# Replay memory to store past experiences\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \n",
        "        '''Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        '''\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    # Add a new experience to memory\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    # Sampling a batch of past experiences from memory\n",
        "    def sample(self):\n",
        "        \n",
        "        experiences = random.sample(self.memory, k = self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "  \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    # Return the current size of internal memory\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvg1ETNfIh1C"
      },
      "source": [
        "We next write a class which brings together Q-Network and Replay memory and allows for the training of the agent. We also now have the hyperparameters:\n",
        "- Update frequency of target network\n",
        "- Gradient truncation limit\n",
        "- Discount factor\n",
        "- Control Parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnbWcWq0OV2-"
      },
      "source": [
        "We now define all the hyperparamters at one location for easier accesibility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBXxnF_XOoIz"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOntYAb6N91I"
      },
      "outputs": [],
      "source": [
        "class DQNAgent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, q_network, seed):\n",
        "\n",
        "        # Environment-related parameters\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Defining the local network and the target network as well as the optimizer\n",
        "        self.qnetwork_local = q_network.to(device)\n",
        "        self.qnetwork_target = q_network.to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        # Defining the replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "        # Keeping track of timesteps as this is needed for tracking update frequency of target network\n",
        "        self.t_step = 0\n",
        "    \n",
        "    # Method for an agent' step\n",
        "    # The agent first stores the new experience in replay memory\n",
        "    # Then replay memory is sampled and the agent learns\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        # Save experience in replay memory \n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        # If enough samples are available in memory, get random subset and learn '''\n",
        "        if len(self.memory) >= BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "        \n",
        "        # Updating the taget network after every UPDATE_FREQ steps    \n",
        "        self.t_step = (self.t_step + 1) % UPDATE_FREQ\n",
        "        if self.t_step == 0:\n",
        "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    # Choosing an action given a state\n",
        "    # Epsilon-greedy policy is adopted\n",
        "    def act(self, state, eps=0.):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection \n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    # Learning of the agent\n",
        "    def learn(self, experiences, gamma):\n",
        "\n",
        "        # Sampled experiences\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        # Q-value predcition for the next state from the target network\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # Bootstrapped Q-value prediction for the current state from the target network\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        # Get expected Q values from local model \n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        # Compute loss \n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        # Optimizie\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradiant Clipping \n",
        "        for param in self.qnetwork_local.parameters():\n",
        "            param.grad.data.clamp_(-GRAD_CLIP, GRAD_CLIP)\n",
        "            \n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hPbCF3qSkPC"
      },
      "source": [
        "Next, we define a DQNSolve() class which takes in the environment and solves it using the DQN agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQJhfdW7U4dF"
      },
      "outputs": [],
      "source": [
        "class DQNSolve():\n",
        "\n",
        "  def __init__(self, env, nn_arch, seed = 1):\n",
        "\n",
        "    self.state_shape = env.observation_space.shape[0]\n",
        "    self.action_shape = env.action_space.n\n",
        "    self.env = env\n",
        "\n",
        "    if nn_arch == \"q1\":\n",
        "      self.q_network = QNetwork1(self.state_shape, self.action_shape, seed)\n",
        "\n",
        "    if nn_arch == \"q2\":\n",
        "      self.q_network = QNetwork2(self.state_shape, self.action_shape, seed)\n",
        "\n",
        "    if nn_arch == \"q3\":\n",
        "      self.q_network = QNetwork3(self.state_shape, self.action_shape, seed)\n",
        "\n",
        "    self.agent = DQNAgent(self.state_shape, self.action_shape, self.q_network, seed)\n",
        "\n",
        "  \n",
        "  def solve(self, n_episodes=10000, max_t = 500, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "    scores = []                 \n",
        "    ''' list containing scores from each episode '''\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    steps = []\n",
        "    \n",
        "    scores_window = deque(maxlen=100) \n",
        "\n",
        "    eps = eps_start                    \n",
        "    ''' initialize epsilon '''\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "\n",
        "        state = self.env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = self.agent.act(state, eps)\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "            self.agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break \n",
        "      \n",
        "        \n",
        "        scores_window.append(score)   \n",
        "        rewards.append(score)\n",
        "        steps.append(t)\n",
        "        ''' save most recent score '''           \n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps) \n",
        "        ''' decrease epsilon '''\n",
        "\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score (prev 100): {:.2f}\\tSteps:{}'.format(i_episode, np.mean(scores_window), t), end=\"\")         \n",
        "        \n",
        "        if ((np.mean(scores_window)>=195.0) and (i_episode > 100) ):\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "\n",
        "    return [i_episode, rewards, steps]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Xn-NP7l2GY"
      },
      "source": [
        "We write a function for solving the environment ten times and recording number of episodes and average rewards:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sg1izxt9csE"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QehsuegimvG1"
      },
      "outputs": [],
      "source": [
        "def analyze_variant(nn_arch, var_count):\n",
        "\n",
        "  print(\"Running variant with hyperparameters: LR: {}, BUFFER_SIZE: {}, BATCH_SIZE: {}, UPDATE_FREQ: {}, GAMMA: {}, GRAD_CLIP: {}\".format(LR, BUFFER_SIZE, BATCH_SIZE, UPDATE_FREQ, GAMMA, GRAD_CLIP))\n",
        "  print(\"Using network architecture: \", nn_arch)\n",
        "  run_episode_counts = []\n",
        "  run_rewards = []\n",
        "  run_steps = []\n",
        "\n",
        "  # Averaging over 10 runs\n",
        "  for i in range(10):\n",
        "    print(\"Performing Run \", i+1)\n",
        "    dq_solver = DQNSolve(env, nn_arch, seed = i)\n",
        "    epi_count, rewards, steps = dq_solver.solve()\n",
        "    run_episode_counts.append(epi_count)\n",
        "    run_rewards.append(rewards)\n",
        "    run_steps.append(steps)\n",
        "\n",
        "  avg_num_episodes = np.mean(run_episode_counts)\n",
        "  avg_rewards = np.ones((10, np.max(run_episode_counts)))\n",
        "  avg_steps = np.ones((10, np.max(run_episode_counts)))\n",
        "  for i in  range(10):\n",
        "    avg_rewards[i, :run_episode_counts[i]] = run_rewards[i]\n",
        "    avg_rewards[i, run_episode_counts[i]:] = run_rewards[i][-1]\n",
        "    avg_steps[i, :run_episode_counts[i]] = run_steps[i]\n",
        "    avg_steps[i, run_episode_counts[i]:] = run_steps[i][-1]\n",
        "\n",
        "  avg_rewards =  np.mean(avg_rewards, axis = 0)\n",
        "  avg_steps = np.mean(avg_steps, axis = 0)\n",
        "\n",
        "  print(\"Average number of episodes taken to solve the environment:\", avg_num_episodes)\n",
        "\n",
        "  # Plotting average rewards\n",
        "  plt.plot(list(range(avg_rewards.shape[0])), avg_rewards)\n",
        "  plt.xlabel(\"Episode count\")\n",
        "  plt.ylabel(\"Total reward \")\n",
        "  plt.title(\"Total reward per episode (averaged over 10 runs)\")\n",
        "  plt.savefig(\"cart_variant_{}_reward_plot.png\".format(var_count))\n",
        "  plt.show()\n",
        "  files.download(\"cart_variant_{}_reward_plot.png\".format(var_count)) \n",
        "  \n",
        "\n",
        "  # Plotting average steps\n",
        "  plt.plot(list(range(avg_steps.shape[0])), avg_steps)\n",
        "  plt.xlabel(\"Episode count\")\n",
        "  plt.ylabel(\"Steps \")\n",
        "  plt.title(\"Steps per episode (averaged over 10 runs)\")\n",
        "  plt.savefig(\"cart_variant_{}_step_plot.png\".format(var_count))\n",
        "  plt.show()\n",
        "  files.download(\"cart_variant_{}_step_plot.png\".format(var_count)) \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmuzjEuJm-Jm"
      },
      "source": [
        "## CartPole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qJWN2ND1FsC"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFG5aL-t0_il"
      },
      "source": [
        "### First trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jvq4OgL1CLt"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_wqZwQGnRM3"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9T_DeL6nzah"
      },
      "source": [
        "### Second Trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngTauPFjRwFi"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSu4s7-If0_E"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q2\", 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third Trial"
      ],
      "metadata": {
        "id": "ZCHE_GhcAU3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.9            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ],
      "metadata": {
        "id": "D5-WxqJq55Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_variant(\"q1\", 3)"
      ],
      "metadata": {
        "id": "ZmNJvE-s57ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fourth Trial"
      ],
      "metadata": {
        "id": "oh_ujAIPAfB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG5AjtUv9v-P"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.9            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWtuvE039yzp"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fifth Trial"
      ],
      "metadata": {
        "id": "I5zWMZqpA26Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v23fJaFQ95kT"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128         # Training batch size\n",
        "UPDATE_FREQ = 40        # Update frequency of target network \n",
        "GAMMA = 0.9            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ2sMfns9_8v"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sixth Trial"
      ],
      "metadata": {
        "id": "MpB_-CW1A8Lx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mRMpHFR-Cbh"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4              # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.9            # Discount factor\n",
        "GRAD_CLIP = 100.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xKU0y6u-FTf"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q2\", 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seventh Trial"
      ],
      "metadata": {
        "id": "LxcUGkzgBHoB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBOaFaqg-IpQ"
      },
      "outputs": [],
      "source": [
        "LR = 7.5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128        # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.9            # Discount factor\n",
        "GRAD_CLIP = 100.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v9O9XYI-Tj1"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q2\", 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eighth Trial"
      ],
      "metadata": {
        "id": "1rY9vehUBNjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPEZUwqe-U2h"
      },
      "outputs": [],
      "source": [
        "LR = 7.5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 256         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.9            # Discount factor\n",
        "GRAD_CLIP = 100.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_variant(\"q1\", 7)"
      ],
      "metadata": {
        "id": "Mxv-2gW2aLK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOrjs7Gv0sCz"
      },
      "source": [
        "## Acrobot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7gyrGBLqwp"
      },
      "source": [
        "We define DQNSolve() with the conditions for the acrobot environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdTbW4ZxLtqS"
      },
      "outputs": [],
      "source": [
        "class DQNSolve():\n",
        "\n",
        "  def __init__(self, env, nn_arch, seed = 1):\n",
        "\n",
        "    self.state_shape = env.observation_space.shape[0]\n",
        "    self.action_shape = env.action_space.n\n",
        "    self.env = env\n",
        "\n",
        "    if nn_arch == \"q1\":\n",
        "      self.q_network = QNetwork1(self.state_shape, self.action_shape, seed)\n",
        "\n",
        "    if nn_arch == \"q2\":\n",
        "      self.q_network = QNetwork2(self.state_shape, self.action_shape, seed)\n",
        "\n",
        "    if nn_arch == \"q3\":\n",
        "      self.q_network = QNetwork3(self.state_shape, self.action_shape, seed)\n",
        "\n",
        "    self.agent = DQNAgent(self.state_shape, self.action_shape, self.q_network, seed)\n",
        "\n",
        "  \n",
        "  def solve(self, n_episodes=10000, max_t = 500, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "    scores = []                 \n",
        "    ''' list containing scores from each episode '''\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    steps = []\n",
        "    \n",
        "    scores_window = deque(maxlen=100) \n",
        "\n",
        "    eps = eps_start                    \n",
        "    ''' initialize epsilon '''\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "\n",
        "        state = self.env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = self.agent.act(state, eps)\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "            self.agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break \n",
        "      \n",
        "        \n",
        "        scores_window.append(score)   \n",
        "        rewards.append(score)\n",
        "        steps.append(t+1)\n",
        "        ''' save most recent score '''           \n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps) \n",
        "        ''' decrease epsilon '''\n",
        "\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score (prev 100): {:.2f}\\tSteps:{}'.format(i_episode, np.mean(scores_window), t+1), end=\"\")         \n",
        "        \n",
        "        if ((np.mean(scores_window)>=-100.0) and (i_episode > 100) ):\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "\n",
        "    return [i_episode, rewards, steps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGSIEEjmM5G7"
      },
      "outputs": [],
      "source": [
        "def analyze_variant(nn_arch, var_count):\n",
        "\n",
        "  print(\"Running variant with hyperparameters: LR: {}, BUFFER_SIZE: {}, BATCH_SIZE: {}, UPDATE_FREQ: {}, GAMMA: {}, GRAD_CLIP: {}\".format(LR, BUFFER_SIZE, BATCH_SIZE, UPDATE_FREQ, GAMMA, GRAD_CLIP))\n",
        "  print(\"Using network architecture: \", nn_arch)\n",
        "  run_episode_counts = []\n",
        "  run_rewards = []\n",
        "  run_steps = []\n",
        "\n",
        "  # Averaging over 10 runs\n",
        "  for i in range(10):\n",
        "    print(\"Performing Run \", i+1)\n",
        "    dq_solver = DQNSolve(env, nn_arch, seed = i)\n",
        "    epi_count, rewards, steps = dq_solver.solve()\n",
        "    run_episode_counts.append(epi_count)\n",
        "    run_rewards.append(rewards)\n",
        "    run_steps.append(steps)\n",
        "\n",
        "  avg_num_episodes = np.mean(run_episode_counts)\n",
        "  avg_rewards = np.ones((10, np.max(run_episode_counts)))\n",
        "  avg_steps = np.ones((10, np.max(run_episode_counts)))\n",
        "  for i in  range(10):\n",
        "    avg_rewards[i, :run_episode_counts[i]] = run_rewards[i]\n",
        "    avg_rewards[i, run_episode_counts[i]:] = run_rewards[i][-1]\n",
        "    avg_steps[i, :run_episode_counts[i]] = run_steps[i]\n",
        "    avg_steps[i, run_episode_counts[i]:] = run_steps[i][-1]\n",
        "\n",
        "  avg_rewards =  np.mean(avg_rewards, axis = 0)\n",
        "  avg_steps = np.mean(avg_steps, axis = 0)\n",
        "\n",
        "  print(\"Average number of episodes taken to solve the environment:\", avg_num_episodes)\n",
        "\n",
        "  # Plotting average rewards\n",
        "  plt.plot(list(range(avg_rewards.shape[0])), avg_rewards)\n",
        "  plt.xlabel(\"Episode count\")\n",
        "  plt.ylabel(\"Total reward \")\n",
        "  plt.title(\"Total reward per episode (averaged over 10 runs)\")\n",
        "  plt.savefig(\"acro_variant_{}_reward_plot.png\".format(var_count))\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "  # Plotting average steps\n",
        "  plt.plot(list(range(avg_steps.shape[0])), avg_steps)\n",
        "  plt.xlabel(\"Episode count\")\n",
        "  plt.ylabel(\"Steps \")\n",
        "  plt.title(\"Steps per episode (averaged over 10 runs)\")\n",
        "  plt.savefig(\"acro_variant_{}_step_plot.png\".format(var_count))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chfHV-SqNQuF"
      },
      "outputs": [],
      "source": [
        "env = gym.make('Acrobot-v1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPX4yBKqNOv0"
      },
      "source": [
        "### First trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7ncAgbRNUiE"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIOxlqejNiNn"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov8mQK5fNmSv"
      },
      "source": [
        "### Second Trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo1xKG-KdjCt"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 40        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKDZhMvodmVl"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q2\", 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix-h_XOrdoei"
      },
      "source": [
        "### Third trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKAj4gosipiU"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 40        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nih9NHLiwFh"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY53XuguizXB"
      },
      "source": [
        "### Fourth trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbmUd5N-vjlP"
      },
      "outputs": [],
      "source": [
        "LR = 10e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 40        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqBng8T3vpvU"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q2\", 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp0ua8-9vyRW"
      },
      "source": [
        "### Fifth trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HefDonuBMfs"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128         # Training batch size\n",
        "UPDATE_FREQ = 40        # Update frequency of target network \n",
        "GAMMA = 0.999            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIYN4fPnBkWw"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0lG7GcDBm3i"
      },
      "source": [
        "### Sixth trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbZSBzjAX-tx"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128         # Training batch size\n",
        "UPDATE_FREQ = 40        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BqGUMTRYKbx"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUROqOuqYMjB"
      },
      "source": [
        "### Seventh trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmrYAx5ivseU"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128         # Training batch size\n",
        "UPDATE_FREQ = 40        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 100.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKzxsNfVvnO9"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znsLBSzOvulz"
      },
      "source": [
        "### Eighth trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzaEWb9C2gPb"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 256         # Training batch size\n",
        "UPDATE_FREQ = 40        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 100.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meY6Z7ZTgp1C"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_2nrbp6gsIw"
      },
      "source": [
        "## Mountain Car"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGAuoz7x4bGX"
      },
      "outputs": [],
      "source": [
        "class DQNSolve():\n",
        "\n",
        "  def __init__(self, env, nn_arch, seed = 1):\n",
        "\n",
        "    self.state_shape = env.observation_space.shape[0]\n",
        "    self.action_shape = env.action_space.n\n",
        "    self.env = env\n",
        "\n",
        "    if nn_arch == \"q1\":\n",
        "      self.q_network = QNetwork1(self.state_shape, self.action_shape, seed)\n",
        "\n",
        "    if nn_arch == \"q2\":\n",
        "      self.q_network = QNetwork2(self.state_shape, self.action_shape, seed)\n",
        "\n",
        "    if nn_arch == \"q3\":\n",
        "      self.q_network = QNetwork3(self.state_shape, self.action_shape, seed)\n",
        "\n",
        "    self.agent = DQNAgent(self.state_shape, self.action_shape, self.q_network, seed)\n",
        "\n",
        "  \n",
        "  def solve(self, n_episodes=10000, max_t = 500, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "    scores = []                 \n",
        "    ''' list containing scores from each episode '''\n",
        "\n",
        "    rewards = []\n",
        "\n",
        "    steps = []\n",
        "    \n",
        "    scores_window = deque(maxlen=100) \n",
        "\n",
        "    eps = eps_start                    \n",
        "    ''' initialize epsilon '''\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "\n",
        "        state = self.env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = self.agent.act(state, eps)\n",
        "            next_state, reward, done, _ = self.env.step(action)\n",
        "            self.agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break \n",
        "      \n",
        "        \n",
        "        scores_window.append(score)   \n",
        "        rewards.append(score)\n",
        "        steps.append(t+1)\n",
        "        ''' save most recent score '''           \n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps) \n",
        "        ''' decrease epsilon '''\n",
        "\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score (prev 100): {:.2f}\\tSteps:{}'.format(i_episode, np.mean(scores_window), t+1), end=\"\")         \n",
        "        \n",
        "        if ((done) and (t < 199) ):\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "\n",
        "    return [i_episode, rewards, steps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO55PS7r440-"
      },
      "outputs": [],
      "source": [
        "def analyze_variant(nn_arch, var_count):\n",
        "\n",
        "  print(\"Running variant with hyperparameters: LR: {}, BUFFER_SIZE: {}, BATCH_SIZE: {}, UPDATE_FREQ: {}, GAMMA: {}, GRAD_CLIP: {}\".format(LR, BUFFER_SIZE, BATCH_SIZE, UPDATE_FREQ, GAMMA, GRAD_CLIP))\n",
        "  print(\"Using network architecture: \", nn_arch)\n",
        "  run_episode_counts = []\n",
        "  run_rewards = []\n",
        "  run_steps = []\n",
        "\n",
        "  # Averaging over 10 runs\n",
        "  for i in range(10):\n",
        "    print(\"Performing Run \", i+1)\n",
        "    dq_solver = DQNSolve(env, nn_arch, seed = i)\n",
        "    epi_count, rewards, steps = dq_solver.solve()\n",
        "    run_episode_counts.append(epi_count)\n",
        "    run_rewards.append(rewards)\n",
        "    run_steps.append(steps)\n",
        "\n",
        "  avg_num_episodes = np.mean(run_episode_counts)\n",
        "  avg_rewards = np.ones((10, np.max(run_episode_counts)))\n",
        "  avg_steps = np.ones((10, np.max(run_episode_counts)))\n",
        "  for i in  range(10):\n",
        "    avg_rewards[i, :run_episode_counts[i]] = run_rewards[i]\n",
        "    avg_rewards[i, run_episode_counts[i]:] = run_rewards[i][-1]\n",
        "    avg_steps[i, :run_episode_counts[i]] = run_steps[i]\n",
        "    avg_steps[i, run_episode_counts[i]:] = run_steps[i][-1]\n",
        "\n",
        "  avg_rewards =  np.mean(avg_rewards, axis = 0)\n",
        "  avg_steps = np.mean(avg_steps, axis = 0)\n",
        "\n",
        "  print(\"Average number of episodes taken to solve the environment:\", avg_num_episodes)\n",
        "\n",
        "  # Plotting average rewards\n",
        "  plt.plot(list(range(avg_rewards.shape[0])), avg_rewards)\n",
        "  plt.xlabel(\"Episode count\")\n",
        "  plt.ylabel(\"Total reward \")\n",
        "  plt.title(\"Total reward per episode (averaged over 10 runs)\")\n",
        "  plt.savefig(\"mount_variant_{}_reward_plot.png\".format(var_count))\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "  # Plotting average steps\n",
        "  plt.plot(list(range(avg_steps.shape[0])), avg_steps)\n",
        "  plt.xlabel(\"Episode count\")\n",
        "  plt.ylabel(\"Steps \")\n",
        "  plt.title(\"Steps per episode (averaged over 10 runs)\")\n",
        "  plt.savefig(\"mount_variant_{}_step_plot.png\".format(var_count))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lG63tx-5J01"
      },
      "outputs": [],
      "source": [
        "env = gym.make('MountainCar-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCWmIoAZ4_oz"
      },
      "source": [
        "### First trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kupfMuRq5EwV"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_497iN7k5SIW"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q1\", 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzx0AYSr5V7l"
      },
      "source": [
        "### Second trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzYbONxZ-dKY"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJPBgAmK-ffw"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q2\", 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA9XXLy8-iR2"
      },
      "source": [
        "### Third trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SSli6uh-kFB"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVD4Gg8t-l7V"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q3\", 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXfj6tiC-oUA"
      },
      "source": [
        "### Fourth trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxJpD4YKrGDP"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKb0xxT4rJeo"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q3\", 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2Ug-Cae6rzJ"
      },
      "source": [
        "### Fifth trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUwgRhtKrL9b"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128         # Training batch size\n",
        "UPDATE_FREQ = 40        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 1.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cloVZBR36xJJ"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q3\", 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIorQU8a6zBP"
      },
      "source": [
        "### Sixth trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNiJ0-0qSaEj"
      },
      "outputs": [],
      "source": [
        "LR = 5e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 128         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.99            # Discount factor\n",
        "GRAD_CLIP = 100.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tafb9ApBSeN7"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q3\", 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxdw0cEm0-x0"
      },
      "source": [
        "### Seventh trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6V1HZzkSf9V"
      },
      "outputs": [],
      "source": [
        "LR = 10e-4               # Learning rate\n",
        "BUFFER_SIZE = int(1e5)  # Replay buffer size \n",
        "BATCH_SIZE = 64         # Training batch size\n",
        "UPDATE_FREQ = 20        # Update frequency of target network \n",
        "GAMMA = 0.9            # Discount factor\n",
        "GRAD_CLIP = 100.0         # The gradient is clipped between (-GRAD_CLIP and GRAD_CLIP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOsuUQkW1GJ6"
      },
      "outputs": [],
      "source": [
        "analyze_variant(\"q3\", 7)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}