{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fdecfd1",
   "metadata": {},
   "source": [
    "# <center >CS6700: Reinforcement Learning\n",
    "## <center >Programming Assignment 2\n",
    "## <center> DQN \\& AC: AC Acrobot\n",
    "###  Submitted by: \n",
    "### Gautham Govind A: EE19B022\n",
    "### Vishnu Vinod: CS19B048 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c862e549",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfa317",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Installing packages for rendering the game on Colab\n",
    "'''\n",
    "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "# !apt-get update > /dev/null 2>&1\n",
    "# !apt-get install cmake > /dev/null 2>&1\n",
    "# !pip install --upgrade setuptools 2>&1\n",
    "# !pip install ez_setup > /dev/null 2>&1\n",
    "# !pip install gym[atari] > /dev/null 2>&1\n",
    "# !pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
    "# !pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "import tensorflow as tf\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "import tensorflow_probability as tfp\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c05bf",
   "metadata": {},
   "source": [
    "## Acrobot-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36576fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Acrobot-v1')\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "\n",
    "print(state_shape, no_of_actions)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a25f7",
   "metadata": {},
   "source": [
    "## AC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ff95d",
   "metadata": {},
   "source": [
    "**Actor-Critic methods** learn both a policy $\\pi(a|s;\\theta)$ and a state-value function $v(s;w)$ simultaneously. The policy is referred to as the actor that suggests actions given a state. The estimated value function is referred to as the critic. It evaluates actions taken by the actor based on the given policy. In this exercise, both functions are approximated by feedforward neural networks. \n",
    "\n",
    "- The policy network is parametrized by $\\theta$ - it takes a state $s$ as input and outputs the probabilities $\\pi(a|s;\\theta)\\ \\forall\\ a$\n",
    "- The value network is parametrized by $w$ - it takes a state $s$ as input and outputs a scalar value associated with the state, i.e., $v(s;w)$\n",
    "\n",
    "- **NOTE: Here, weights of the first two hidden layers are shared by the policy and the value network**\n",
    "    - Default hidden layer sizes: [1024, 512]\n",
    "    - Output size of policy network: 2 (Softmax activation)\n",
    "    - Output size of value network: 1 (Linear activation)\n",
    "\n",
    "$$\\pi(a|s;\\theta) = \\phi_{\\theta}(a,s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Defining policy and value networkss\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, n_hidden1=1024, n_hidden2=512):\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "\n",
    "        #Hidden Layer 1\n",
    "        self.fc1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
    "        #Hidden Layer 2\n",
    "        self.fc2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
    "        \n",
    "        #Output Layer for policy\n",
    "        self.pi_out = tf.keras.layers.Dense(action_size, activation='softmax')\n",
    "        #Output Layer for state-value\n",
    "        self.v_out = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        \"\"\"\n",
    "        Computes policy distribution and state-value for a given state\n",
    "        \"\"\"\n",
    "        layer1 = self.fc1(state)\n",
    "        layer2 = self.fc2(layer1)\n",
    "\n",
    "        pi = self.pi_out(layer2)       # policy network outputs\n",
    "        v = self.v_out(layer2)         # value network outputs\n",
    "\n",
    "        return pi, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14116632",
   "metadata": {},
   "source": [
    "## Agent 1: One-Step AC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3bead",
   "metadata": {},
   "source": [
    "- The single step TD error can be defined as follows:\n",
    "$$\\delta_t  = R_{t+1} + \\gamma v(s_{t+1};w) - v(s_t;w)$$\n",
    "\n",
    "- The loss function to be minimized at every step ($L_{tot}^{(t)}$) is a summation of two terms, as follows:\n",
    "$$L_{tot}^{(t)} = L_{actor}^{(t)} + L_{critic}^{(t)}$$\n",
    "where,\n",
    "$$L_{actor}^{(t)} = -\\log\\pi(a_t|s_t; \\theta)\\delta_t$$\n",
    "$$L_{critic}^{(t)} = \\delta_t^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d08298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent1:\n",
    "    \"\"\"\n",
    "    Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, lr=1e-4, gamma=0.99, seed = 85, h1 = 1024, h2 = 512):\n",
    "        self.gamma = gamma\n",
    "        self.ac_model = ActorCriticModel(action_size, h1, h2)\n",
    "        self.ac_model.compile(tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Given a state, compute the policy distribution over all actions and sample one action\n",
    "    def sample_action(self, state):\n",
    "        pi,_ = self.ac_model(state)\n",
    "        action_probabilities = tfp.distributions.Categorical(probs=pi)\n",
    "        sample = action_probabilities.sample()\n",
    "        return int(sample.numpy()[0])\n",
    "\n",
    "    # actor loss\n",
    "    def actor_loss(self, action, pi, delta):\n",
    "        return -tf.math.log(pi[0,action]) * delta\n",
    "\n",
    "    # critic loss\n",
    "    def critic_loss(self,delta):\n",
    "        return delta**2\n",
    "\n",
    "    @tf.function\n",
    "    # For a given transition (s,a,s',r) update the paramters\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            pi, V_s = self.ac_model(state)\n",
    "            _, V_s_next = self.ac_model(next_state)\n",
    "\n",
    "            V_s = tf.squeeze(V_s)\n",
    "            V_s_next = tf.squeeze(V_s_next)\n",
    "            \n",
    "            # Equation for TD error\n",
    "            delta = reward + self.gamma*V_s_next - V_s\n",
    "            \n",
    "            \n",
    "            loss_a = self.actor_loss(action, pi, delta)\n",
    "            loss_c =self.critic_loss(delta)\n",
    "            loss_total = loss_a + loss_c\n",
    "\n",
    "        gradient = tape.gradient(loss_total, self.ac_model.trainable_variables)\n",
    "        self.ac_model.optimizer.apply_gradients(zip(gradient, self.ac_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepAC:\n",
    "    def __init__(self, env_name, exp_name, eps = 2500, runs = 5, lr = 1e-4, h1 = 1024, h2 = 512):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state = self.env.reset()[0]\n",
    "        self.thresh = self.env.spec.reward_threshold\n",
    "        self.lr = lr\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "        \n",
    "        self.num_runs = runs\n",
    "        self.num_eps = eps\n",
    "        self.avg_steps = []\n",
    "        self.avg_reward = []\n",
    "        self.var_reward = []\n",
    "        \n",
    "        self.eps_to_solve = [self.num_eps]*self.num_runs\n",
    "        self.solved = False\n",
    "        \n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.exp_name = exp_name\n",
    "        \n",
    "    def train(self, verbose = 1):\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        for run in range(self.num_runs):\n",
    "            agent = Agent1(lr = self.lr, action_size = self.env.action_space.n, h1 = self.h1, h2 = self.h2)\n",
    "            eps_rewards, eps_steps = [], []\n",
    "            self.solved = False\n",
    "            \n",
    "            if verbose: print('Starting RUN:', run+1)\n",
    "                \n",
    "            for ep in range(1, self.num_eps+1):\n",
    "                steps, rewards = 0, 0\n",
    "                state = self.env.reset()[0].reshape(1,-1)\n",
    "                done = False\n",
    "                \n",
    "                while not done and steps<500:\n",
    "                    action = agent.sample_action(state)\n",
    "                    n_state, rew, done, info, _ = self.env.step(action)\n",
    "                    n_state = n_state.reshape(1,-1)\n",
    "                    \n",
    "                    agent.learn(state, action, rew, n_state, done)\n",
    "                    state = n_state\n",
    "                    \n",
    "                    rewards += rew\n",
    "                    steps += 1\n",
    "                \n",
    "                eps_rewards.append(rewards)\n",
    "                eps_steps.append(steps)\n",
    "\n",
    "                if ep%10==0 and verbose>1: \n",
    "                    print('\\r','Ep', ep, 'Avg Reward: %f'%np.mean(eps_rewards[-10:]), end='')\n",
    "                    \n",
    "                if ep%100 and not self.solved:\n",
    "                    avg_100 = np.mean(eps_rewards[-100:])\n",
    "                    if avg_100 > self.thresh:\n",
    "                        self.eps_to_solve[run] = ep - 100 if ep > 100 else ep\n",
    "                        self.solved = True\n",
    "                        if verbose:\n",
    "                            print('\\nEnvironment solved in {:d} episodes!'.format(self.eps_to_solve[run]))\n",
    "                            print('Env. Threshold:{:.2f} \\tAverage Score: {:.2f}'.format(self.thresh, np.mean(eps_rewards[-100:])))\n",
    "                        break\n",
    "                        \n",
    "            if self.solved:\n",
    "                avg_100 = np.mean(eps_rewards[-100:])\n",
    "                while len(eps_rewards)<self.num_eps:\n",
    "                    eps_rewards.append(avg_100)\n",
    "                \n",
    "            if verbose: \n",
    "                print('Solved:', self.solved)\n",
    "        \n",
    "            self.avg_reward.append(eps_rewards)\n",
    "            self.avg_steps.append(eps_steps)\n",
    "            \n",
    "        self.end_time = datetime.datetime.now()\n",
    "        if verbose: print('Time Taken', self.end_time - self.start_time)\n",
    "            \n",
    "    def plot_results(self):\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        maxlen = min(max(self.eps_to_solve) + 100, 2500)\n",
    "        minlen = min(min(self.eps_to_solve) + 99, 2500)\n",
    "        reward_list = np.mean(self.avg_reward, axis = 0)[:maxlen]\n",
    "        avg100_reward = np.array([np.mean(reward_list[max(0,i-100):i]) for i in range(1,len(reward_list)+1)])[:maxlen]\n",
    "\n",
    "\n",
    "        print('Average Number of Episodes To Solve the Environment:', np.mean(self.eps_to_solve))\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Episode Reward')\n",
    "        plt.title('Rewards vs Episodes: Avg Reward: %.3f'%np.mean(reward_list))\n",
    "        plt.plot(np.arange(maxlen), reward_list, 'b')\n",
    "        plt.plot(np.arange(maxlen), avg100_reward, 'r', linewidth=2.5)\n",
    "        plt.savefig('./plots/ac_ab_'+self.exp_name+'_rewards.jpg', pad_inches = 0)\n",
    "        plt.show()\n",
    "\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Variance by Episode')\n",
    "        plt.title('Variance across Runs')\n",
    "        plt.plot(np.arange(minlen), np.var(self.avg_reward, axis = 0)[:minlen], 'b')\n",
    "        plt.savefig('./plots/ac_ab_'+self.exp_name+'_variance.jpg', pad_inches = 0)\n",
    "        plt.show()\n",
    "        \n",
    "        return self.eps_to_solve, self.avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7eb5b",
   "metadata": {},
   "source": [
    "## HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa423cf2",
   "metadata": {},
   "source": [
    "In order to obtain the best results we need to tune the following hyperparameters:\n",
    "\n",
    "- **Learning Rate ($\\alpha$)**:\n",
    "The learning rate controls how fast the model will learn. A learning rate that is too small will prove to be extremely slow while learning and is generally not useful. Similarly, a learning rate that is too high will cause huge jumps in values causing the model to overshoot the optimum parameters in the multi-dimensional parameter space. This will cause unpredictable and oscillatory behaviour of the model.\n",
    "\n",
    "For our purposes we will explore $lr \\in [0.0005, 0.0001, 0.00005]$\n",
    "\n",
    "- **Hidden Layers ($h_1$ & $h_2$)**:\n",
    "The hidden layer sizes in the model greatly affect the performance of the AC model. The ability of the model to learn specific actions (overfitting on the action space) and general patterns of actions depedns on size and number of hidden layers.\n",
    "\n",
    "For our purposes we will explore $(h1, h2) \\in [(512, 256), (1024, 512), (2048, 1024)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436f82ed",
   "metadata": {},
   "source": [
    "### Experiment 1: $lr = 0.00001, h_1 = 512, h_2 = 256$ \n",
    "\n",
    "First we try out a very simple configuration of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807b92c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = OneStepAC(env_name = 'Acrobot-v1', exp_name = 'e1', lr = 0.00001, h1 = 512, h2 = 256)\n",
    "trainer.train(verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps1, rew1 = trainer.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c55e54",
   "metadata": {},
   "source": [
    "### Experiment 2: $lr = 0.000005, h_1 = 1024, h_2 = 512$ \n",
    "\n",
    "Now we try to increase the size of the model by changing the size of the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b78573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = OneStepAC(env_name = 'Acrobot-v1', exp_name = 'e2', lr = 0.000005, h1 = 1024, h2 = 512)\n",
    "trainer.train(verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75906441",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps2, rew2 = trainer.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42610f59",
   "metadata": {},
   "source": [
    "### Experiment 3: $lr = 0.000005, h_1 = 512, h_2 = 256$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411d713",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = OneStepAC(env_name = 'Acrobot-v1', exp_name = 'e3', lr = 0.000005, h1 = 512, h2 = 256)\n",
    "trainer.train(verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps3, rew3 = trainer.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccaf24",
   "metadata": {},
   "source": [
    "### Experiment 4: $lr = 0.000005, h_1 = 2048, h_2 = 1024$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = OneStepAC(env_name = 'Acrobot-v1', exp_name = 'e4', lr = 0.000005, h1 = 2048, h2 = 1024)\n",
    "trainer.train(verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880281c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps4, rew4 = trainer.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934399c",
   "metadata": {},
   "source": [
    "### Experiment 5: $lr = 0.000005,  h_1 = 1024, h_2 = 512$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6faac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = OneStepAC(env_name = 'Acrobot-v1', exp_name = 'e5', lr = 0.000005, h1 = 1024, h2 = 512)\n",
    "trainer.train(verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83272d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps5, rew5 = trainer.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b1620",
   "metadata": {},
   "source": [
    "## Agent 2: Full-Return AC\n",
    "\n",
    "- The full return step TD error can be defined as follows:\n",
    "$$\\delta_t^{(T)}  = \\sum_{t'=t}^T \\gamma^{t'-t}R_{t'+1} - v(s_t)$$\n",
    "\n",
    "- The loss function to be minimized at every step ($L_{tot}$) is a summation of two terms, as follows:\n",
    "$$L_{tot} = L_{actor} + L_{critic}$$\n",
    "where,\n",
    "$$L_{actor} = -\\sum_{t=1}^T \\log\\pi(a_t|s_t; \\theta)\\delta_t^{(T)}$$\n",
    "$$L_{critic} = \\sum_{t=1}^T \\delta_t^{(T)^2}$$\n",
    "\n",
    "- The best hyperparameters from the previous case has been used here to prepare a comparison of the 3 AC methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d29962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent2:\n",
    "    \"\"\"\n",
    "    Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, lr=1e-4, gamma=0.99, seed = 85, h1 = 1024, h2 = 512, h3 = None):\n",
    "        self.gamma = gamma\n",
    "        self.ac_model = ActorCriticModel(action_size, h1, h2)\n",
    "        self.ac_model.compile(tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Given a state, compute the policy distribution over all actions and sample one action\n",
    "    def sample_action(self, state):\n",
    "        pi,_ = self.ac_model(state)\n",
    "        action_probabilities = tfp.distributions.Categorical(probs=pi)\n",
    "        sample = action_probabilities.sample()\n",
    "        return int(sample.numpy()[0])\n",
    "\n",
    "    # actor loss\n",
    "    def actor_loss(self, action, pi, delta):\n",
    "        return -tf.math.log(pi[0,action]) * delta\n",
    "\n",
    "    # critic loss\n",
    "    def critic_loss(self,delta):\n",
    "        return delta**2\n",
    "\n",
    "    @tf.function\n",
    "    # For a given transition (s,a,s',r) update the paramters\n",
    "    def learn(self, state, action, target):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            pi, V_s = self.ac_model(state)\n",
    "            V_s = tf.squeeze(V_s)\n",
    "\n",
    "            # Equation for TD error\n",
    "            delta = target - V_s\n",
    "            \n",
    "            loss_a = self.actor_loss(action, pi, delta)\n",
    "            loss_c =self.critic_loss(delta)\n",
    "            loss_total = loss_a + loss_c\n",
    "\n",
    "        gradient = tape.gradient(loss_total, self.ac_model.trainable_variables)\n",
    "        self.ac_model.optimizer.apply_gradients(zip(gradient, self.ac_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c371d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullReturnAC:\n",
    "    def __init__(self, env_name, exp_name, gamma = 0.99, eps = 2500, runs = 3, lr = 1e-4, h1 = 1024, h2 = 512):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state = self.env.reset()[0]\n",
    "        self.thresh = 195.0\n",
    "        self.max_steps = 500\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "        \n",
    "        self.num_runs = runs\n",
    "        self.num_eps = eps\n",
    "        self.avg_steps = []\n",
    "        self.avg_reward = []\n",
    "        self.var_reward = []\n",
    "        \n",
    "        self.eps_to_solve = [self.num_eps]*self.num_runs\n",
    "        self.solved = False\n",
    "        \n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.exp_name = exp_name\n",
    "        \n",
    "    def train(self, verbose = 1):\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        for run in range(self.num_runs):\n",
    "            agent = Agent2(lr = self.lr, action_size = self.env.action_space.n, h1 = self.h1, h2 = self.h2)\n",
    "            eps_rewards, eps_steps = [], []\n",
    "            self.solved = False\n",
    "            \n",
    "            if verbose: print('Starting RUN:', run+1)\n",
    "                \n",
    "            for ep in range(1, self.num_eps+1):\n",
    "                steps, rewards = 0, 0\n",
    "                state = self.env.reset()[0].reshape(1,-1)\n",
    "                action_list, state_list = [], []\n",
    "                step_rewards = []\n",
    "                done = False\n",
    "                \n",
    "                for steps in range(self.max_steps):\n",
    "                    action = agent.sample_action(state)\n",
    "                    state_list.append(state)\n",
    "                    action_list.append(action)\n",
    "                    \n",
    "                    n_state, rew, done, info, _ = self.env.step(action)\n",
    "                    n_state = n_state.reshape(1,-1)\n",
    "                    \n",
    "                    rewards += rew\n",
    "                    step_rewards.append(rew)\n",
    "                    \n",
    "                    state = n_state\n",
    "                    steps += 1\n",
    "                    \n",
    "                    if done or steps == self.max_steps - 1:\n",
    "                        eps_rewards.append(rewards)\n",
    "                        eps_steps.append(steps)\n",
    "                        break\n",
    "                \n",
    "                for t in range(len(step_rewards)):\n",
    "                    target = 0\n",
    "                    for k in range(t, len(step_rewards) - 1):\n",
    "                        target += (self.gamma**(k-t)) * step_rewards[k+1]\n",
    "                    agent.learn(state_list[t], action_list[t], target)\n",
    "\n",
    "                if ep%10==0 and verbose>1: \n",
    "                    print('\\r','Ep', ep, 'Avg Reward: %f'%np.mean(eps_rewards[-10:]), end='')\n",
    "                    \n",
    "                if ep%100 and not self.solved:\n",
    "                    avg_100 = np.mean(eps_rewards[-100:])\n",
    "                    if avg_100 > self.thresh:\n",
    "                        self.eps_to_solve[run] = ep - 100 if ep > 100 else ep\n",
    "                        self.solved = True\n",
    "                        if verbose:\n",
    "                            print('\\nEnvironment solved in {:d} episodes!'.format(self.eps_to_solve[run]))\n",
    "                            print('Env. Threshold:{:.2f} \\tAverage Score: {:.2f}'.format(self.thresh, np.mean(eps_rewards[-100:])))\n",
    "                        break\n",
    "                        \n",
    "            if self.solved:\n",
    "                avg_100 = np.mean(eps_rewards[-100:])\n",
    "                while len(eps_rewards)<self.num_eps:\n",
    "                    eps_rewards.append(avg_100)\n",
    "                \n",
    "            if verbose: \n",
    "                print('Solved:', self.solved)\n",
    "        \n",
    "            self.avg_reward.append(eps_rewards)\n",
    "            self.avg_steps.append(eps_steps)\n",
    "            \n",
    "        self.end_time = datetime.datetime.now()\n",
    "        if verbose: print('Time Taken', self.end_time - self.start_time)\n",
    "            \n",
    "    def plot_results(self):\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        maxlen = min(max(self.eps_to_solve) + 100, 2500)\n",
    "        minlen = min(min(self.eps_to_solve) + 99, 2500)\n",
    "        reward_list = np.mean(self.avg_reward, axis = 0)[:maxlen]\n",
    "        avg100_reward = np.array([np.mean(reward_list[max(0,i-100):i]) for i in range(1,len(reward_list)+1)])[:maxlen]\n",
    "\n",
    "        \n",
    "        print('Average Number of Episodes To Solve the Environment:', np.mean(self.eps_to_solve))\n",
    "    \n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Episode Reward')\n",
    "        plt.title('Rewards vs Episodes: Avg Reward: %.3f'%np.mean(reward_list))\n",
    "        plt.plot(np.arange(maxlen), reward_list, 'b')\n",
    "        plt.plot(np.arange(maxlen), avg100_reward, 'r', linewidth=2.5)\n",
    "        plt.savefig('./plots/ac_cp_'+self.exp_name+'_rewards.jpg', pad_inches = 0)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Variance by Episode')\n",
    "        plt.title('Variance across Runs')\n",
    "        plt.plot(np.arange(minlen), np.var(self.avg_reward, axis = 0)[:minlen], 'b')\n",
    "        plt.savefig('./plots/ac_cp_'+self.exp_name+'_variance.jpg', pad_inches = 0)\n",
    "        plt.show()\n",
    "        \n",
    "        return self.eps_to_solve, self.avg_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
