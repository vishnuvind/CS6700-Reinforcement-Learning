{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xEl5AbPCamd5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from enum import Enum\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9tjMH9cgH29"
   },
   "source": [
    "Consider a standard grid world, where only 4 (up, down, left, right) actions are allowed and the agent deterministically moves accordingly, represented as below. Here yellow is the start state and white is the goal state.\n",
    "\n",
    "Say, we define our MDP as:\n",
    "- S: 121 (11 x 11) cells \n",
    "- A: 4 actions (up, down, left, right)\n",
    "- P: Deterministic transition probability\n",
    "- R: -1 at every step\n",
    "- gamma: 0.9\n",
    "\n",
    "Our goal is to find an optimal policy (shown in right).\n",
    "\n",
    "\n",
    "\n",
    "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAwAAAAF9CAYAAACzog+4AAAgAElEQVR4Xu3df5Cd11nY8Wd3tZJ2heXIin8oxI7lBIhjnBgMNBBo1wkmybTTmUaEwrQemSAG4jKQWA0BPKmbNJ3OlMx0EuOMXYZMmpK2buKQGQqFpHWWUqraM6GU2MFQrMQOsuMfkWRrtZIs7d7uyj+iSCu97927555zz/vZDPyhe+55zvk+z3O83z3v3R3rLX2FLwQQQAABBBBAAAEEEOgEgTEC0Ik82yQCCCCAAAIIIIAAAicJEACFgAACCCCAAAIIIIBAhwgQgA4l21YRQAABBBBAAAEEECAAagABBBBAAAEEEEAAgQ4RIAAdSratIoAAAggggAACCCBAANQAAggggAACCCCAAAIdIkAAOpRsW0UAAQQQQAABBBBAgACoAQQQQAABBBBAAAEEOkSAAHQo2baKAAIIIIAAAggggAABUAMIIIAAAggggAACCHSIAAHoULJtFQEEEEAAAQQQQACB1gIwOzuLFgIIIIBAZQRmZmYq25HtIIAAAgg0EVhTAVhcXIxnnnkm1q1bF+Pj4zE2NtYU3+sIIIAAAokI9Hq9ePbZZ2NqairWr1+/4plMABLBNy0CCCBQMIE1E4Dlb/4PHjwYv/7rvx7z8/MFb9nSEEAAge4QWD6bf/InfzK+7/u+LzZs2HDGxglAd2rBThFAAIEXCKypADz99NPxjne8I7aOvTmmJ7fFWIwjjQACCCCQicBi79l46OCn4xdv3hnXXXddawHwyGemhAmLAAIIJCRw6g981kwAlq+aDx06FDfeeGN81/QtsWXqNTE+NpFwG6ZGAAEEEDgXgRMLR+J/7/vluOnmHScFYHJystUNAAFQVwgggEB9BJIJwNzcXOzcuTOu3HTrkgBcRQDqqx07QgCBESJwYmE+9uzbfVIAlg9+AjBCybNUBBBAYI0JEIA1Bmo6BBBAoEQCwxCA5dvf5R/+TExMnPzlD74QQAABBIZHYPkXPWzatOnkGdz0RQCaCHkdAQQQqIBAagF44Zv/T3ziE3H48GG/+a2CmrEFBBAYHQLLZ/DCwkLccMMNsW3btkYJIACjk1srRQABBFZNYBgCsPzLH37+538+Juaviql1Fy398ge//nnVCfNGBBBAoA8CC72j8fDTvxu3f/S2eNWrXnXy1/Cf64sA9AHXUAQQQGBUCQxDAJZ/7fOuXbvisvFfiq1TVy/dApz7P0CjytK6EUAAgZII9GIxnj1xMGYfeUfccedtsX37djcAJSXIWhBAAIFcBIYpAJdP7I6t09f45Q+5ki0uAgh0ikAvenHs+P645+EbCECnMm+zCCCAQAMBAqBEEEAAgToJEIA682pXCCCAwMAECMDACE2AAAIIFEmAABSZFotCAAEE8hMgAPlzYAUIIIBACgIEIAVVcyKAAAIVECAAFSTRFhBAAIEVCBAAZYEAAgggsCIBAqAwEEAAgToJEIA682pXCCCAwMAECMDACE2AAAIIFEmAABSZFotCAAEE8hMgAPlzYAUIIIBACgIEIAVVcyKAAAIVECAAFSTRFhBAAIEVCBAAZYEAAgggsCIBAqAwEEAAgToJEIA682pXCCCAwMAECMDACE2AAAIIFEmAABSZFotCAAEE8hMgAPlzYAUIIIBACgIEIAVVcyKAAAIVECAAFSTRFhBAAIEVCBAAZYEAAgggsCIBAqAwEEAAgToJEIA682pXCCCAwMAECMDACE2AAAIIFEmAABSZFotCAAEE8hMgAPlzYAUIIIBACgIEIAVVcyKAAAIVECAAFSTRFhBAAIEVCBAAZYEAAgggsCIBAqAwEEAAgToJEIA682pXCCCAwMAECMDACE2AAAIIFEmAABSZFotCAAEE8hMgAPlzYAUIIIBACgIEIAVVcyKAAAIVECAAFSTRFhBAAIEVCBAAZYEAAgggsCIBAqAwEEAAgToJEIA682pXCCCAwMAECMDACE2AAAIIFEmAAJyWlscP/684cOQvikyWRSHQNQKv3PITMTlxXtZt7z9yfzxx+N5sa1g3Ph2vuuCnssQnAFmwC4oAAggkJ0AATkP8pSc+El89+Nnk4AVAAIFmAm/a/smYntzWPDDhiL0H7o4Hnrw9YYRzT71x3YVx/RV3ZYlPALJgFxQBBBBIToAAEIDkRSYAAqslQAAiCMBqq8f7EEAAAQTORoAAEADdgUCxBAgAASi2OC0MAQQQGGECBIAAjHD5WnrtBAgAAai9xu0PAQQQyEGAABCAHHUnJgKtCBAAAtCqUAYYtLB4NO579NdioXd8gFlG+63bN07Gb195yWhvYoRX/x8ePxS3P3pwhHcw2NJfvfXGeOn0tYNNMuC7H3zqY/HUkf8z4Cyrf/trL3pXbN7wytVPsIp3EgACsIqy8RYEhkOAABCA1JV2YnE+PvfQjiUBOJY6VLHzv2Z6fdz/A68odn21L+xDXzsQv/zQU7Vv86z7u3bb++Jl512Xdf9ffOwD8eih2WxreMOlH44Lpq4eanwCQACGWnCCIdAPAQJAAPqpl9WMJQARBGA1lbN27yEABIAAzM3Fzp0748pNt8aWqatifGxi7Tqs5Ux+DWhLUIYhMAQCBIAApC4zAkAAUtdY0/wEgAAQAALQdE54HYFOESAABCB1wRMAApC6xprmJwAEgAAQgKZzwusIdIoAASAAqQueABCA1DXWND8BIAAEgAA0nRNeR6BTBAgAAUhd8ASAAKSusab5CQABIAAEoOmc8DoCnSJAAAhA6oInAAQgdY01zU8ACAABIABN54TXEegUAQJAAFIXPAEgAKlrrGl+AkAACAABaDonvI5ApwgQAAKQuuAJAAFIXWNN8xMAAkAACEDTOeF1BDpFgAAQgNQFTwAIQOoaa5qfABAAAkAAms4JryPQKQIEgACkLngCQABS11jT/ASAABAAAtB0TngdgU4RIAAEIHXBEwACkLrGmuYnAASAABCApnPC6wh0igABIACpC54AEIDUNdY0PwEgAASAADSdE15HoFMECAABSF3wBIAApK6xpvkJAAEgAASg6ZzwOgKdIkAACEDqgicABCB1jTXNTwAIAAEgAE3nhNcR6BQBAkAAUhc8ASAAqWusaX4CQAAIAAFoOie8jkCnCBAAApC64AkAAUhdY03zEwACQAAIQNM54XUEOkWAABCA1AVPAAhA6hprmp8AEAACQACazgmvI9ApAgSAAKQueAJAAFLXWNP8BIAAEAAC0HROeB2BThEgAAQgdcETAAKQusaa5icABIAAEICmc8LrCHSKAAEgAKkLngAQgNQ11jQ/ASAABIAANJ0TXkegUwQIAAFIXfAEgACkrrGm+QkAASAABKDpnPA6Ap0iQAAIQOqCJwAEIHWNNc1PAAgAASAATeeE1xHoFAECQABSFzwBIACpa6xpfgJAAAgAAWg6J7yOQKcIEAACkLrgCQABSF1jTfMTAAJAAAhA0znhdQQ6RYAAEIDUBU8ACEDqGmuanwAQAAJQgAD89f7/FI/OzTb1a/WvH352X5xYPJxtn+snzo+pyYuzxe/1FuPQsb3Ri8Vsa5ievCQmJzZni19C4B942Qdj47qXZl3K3gN3xwNP3p5tDRvXXRjXX3FXlvgnFuZjz77dcdPNO2JmZiYmJyfPWMfyv5/+NTvb7gzt9XoxPz8fu3btissndsfW6WtifGxiqHslAARgqAW3QjACQAAIQAECkPsgKCX+vft+JZ44fF+25Vx2/lvjdRe/J1v840vy8/m9b4+FxaPZ1nDNJe+NSze/OVt8gZ8jQAAIQO298Jrp9XH/D7yi9m0Wuz8CQAAIAAEo5oAiAASgmGLMvBACQAAyl2Dy8AQgOeJzBiAABIAAEIC8p9Ap0QkAASimGDMvhAAQgMwlmDw8AUiOmACcg8C12wgAASAAeU8hAvAiAY8AFVOK2RdCAAhA9iJMvAACkBhww/RuAAgAASAAeU8hAkAAiqnAchZCAOoWgN7SR/2PHP/60v/vlVN0Q17JKyafjN962UeHHPWb4R589pXxbw7+dLb4f2vj/413bP5Utvj/+Zkfin974Pps8XMH3jCxJdaNT2Vdxhcf+0A8emg22xoIAAHIVnynB/YIkEeAiinGzAshAHULQObyKiL8pesei9+46J9nW8ufH3t1vO8b784Wf2bq3nj3lo9li/87cz8WH39mR7b4AkcQgNti+/btMTFx7t/CdupvfRtb+jVurX5s0vRr4ZanmSMAxfQhASAAxRRj5oUQAAKQuQSThycABCB5kRUegAAQgMJLdHjLIwAEYHjVVnYkAkAAyq7QwVdHAAjA4FU02jMQAAIw2hW8hqsnAARgDctppKciAARgpAu4xeIJAAFoUSZVDyEABKDqAu9ncwSAAPRTLzWPJQAEoOb6Xt4bASAAtdd40/4IAAFoqpHOvE4ACEBnir1howSAANTeCwSAANRe4037IwAEoKlGOvM6ASAAnSl2AnBWAicW5mPPPgJQey8QAAJQe4037Y8AEICmGunM6wSAAHSm2AkAAeh4sRMAAtDxFvBrQO8kAF3vgRf3TwAIgGZ4joBHgNwA1N4LBIAA1F7jTftzA0AAmmqkM68TAALQmWJ3A+AGoOPFTgAIQMdbwA2AG4Cut8A3908ACIBucAPgMwDd6AICQAC6Ueln36UbADcAXe8BjwA9T+D4IgHQDASAAHSjCwgAAehGpROAUwn0ohfHju+Pex6+Ie5wA9D1FnAD8AIBAqAXXiDgMwA+A1B7NxAAAlB7jTftzw2AG4CmGunM6x4BcgPQmWL3GQCfAeh4sRMAAtDxFvAZADcAXW8BNwBuAPTA6QTcALgBqL0rCAABqL3Gm/bnBsANQFONdOZ1NwBuADpT7G4A3AB0vNgJAAHoeAu4AXAD0PUWcAPgBkAPuAH4JgEfAu5GPxAAAtCNSj/7Lt0AuAHoeg+8uH83AG4ANMNzBDwC5BGg2nuBABCA2mu8aX8EgAA01UhnXicABKAzxe4RII8AdbzYCQAB6HgLeATII0BdbwGPAHkESA94BMgjQF3rAgJAALpW86fv1w2AG4Cu94BHgJ4n4O8AaIUXCHgEyCNAtXcDASAAtdd40/4IAAFoqpHOvO4RII8AdabYPQLkEaCOFzsBIAAdbwGPAHkEqOst4BEgjwDpAY8AeQSoa11AAAhA12reI0ARvaX/HTu+P+55+Ia4gwB0vQUIAAHQAwSAAHStCwgAAehazRMAAtD1mj/r/j0C5BEgzfEcAZ8B8BmA2nuBABCA2mu8aX8+A+AzAE010pnXCQAB6EyxN2yUABCA2nuBABCA2mu8aX8EgAA01UhnXicABKAzxU4AzkrAXwLuRhcQAALQjUo/+y4JAAHoeg+8uH8CQAA0g0eACEA3uoAAEIBuVDoBOJWADwGfVg/X//i2+N4fuSBbLzzwwAPx/ve/P1v8FwJ/48iX4tiJ/dnWcdn5b43XXfyebPH9HYCIp4/+Vfz1gf+YLQfLgb/7ol+MDRNbsq7BI0AeAcpagEMITgAIwBDKrOgQbgDcAMTPve874+/+42/PVqhf+MIX4o1vfGO2+KUEJgAR11zy3rh085uzpeTxw3vivn23ZIu/HPhN2z8Z05Pbsq6BABCArAU4hOAEgAAMocyKDkEACAABKKRFCQABIADPNePGdRfG9VfclaUzPQKUBfvQgxIAAjD0oissIAEgAASgkKYkAASAABCAQo6j6pdBAAhA9UXesEECQAAIQCGnAAEgAASAAAzjONo4PREf+tS1sX79+DDCnRnjsa9F75/+dJ7Yz0ddN3YiXjpxINsa/vzYq+N933h3tvgzU90WgP+3/7fja0//QTb+V130C3Hxptdni78cmAAQAAKQtQW/GZwAEAACQACGcRxNf9u6+MSfvCHWb8wkAI/sjd7Otwxjq8XGIAA/Fh9/Zke2/Dzw5EeX/uDhp7PFv3bb++Jl512XLT4BuCHuuJMAEICsLUgATsXvQ8A+BLxcDz4DkPZQIgBp+baZnQAQAALw4bhg6uo27bJmY/wa0NNQ+i1Aa1ZbA03kBsANgBsANwADHSIt30wAWoJKOIwAEAACQABi586dceWmW2PL1FUxPjaR8MhZeWoCMHTkKwYkAASAABCAYZxGBGAYlM8dgwAQAAJAAAiAvwNw8r8UBIAAEAACMIxvTQnAMCgTgHMR+J05AkAACAABIAAE4Pn/UvgMgM8ALJeCzwCk/QaVAKTl22Z2NwAEgAAQAAJAAAgAAXjxewZ/CZgAtPkGcpAxBGAQemvzXgJAAAgAASAABIAAEAACcMr3VW4A1uabzLPNQgDS8m0zOwEgAASAABAAAkAACAABIABtvm9ckzEEYE0wDjQJASAABIAAEAACQAAIAAEgAAN9Q9nPmwlAP7TSjCUABIAAEAACQAAIAAEgAAQgzXeaK8xKAIaG+qyBCAABIAAEgAAQAAJAAAgAARjad6UEYGioCcBZCPg1oO8LAkAACAABIAAEgAAQgKF9V0oAhoaaABCAFQlcu40AvOFSAkAACAABIAAEgAAM7btSAjA01ASAABCAs9QAAZibIwAEgAAQAAJAAIb2XSkBGBpqAkAACAABOJNAr9eLOQIQXyAABIAAEAACMLTvSgnA0FATAAJAAAgAATjbSUgAniNz2flvjddd/J5s/2U6vng4Pr/37bGweDTbGq655L1x6eY3Z4v/+OE9cd++W7LFXw7sLwH7S8CpC5AApCbcPL/fAuS3APkQsM8AeATIDQABcAPgBsANQPN3jWs0ggCsEcgBpiEABIAAEAACQAAIAAEgAARggG8n+3srAeiPV4rRBIAAEAACQAAIAAEgAASAAKT4PnPFOQnA0FCfNRABIAAEgAAQAAJAAAgAASAAQ/uulAAMDTUBOAsBfwjM3wHwa0D9FiC/Bej5A9KHgCN8CNiHgJfbYeO6C+P6K+7K8l3aiYX52LNvd9x0846YmZmJycnJM9ax/O+nf83OzrZa7/Jvf5ufn49du3bF5RO7Y+v0NTE+NtHqvWs1iACsFcnVz+MGwA2AGwA3AG4A3AC4AXAD4AbADcDqv5vs850EoE9gCYYTAAJAAAgAASAABIAAEAACkODbzJWnJABDQ+0RII8ArUjg2m0eAfIIkEeAPALkEaAXD0iPAHkEaLkYPAKU9htUApCWb5vZ3QC4AXAD4AbADYAbADcAbgDcALgBaPN945qMIQBrgnGgSQgAASAABIAAEAACQAAIAAEY6BvKft5MAPqhlWYsASAABIAAZBeAN71tW3zPD29Jc8q1mPXLD3w5PvjBD7YYWfeQ5d8G8orz/162TR5fPByf3/v2WFg8mm0NHgHyCJBHgNK3HwFIz7gpAgEgAASAAGQXgKaDyuvdIEAAIh4/vCfu23dL1oS/afsnY3pyW9Y17D1wdzzw5O3Z1uAzAGnRE4C0fNvMTgAIAAEgAASgzWlpTHICBIAAvFBkBMDfAUh64DyyN3o735I0ROmTEwACQAAIAAEo/aTuyPoIAAEgABFd+ENgU5sm4jf/+w/G+g3jeU63r30lej/3D/LELiTq/ce+M/7F/l/ItpqZqXvj3Vs+li2+vwTs14D6NaAF/BrQbCeAwEURIAAEgAB0QwBibOlXrU4N968Pf8th11uMOJrvs0YlHLyLMR7P9s78K9PDWhsB+GjsPfDpYeE+I46/AxBBAAhAtgYU+FsJEAACQAA6IgAOv84TIAAE4IuPfSAePTSbrRcIAAHIVnwCE4DTa8CHgJ8j4jMAdX8GwNmHAAEgAATgtti+fXtMTJz7NnRmZubFA2Ost/TV5viYnT23WS1PM0cA2qA0ZggE3AC4AXAD4AZgCEeNEAUQIAAEgAAQgAKOIksogQABIAAEgACUcBZZQ3oCBIAAEAACkP6kEWEkCBAAAkAACMBIHFYWOTABAkAACAABGPggMUEdBAgAASAABKCO08wumggQAAJAAAhA0znh9Y4QIAAEgAAQgI4cd53fJgEgAASAAHT+IATgOQIEgAAQAALgPOwGAQJAAAgAAejGaWeXjQQIAAEgAASg8aAwoAoCBIAAEAACUMVhZhODEyAABIAAEIDBTxIzjAIBAkAACAABGIWzyhqHQIAAEAACQACGcNQIUQABAkAACAABKOAosoQSCBAAAkAACEAJZ5E1pCdAAAgAASAA6U8aEUaCAAEgAASAAIzEYWWRAxMgAASAABCAgQ8SE9RBgAAQAAJAAOo4zeyiiQABIAAEgAA0nRNe7wgBAkAACAAB6Mhx1/ltEgACQAAIQOcPQgCeI0AACAABIADOw24QIAAEgAAQgG6cdnbZSIAAEAACQAAaDwoDqiBAAAgAASAAVRxmNjE4AQJAAAgAARj8JDHDKBAgAASAABCAUTirrHEIBAgAASAABGAIR40QBRAgAASAABCAAo4iSyiBAAEgAASAAJRwFllDegIEgAAQAAKQ/qQRYSQIEAACQAAIwEgcVhY5MAECQAAIAAEY+CAxQR0ECAABIAAEoI7TzC6aCBAAAkAACEDTOeH1jhAgAASAABCAjhx3nd8mASAABIAAdP4gBOA5AgSAABAAAuA87AYBAkAACAAB6MZpZ5eNBAgAASAABKDxoDCgCgIEgAAQAAJQxWFmE4MTIAAEgAAQgMFPEjOMAgECQAAIAAEYhbPKGodAgAAQAAJAAIZw1AhRAAECQAAIAAGIXm8hekv/85WXwFgs/W9sItsiCEAZAnDd5f8upie3ZauD5cBfOfiZ+PKTd2Rbw8Z1F8b1V9yVJf6JhfnYs2933HTzjpiZmYnJyckz1rH876d/zc7Otlpvr9eL+fn52LVrV1w+sTu2Tl8T4xn7vtWiDaqOAAEgAASAAMRfPPWb8cjTv1/dATdqG/r2zT8a333hP8m2bAJQhgBMTpy3pILj2epgOfBC71gsLB7NtgYCkA29wB0hQAAIAAEgAPGlJz4SXz342Y4ce+Vu87Lz3xqvu/g92RZIAMoQgGwFUFBgAlBQMiylSgIEgAAQAAJAAAo53glAxDWXvDcu3fzmbBl5/PCeuG/fLdniC/wcAQKQthKWH/k8cvzrHv1Mi/mcs0+MbViq863ZVkAA8grAd1/0C3HRptdny/9y4PufuC2eOHxvtjW84dIPxwVTVw81/vLZd+z4/rjn4RvijjsJAAEYavmdPRgBIACFlGL2ZRCAtCk4sTgfn3tox8lHvXzlIfDS6e+NH3z5h/IEX4pKAPIKQLbEFxSYAMzNxc6dO+PKTbfGlqmrsnwYzCNAZXQEASAAZVRi/lUQgLQ5IABp+baZnQD8WHz8mR1tUCUZ88CTBCAJ2D4mJQAEoI9yqXsoASAAdVd4+90RgPasVjOSAKyG2tq+hwAQgLWtqNGbjQAQgNGr2kQrJgAEIFFpjdy0BCBtyghAWr5tZicABKBNndQ8hgAQgJrru6+9EQAC0FfBVDyYAKRNLgFIy7fN7ASAALSpk5rHEAACUHN997U3AkAA+iqYigcTgLTJJQBp+baZnQAQgDZ1UvMYAkAAaq7vvvZGAAhAXwVT8WACkDa5BCAt3zazEwAC0KZOah5DAAhAzfXd194IAAHoq2AqHkwA0iaXAKTl22Z2AkAA2tRJzWMIAAGoub772hsBIAB9FUzFgwlA2uQSgLR828xOAAhAmzqpeQwBIAA113dfeyMABKCvgql4MAFIm1wCkJZvm9kJAAFoUyc1jyEABKDm+u5rbwSAAPRVMBUPJgBpk0sA0vJtMzsBIABt6qTmMQSAANRc333tjQAQgL4KpuLBBCBtcglAWr5tZicABKBNndQ8hgAQgJrru6+9EQAC0FfBVDyYAKRNLgFIy7fN7ASAALSpk5rHEAACUHN997U3AkAA+iqYigcTgLTJJQBp+baZnQAQgDZ1UvMYAkAAaq7vvvZGAAhAXwVT8WACkDa5BCAt3zazEwAC0KZOah5DAAhAzfXd194IAAHoq2AqHkwA0iaXAKTl22Z2AkAA2tRJzWMIAAGoub772hsBIAB9FUzFgwlA2uQSgLR828xOAAhAmzqpeQwBIAA113dfeyMABKCvgql4MAFIm1wCkJZvm9kJAAFoUyc1jyEABKDm+u5rbwSAAPRVMBUPJgBpk0sA0vJtMzsBIABt6qTmMQSAANRc333tjQAQgL4KpuLBBCBtcglAWr5tZicABKBNndQ8hgAQgJrru6+9EQAC0FfBVDyYAKRNLgFIy7fN7ASAALSpk5rHEAACUHN997U3AkAA+iqYigcTgLTJJQBp+baZnQAQgDZ1UvMYAkAAaq7vvvZGAAhAXwVT8WACkDa5BCAt3zazEwAC0KZOah5DAAhAzfXd194IAAHoq2AqHkwA0iaXAKTl22Z2AkAA2tRJzWMIAAGoub772hsBIAB9FUzFgwlA2uQSgLR828xOAAhAmzqpeQwBIAA113dfeyMABKCvgql4MAFIm1wCkJZvm9kJAAFoUyc1jyEABKDm+u5rbwSAAPRVMBUPJgBpk0sA0vJtMzsBIABt6qTmMQSAANRc333tjQAQgL4KpuLBBCBtcglAWr5tZicABKBNndQ8hgAQgJrru6+9EQAC0FfBVDyYAKRNLgFIy7fN7ASAALSpk5rHEIACBGD/kfvj0LNfrbnOWu3tKwc+k5UDASAAy4X66pe+I9ZPvKRVzdY6aGJsY7x8849m2d6JhfnYs2933HTzjpiZmYnJyckz1rH876d/zc7Otlpvr9eL+fn52LVrV1w+sTu2Tl8T42MTrd67VoMIwFqRXP08BIAArL566ngnAShAAOoopcF3ce++X4knDt83+ESrnIEAEIDl0nnT9k/G9OS2VVaRtw1KgAAMStD72xAgAASgTZ3UPIYAEIBi6psAHI7P7317LCwezZaTay55b1y6+c3Z4j9+eE/ct++WbPEJQFb0J4N3QQCWe/y+R38tFnrH8wPPtILFxWPx9LG/zhQ9ggB0WwA2rX/50k3v+YSYtV0AABgGSURBVNnqr4TAr73oXbF5wyuHupRe9OLY8f1xz8M3xB133hbbt2+PiYlz38CeeuM7tnSF22uz4qYr4eVp5ghAG5RDGUMACAABGEqrFR2kCwJQdAKGtLhDzz4cs1/96SFFOzMMAei2AFy77X3xsvOuy1Z/XQ1MALqa+YZ9EwACQAAcDgSgGzVAAO6Nd2/5WLZk/84cASAAwy8/AjB85iMRkQAQAAIwEq2adJEEICneYiYnAARg74FPZ6tHNwB50BOAPNyLj0oACAABKL5Nky+QACRHXEQAAkAACEARrTjURRCAoeIenWAEgAAQgNHp11QrJQCpyJY1LwEgAASgrJ4cxmoIwDAoj2AMAkAACMAINu4aL5kArDHQQqcjAASAABTanAmXRQASwh3lqQkAASAAo9zBa7N2ArA2HEufhQAQAAJQepeu/foIwNozrWJGAkAACEAVrTzQJgjAQPhG5s0EgAAQgJFp1zVbKAFYM5R1TUQACAABqKunV7MbArAaaqP3HgJAAAjA6PXtoCsmAIMSrPT9BIAAEIBKm7uPbRGAPmCN8FACQAAIwAg38CqXTgBWCa72txEAAkAAau/y5v0RgGZGNYwgAASAANTQyf3tgQD0x6szowkAASAAnWn3s26UAHSjBggAASAA3ej1U3dJALqX81Y7JgAEgAC0apWqBxGAqtP74uYIAAEgAN3odQLQvTz3vWMCQAAIQN9tU90bCEB1KV1xQwSAABCAbvQ6AehenvveMQEgAASg77ap7g0EoLqUEoAVCMxMEQAC0I1eJwDdy3PfOyYABIAA9N021b2BAFSXUgJAAM4g8MCTHw0C0I1eJwDdy3PfOyYABIAA9N021b2BAFSXUgJAAAhAN9q6cZc+BNyIqJsDCAABIADd7P1Td00AulEDPgPgESA3AN3odTcA3ctz3zsmAASAAPTdNtW9gQBUl1I3AG4A3AB0o60bd+kGoBFRNwcQAAJAALrZ+24Aupd3NwBuANwAdK/vCUD3ct5qxwSAABCAVq1S9SA3AFWn98XNEQACQAC60eseAepenvveMQEgAASg77ap7g0EoLqUegTII0AeAepGWzfu0g1AI6JuDiAABIAAdLP3PQLUvby7AXAD4Aage31PALqX81Y7JgAEgAC0apWqB7kBqDq9HgF6noA/BObvAHSj0791lwSgi1lvsWcCQAAIQItGqXwIAag8wc9vzw2AGwA3AN3o9VN3SQC6l/NWOyYABIAAtGqVqgcRgKrT6wbADcBJAv4ScDf6/PRdEoBu5r1x1wSAABCAxjapfgABqD7FJzfoBsANgBuAbvS6G4Du5bnvHRMAAkAA+m6b6t5AAKpL6YobIgAEgAB0o9cJQPfy3PeOCQABIAB9t011byAA1aWUAKxAwIeAfQi4G53+rbv0CNBpWZ979pE4cvzxLtbCt+z5wW98LA4e/ctsHC7a9P1xxUt+PFv8E72j8aeP/ctY7D2bbQ2v3PITceH0tdniHzj6F/GX3/h4tvjLgd+0/ZMxPbkt6xqWz4PlcyHX1/j4+tg69bos4QlAFuxDD+oGwA2AG4Cht132gATgtBR86YmPxFcPfjZ7YiwAAQTKEIC9B+5e+pDc7dnSsXHdhXH9FXdliU8AsmAfelACQAAIwNDbLntAAkAAshehBSBwNgIl3AAQgN1x0807YmZmJiYnJ89I1fK/n/41Ozvbqqh7vV7Mz8/Hrl274vKJ3bF1+poYH5to9d61GtTrLcTjh+9dmm5xraYcuXmOnHgy7n/itmzr3rzhiviurTdmi/+WzV+J91/yP7PFv/foNXHP/A9mi/+1Z/5rfH1uT7b4i9/2nuit/5Fs8ZcDHzz6YBw98VS2NVww9dpYP7F5qPEJAAEYasEJhkA/BAhAhBuAfiqm/7EnFufjcw/tiIXesf7f7B1VEPhHF58X//7KS6rYyyhu4kMHdsUfH/n+rEv/4mMfiEcPtfvBRYqFvuHSD8cFU1enmPqscxIAAjDUghMMgX4IEAAC0E+9rGYsAVgNtbreQwDy5pMARBCAubnYuXNnXLnp1tgyddXQr4KXW8BnAPIeBKIjcCoBAkAAUncEAUhNuPz5CUDeHBEAAhBzBCBvF4qOQGEECAABSF2SBCA14fLnJwB5c0QACAAByNuDoiNQHAECQABSFyUBSE24/PkJQN4cEQACQADy9qDoCBRHgAAQgNRFSQBSEy5/fgKQN0cEgAAQgLw9KDoCxREgAAQgdVESgNSEy5+fAOTNEQEgAAQgbw+KjkBxBAgAAUhdlAQgNeHy5ycAeXNEAAgAAcjbg6IjUBwBAkAAUhclAUhNuPz5CUDeHBEAAkAA8vag6AgUR4AAEIDURUkAUhMuf34CkDdHBIAAEIC8PSg6AsURIAAEIHVREoDUhMufnwDkzREBIAAEIG8Pio5AcQQIAAFIXZQEIDXh8ucnAHlzRAAIAAHI24OiI1AcAQJAAFIXJQFITbj8+QlA3hwRAAJAAPL2oOgIFEeAABCA1EVJAFITLn9+ApA3RwSAABCAvD0oOgLFESAABCB1URKA1ITLn58A5M0RASAABCBvD4qOQHEECAABSF2UBCA14fLnJwB5c0QACAAByNuDoiNQHAECQABSFyUBSE24/PkJQN4cEQACQADy9qDoCBRHgAAQgNRFSQBSEy5/fgKQN0cEgAAQgLw9KDoCxREgAAQgdVESgNSEy5+fAOTNEQEgAAQgbw+KjkBxBAgAAUhdlAQgNeHy5ycAeXNEAAgAAcjbg6IjUBwBAkAAUhclAUhNuPz5CUDeHBEAAkAA8vag6AgUR4AAEIDURUkAUhMuf34CkDdHBIAAEIC8PSg6AsURIAAEIHVREoDUhMufnwDkzREBIAAEIG8Pio5AcQQIAAFIXZQEIDXh8ucnAHlzRAAIAAHI24OiI1AcAQJAAFIXJQFITbj8+QlA3hwRAAJAAPL2oOgIFEeAABCA1EVJAFITLn9+ApA3RwSAABCAvD0oOgLFESAABCB1URKA1ITLn58A5M0RASAABCBvD4qOQHEECAABSF2UBCA14fLnJwB5c0QACAAByNuDoiNQHAECQABSFyUBSE24/PkJQN4cEQACQADy9qDoCBRHgAAQgNRFSQBSEy5/fgKQN0cEgAAQgLw9KDoCxREgAAQgdVESgNSEy5+fAOTNEQEgAEUIwOFn/yaOnHgqbzeIjkABBA4cfSAefOq3sq6EABCA1AVIAFITLn9+ApA3RwSAABQhAHnbQHQEyiHw+OE9cd++W7IuiAAQgNQFuNA7Fn/29X8di73jqUOZv1ACf+clU/FLL39JoatLv6xXrNsX29Y9mT7QWSIQAAJAALK1n8AInEmAADzHZO+Bu+OBJ2/PViIb110Y119xV5b4JxbmY8++3XHTzTtiZmYmJicnz1jH8r+f/jU7O9tqvb1eL+bn52PXrl1x+cTu2Dp9TYyPTbR6r0EIILA2BH7m/E/F39/039ZmslXMQgAIAAFYReN4CwKpCBAAAkAAUnWXeREohwABiPjiYx+IRw+1+8FFisy94dIPxwVTV6eY+qxz9qIXx47vj3seviHuuPO22L59e0xMnPsHMKf+wGds6Sc4vTYrbvqJ0PI0c3NzsXPnzrhy062xZeoqPwlqA9YYBBIRIAAEgAAkai7TIlAQAQJAAAhAQQ1pKQjkJkAACAAByN2F4iOQngABIAAEIH2fiYDAyBAgAASAAIxMu1ooAqsmQAAIAAFYdft4IwL1ESAABIAA1NfXdoTA6QQIAAEgAM4FBBB4kQABIAAEwIGAQP0ECAABIAD197kdItCaAAEgAASgdbsYiMDIEiAABIAAjGz7WjgCa0+AABAAArD2fWVGBEojQAAIAAEorSutB4GMBAgAASAAGRtQaASGRIAAEAACMKRmEwaBUSBAAAgAARiFTrVGBAYjQAAIAAEYrIe8G4GqCBAAAkAAqmppm0FgRQIEgAAQAIcDAgi8SIAAEAAC4EBAoH4CBIAAEID6+9wOEWhNgAAQAALQul0MRGBkCRAAAkAARrZ9LRyBtSdAAAgAAVj7vjIjAqURIAAEgACU1pXWg0BGAgSAABCAjA0oNAJDIkAACAABGFKzCYPAKBAgAASAAIxCp1ojAoMRIAAEgAAM1kPejUBVBAgAASAAVbW0zSCwIgECQAAIgMMBAQReJEAACAABcCAgUD8BAkAACED9fW6HCLQmQAAIAAFo3S4GIjCyBAgAASAAI9u+Fo7A2hMgAASAAKx9X5kRgdIIEAACQABK60rrQSAjAQJAAAhAxgYUGoEhESAABIAADKnZhEFgFAgQAAJAAEahU60RgcEIEAACQAAG6yHvRqAqAgSAABCAqlraZhBYkQABIAAEwOGAAAIvEiAABIAAOBAQqJ8AASAABKD+PrdDBFoTIAAEgAC0bhcDERhZAgSAABCAkW1fC0dg7QkQAAJAANa+r8yIQGkECAABIACldaX1IJCRAAEgAAQgYwMKjcCQCBAAAkAAhtRswiAwCgQIAAEgAKPQqdaIwGAECAABIACD9ZB3I1AVAQJAAAhAVS1tMwisSIAAEAACsNQa3zjy53Ho2F7HRMcJTE6OxT98y3SsW5cPxB9/8Vg89MiJbAt45thX4uGnfzdb/OXA37V1Z6yfOD/rGp6a/7N4bO5/ZFvDxnUXxvVX3JUlPgHIgl1QBIZKgAAQAAKw1HJfeuIj8dWDnx1q8wlWHoHzNo3Hn959SUxvHMu2uHf9qwPxqT+czxZf4DIIEIC0eVjsnYi/eeYPY7G3kDaQ2YslcMWl6+KHv3dDtvVNfn081v/NeLb4P7TxT+O1Gx7MFv+9+74n/uDpl2WLvxz4kWd+P54++lfZ1vCGSz8cF0xdPdT4vejFseP7456HbwgCQACGWnwlByMAJWenW2sjAGnzfWJxPj730I5Y6B1LG8jsxRJ42/XTcdstW7Ktb9OfTMbmz09mi5878E99+etx1xOHci8ja3wCMDcXO3fujCs33Rpbpq6K8bGJoSfEDcDQkRcZkAAUmZZOLooApE07AUjLdxRmJwB5s0QAIggAAcjbhaK/SIAAKIZSCBCAtJkgAGn5jsLsBCBvlggAAYg5ApC3C0UnAGqgOAIEIG1KCEBavqMwOwHImyUCQAAIQN4eFP0UAm4AlEMpBAhA2kwQgLR8R2F2ApA3SwSAABCAvD0oOgFQAwUSIABpk0IA0vIdhdkJQN4sEQACQADy9qDoBEANFEiAAKRNCgFIy3cUZicAebNEAAgAAcjbg6ITADVQIAECkDYpBCAt31GYnQDkzRIBIAAEIG8Pik4A1ECBBAhA2qQQgLR8R2F2ApA3SwSAABCAvD0oOgFQAwUSIABpk0IA0vIdhdkJQN4sEQACQADy9qDoBEANFEiAAKRNCgFIy3cUZicAebNEAAgAAcjbg6ITADVQIAECkDYpBCAt31GYnQDkzRIBIAAEIG8Pik4A1ECBBAhA2qQQgLR8R2F2ApA3SwSAABCAvD0oOgFQAwUSIABpk0IA0vIdhdkJQN4sEQACQADy9qDoBEANFEiAAKRNCgFIy3cUZicAebNEAAgAAcjbg6ITADVQIAECkDYpBCAt31GYnQDkzRIBIAAEIG8Pik4A1ECBBAhA2qQQgLR8R2F2ApA3SwSAABCAvD0oOgFQAwUSIABpk0IA0vIdhdkJQN4sEQACQADy9qDoBEANFEiAAKRNCgFIy3cUZicAebNEAAgAAcjbg6ITADVQIAECkDYpBCAt31GYnQDkzRIBIAAEIG8Pik4A1ECBBAhA2qQQgLR8R2F2ApA3SwSAABCAvD0oOgFQAwUSIABpk0IA0vIdhdkJQN4sEQACQADy9qDoBEANFEiAAKRNCgFIy3cUZicAebNEAAgAAcjbg6ITADVQIAECkDYpBCAt31GYnQDkzRIBIAAEIG8Pik4A1ECBBAhA2qQQgLR8R2F2ApA3SwSAABCAvD0oOgFQAwUSIABpk0IA0vIdhdkJQN4sEQACQADy9qDoBEANFEiAAKRNCgFIy3cUZicAebNEAAgAAcjbg6ITADVQIAECkDYpBCAt31GYnQDkzRIBIAAEIG8Pik4A1ECBBAhA2qQQgLR8R2F2ApA3SwSAABCAvD0oOgFQAwUSIABpk0IA0vIdhdkJQN4sEQACUIQAPPjUb8XXnvmDvN0genYC3zY9Hn/4mxfF1IaxbGv5Z79xMP7L7JFs8QUug8CGdVvjb192R5bFnFiYjz37dsdNN++ImZmZmJycPGMdy/9++tfs7Gyr9fZ6vZifn49du3bF5RO7Y+v0NTE+NtHqvWs1iACsFcnRnYcA5M0dASAARQjAc23Qy9sNohdBYCzf9/7PVaEyLKIOylhEnmLsggAs9o7HQwfuisXeQhmptoqhE3jNFZPx1r+9cehxXwi4/msTsf6h8Wzxcwe++8m5uP/ws7mXkTX+ZZvfElOTFw91Db2l73WPHd8f9zx8Q9xx522xffv2mJg49w9gTv2Bz9jST3BafZvS9BOh5Wnm5uZi586dceWmW2PL1FVD/0nQUMkLhgACCBROoAsCUHgKLA8BBBBIQoAAJMFqUgQQQGD0CRCA0c+hHSCAAAIrESAA6gIBBBBAYEUCBEBhIIAAAnUSIAB15tWuEEAAgYEJEICBEZoAAQQQKJIAASgyLRaFAAII5CdAAPLnwAoQQACBFAQIQAqq5kQAAQQqIEAAKkiiLSCAAAIrECAAygIBBBBAYEUCBEBhIIAAAnUSIAB15tWuEEAAgYEJEICBEZoAAQQQKJIAASgyLRaFAAII5CdAAPLnwAoQQACBFAQIQAqq5kQAAQQqIEAAKkiiLSCAAAIrECAAygIBBBBAYEUCBEBhIIAAAnUSIAB15tWuEEAAgYEJEICBEZoAAQQQKJIAASgyLRaFAAII5CdAAPLnwAoQQACBFAQIQAqq5kQAAQQqIEAAKkiiLSCAAAIrECAAygIBBBBAYEUCBEBhIIAAAnUSIAB15tWuEEAAgYEJEICBEZoAAQQQKJIAASgyLRaFAAII5CdAAPLnwAoQQACBFAQIQAqq5kQAAQQqIEAAKkiiLSCAAAIrECAAygIBBBBAYEUCBEBhIIAAAnUSIAB15tWuEEAAgYEJEICBEZoAAQQQKJIAASgyLRaFAAII5CdAAPLnwAoQQACBFAQIQAqq5kQAAQQqIEAAKkiiLSCAAAIrECAAygIBBBBAYEUCBEBhIIAAAnUSIAB15tWuEEAAgYEJEICBEZoAAQQQKJIAASgyLRaFAAII5CdAAPLnwAoQQACBFAQIQAqq5kQAAQQqIEAAKkiiLSCAAAIrECAAygIBBBBAYEUCBEBhIIAAAnUSIAB15tWuEEAAgYEJEICBEZoAAQQQKJIAASgyLRaFAAII5CdAAPLnwAoQQACBFAQIQAqq5kQAAQQqIEAAKkiiLSCAAAIrEChKAA4dOhQ33nhjfOf0r8aWja+OsbEJSUMAAQQQyETgxOKRuG/fr8ZNN/94XHfddTE5OXnGSmZmZs74t9nZ2VYr7vV6MT8/H7t27YrLxn8xLpi6Osad+63YGYQAAggMQuCkAJw4GH/0yM/EHXfeFtu3b4+JiXN/333qeT+2dID32iyg6T8Ii4uLsX///vjZn/3ZGD/28pgcP29p2rE2UxuDAAIIIJCAQC8W4sCRB+Jdu98Zb3zjG2Pjxo1rLgAHDx6Md77zndGbe0VsmLhg6dR37idIpSkRQACBbyGw/M37Yu9YPDr3R3H77bfFd3zHd8S6devOSSmJALzwk6Df+73fixMnTiz99H/pPwNL/+cLAQQQQCAPgeVzefk8fv3rX3/yp0Mr/cdhLW4APvOZz5y8CRgfH8+zUVERQACBDhJ44Yx/29veFhdeeGGeG4Bl7ssLOXLkyMkF+Oa/g5VoywggUByBZQFYv379WX8yNIgAvHDuL3/z/8K57+wvrgQsCAEEKiTwwgM8x48fj6mpqcZv/pcRJLkBqJCtLSGAAALVExhUAKoHZIMIIIBAJQQIQCWJtA0EEEBgUAIEYFCC3o8AAgiMBgECMBp5skoEEEAgOQECkByxAAgggEARBAhAEWmwCAQQQCA/AQKQPwdWgAACCAyDwKoEYBgLEwMBBBBAAAEEEEAAAQTSEmj9dwDSLsPsCCCAAAIIIIAAAgggMAwCBGAYlMVAAAEEEEAAAQQQQKAQAv8f+UiHdTb3/zAAAAAASUVORK5CYII=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pLSfisLKgKsT"
   },
   "outputs": [],
   "source": [
    "# Above grid is defined as below:\n",
    "#   - 0 denotes an navigable tile\n",
    "#   - 1 denotes an obstruction/wall\n",
    "#   - 2 denotes the start state \n",
    "#   - 3 denotes an goal state\n",
    "\n",
    "# Note: Here the upper left corner is defined as (0, 0)\n",
    "#       and lower right corner as (m-1, n-1)\n",
    "\n",
    "# Optimal Path: RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n",
    "\n",
    "\n",
    "GRID_WORLD = np.array([\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1],\n",
    "    [1, 3, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbTHAOARUWoC"
   },
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FZ3Eh_bRkBFt"
   },
   "outputs": [],
   "source": [
    "class Actions(Enum):\n",
    "  UP    = (0, (-1, 0))  # index = 0, (xaxis_move = -1 and yaxis_move = 0)\n",
    "  DOWN  = (1, (1, 0))   # index = 1, (xaxis_move = 1 and yaxis_move = 0) \n",
    "  LEFT  = (2, (0, -1))  # index = 2, (xaxis_move = 0 and yaxis_move = -1)\n",
    "  RIGHT = (3, (0, 1))   # index = 3, (xaxis_move = 0 and yaxis_move = -1)\n",
    "\n",
    "  def get_action_dir(self):\n",
    "    _, direction = self.value\n",
    "    return direction\n",
    "\n",
    "  @property\n",
    "  def index(self):\n",
    "    indx, _ = self.value\n",
    "    return indx\n",
    "\n",
    "  @classmethod\n",
    "  def from_index(cls, index):\n",
    "    action_index_map = {a.index: a for a in cls}\n",
    "    return action_index_map[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDSO8vMfkkdB",
    "outputId": "2951d358-6906-4b65-831b-0bc6524d4177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: UP, action_id: 0, direction_to_move: (-1, 0)\n",
      "name: DOWN, action_id: 1, direction_to_move: (1, 0)\n",
      "name: LEFT, action_id: 2, direction_to_move: (0, -1)\n",
      "name: RIGHT, action_id: 3, direction_to_move: (0, 1)\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "0 index action is: UP\n"
     ]
    }
   ],
   "source": [
    "# How to use Action enum\n",
    "for a in Actions:\n",
    "  print(f\"name: {a.name}, action_id: {a.index}, direction_to_move: {a.get_action_dir()}\")\n",
    "\n",
    "print(\"\\n------------------------------------\\n\")\n",
    "\n",
    "# find action enum from index 0\n",
    "a = Actions.from_index(0)\n",
    "print(f\"0 index action is: {a.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b53SkgJlDIt"
   },
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "u5SyrH9vkepn"
   },
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "  def update(self, *args):\n",
    "    pass\n",
    "\n",
    "  def select_action(self, state_id: int) -> int:\n",
    "    raise NotImplemented\n",
    "\n",
    "\n",
    "class DeterministicPolicy(BasePolicy):\n",
    "  def __init__(self, actions: np.ndarray):\n",
    "    # actions: its a 1d array (|S| size) which contains action for each state\n",
    "    self.actions = actions\n",
    "\n",
    "  def update(self, state_id, action_id):\n",
    "    assert state_id < len(self.actions), f\"Invalid state_id {state_id}\"\n",
    "    assert action_id < len(Actions), f\"Invalid action_id {action_id}\"\n",
    "    self.actions[state_id] = action_id\n",
    "\n",
    "  def select_action(self, state_id: int) -> int:\n",
    "    assert state_id < len(self.actions), f\"Invalid state_id {state_id}\"\n",
    "    return self.actions[state_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_4I7x4rMlFgp"
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Zt-gPaPOldH0"
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "  def __init__(self, grid):\n",
    "    self.grid = grid\n",
    "    m, n = grid.shape\n",
    "    self.num_states = m*n\n",
    "\n",
    "  def xy_to_posid(self, x: int, y: int):\n",
    "    _, n = self.grid.shape\n",
    "    return x*n + y\n",
    "\n",
    "  def posid_to_xy(self, posid: int):\n",
    "    _, n = self.grid.shape\n",
    "    return (posid // n, posid % n)\n",
    "\n",
    "  def isvalid_move(self, x: int, y: int):\n",
    "    m, n = self.grid.shape\n",
    "    return (x >= 0) and (y >= 0) and (x < m) and (y < n) and (self.grid[x, y] != 1)\n",
    "\n",
    "  def find_start_xy(self) -> int:\n",
    "    m, n = self.grid.shape\n",
    "    for x in range(m):\n",
    "      for y in range(n):\n",
    "        if self.grid[x, y] == 2:\n",
    "          return (x, y)\n",
    "    raise Exception(\"Start position not found.\")\n",
    "\n",
    "  def find_path(self, policy: BasePolicy) -> str:\n",
    "    max_steps = 50\n",
    "    steps = 0\n",
    "\n",
    "    P, R = self.get_transition_prob_and_expected_reward()\n",
    "    num_actions, num_states = R.shape\n",
    "    all_possible_state_posids = np.arange(num_states)\n",
    "\n",
    "    path = \"\"\n",
    "    curr_x, curr_y = self.find_start_xy()\n",
    "    while (self.grid[curr_x, curr_y] != 3) and (steps < max_steps):\n",
    "      curr_posid = self.xy_to_posid(curr_x, curr_y)\n",
    "      action_id = policy.select_action(curr_posid)\n",
    "      next_posid = np.random.choice(\n",
    "          all_possible_state_posids, p=P[action_id, curr_posid])\n",
    "      action = Actions.from_index(action_id)\n",
    "      path += f\" {action.name}\"\n",
    "      curr_x, curr_y = self.posid_to_xy(next_posid)\n",
    "      steps += 1\n",
    "    return path\n",
    "\n",
    "  def get_transition_prob_and_expected_reward(self):  # P(s_next | s, a), R(s, a)\n",
    "    m, n = self.grid.shape\n",
    "    num_states = m*n\n",
    "    num_actions = len(Actions)\n",
    "    P = np.zeros((num_actions, num_states, num_states))\n",
    "    R = np.zeros((num_actions, num_states))\n",
    "    for a in Actions:\n",
    "      for x in range(m):\n",
    "        for y in range(n):\n",
    "          xmove_dir, ymove_dir = a.get_action_dir()\n",
    "          xnew, ynew = x + xmove_dir, y + ymove_dir  # find the new co-ordinate after the action a\n",
    "\n",
    "          posid = self.xy_to_posid(x, y)\n",
    "          new_posid = self.xy_to_posid(xnew, ynew)\n",
    "  \n",
    "          \n",
    "          if self.grid[x, y] == 3:\n",
    "            # the current state is a goal state\n",
    "            P[a.index, posid, posid] = 1\n",
    "            R[a.index, posid] = 0\n",
    "          elif (self.grid[x, y] == 1) or (not self.isvalid_move(xnew, ynew)):\n",
    "            # the current state is a block state or the next state is invalid\n",
    "            P[a.index, posid, posid] = 1\n",
    "            R[a.index, posid] = -1\n",
    "          else:\n",
    "            # action a is valid and goes to a new position\n",
    "            P[a.index, posid, new_posid] = 1\n",
    "            R[a.index, posid] = -1\n",
    "    return P, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXLPFU_DUJMw"
   },
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vi5e3FAHKoZY"
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(P: np.ndarray, R: np.ndarray, gamma: float,\n",
    "                      policy: BasePolicy, theta: float,\n",
    "                      init_V: np.ndarray=None):\n",
    "  num_actions, num_states = R.shape\n",
    "\n",
    "  # Please try different starting point for V you will find it will always\n",
    "  # converge to the same V_pi value.\n",
    "  if init_V is None:\n",
    "    init_V = np.zeros(num_states)\n",
    "  V = copy.deepcopy(init_V)\n",
    "\n",
    "  delta = 100.0\n",
    "  while delta > theta:\n",
    "    delta = 0.0\n",
    "    for state_id in range(num_states):\n",
    "      action_id = policy.select_action(state_id)\n",
    "      v_old = V[state_id]\n",
    "      # Following equation is a different way of writing the same equation given in the slide.\n",
    "      # Note here R is an expected reward term.\n",
    "      V[state_id] = R[action_id, state_id] + gamma * np.dot(P[action_id, state_id], V)\n",
    "      delta = max(delta, abs(V[state_id] - v_old))\n",
    "  return V\n",
    "\n",
    "\n",
    "def policy_improvement(P: np.ndarray, R: np.ndarray, gamma: float,\n",
    "                       policy: BasePolicy, V: np.ndarray):\n",
    "  num_actions, num_states = R.shape\n",
    "  policy_stable = True\n",
    "  for state_id in range(num_states):\n",
    "    old_action_id = policy.select_action(state_id)\n",
    "\n",
    "    # your code here\n",
    "    q_sa = np.zeros(num_actions)\n",
    "    for a in Actions:\n",
    "        q_sa[a.index] = R[a.index, state_id] + gamma * np.dot(P[a.index, state_id], V)\n",
    "    new_action_id = np.argmax(q_sa) # update new_action_id based on the value function.\n",
    "    \n",
    "\n",
    "    policy.update(state_id, new_action_id)\n",
    "    if old_action_id != new_action_id:\n",
    "      policy_stable = False\n",
    "  return policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration(P: np.ndarray, R: np.ndarray, gamma: float,\n",
    "                     theta: float=1e-3, init_policy: BasePolicy = None):\n",
    "  num_actions, num_states = R.shape\n",
    "\n",
    "  # Please try exploring different policies you will find it will always\n",
    "  # converge to the same optimal policy for valid states.\n",
    "  if init_policy is None:\n",
    "    # Say initial policy = all up actions.\n",
    "    init_policy = DeterministicPolicy(actions=np.zeros(num_states, dtype=int))\n",
    "\n",
    "  # creating a copy of a initial policy\n",
    "  policy = copy.deepcopy(init_policy)\n",
    "  policy_stable = False\n",
    "  while not policy_stable:\n",
    "    V = policy_evaluation(P, R, gamma, policy, theta)\n",
    "    policy_stable = policy_improvement(P, R, gamma, policy, V)\n",
    "  return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-x42roWUN5Q"
   },
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jiR7zHODUH5U"
   },
   "outputs": [],
   "source": [
    "# Directly find the optimal value function\n",
    "def get_optimal_value(P: np.ndarray, R: np.ndarray, gamma: float,\n",
    "                      theta: float, init_V: np.ndarray=None):\n",
    "  num_actions, num_states = R.shape\n",
    "\n",
    "  # Please try different starting point for V you will find it will always\n",
    "  # converge to the same V_star value.\n",
    "  if init_V is None:\n",
    "    init_V = np.zeros(num_states)\n",
    "  V = copy.deepcopy(init_V)\n",
    "\n",
    "  delta = 100.0\n",
    "  while delta > theta:\n",
    "    delta = 0.0\n",
    "    for state_id in range(num_states):\n",
    "      v_old = V[state_id]\n",
    "      q_sa = np.zeros(num_actions)\n",
    "      for a in Actions:\n",
    "        q_sa[a.index] = R[a.index, state_id] + gamma * np.dot(P[a.index, state_id], V)\n",
    "      V[state_id] = np.max(q_sa)\n",
    "      delta = max(delta, abs(V[state_id] - v_old))\n",
    "  return V\n",
    "\n",
    "\n",
    "def value_iteration(P: np.ndarray, R: np.ndarray, gamma: float,\n",
    "                    theta: float=1e-3, init_V: np.ndarray=None):\n",
    "  V_star = get_optimal_value(P, R, gamma, theta, init_V)\n",
    "\n",
    "  num_actions, num_states = R.shape\n",
    "  policy = DeterministicPolicy(actions=np.zeros(num_states, dtype=int))\n",
    "  for state_id in range(num_states):\n",
    "    # Your code here\n",
    "    q_sa = np.zeros(num_actions)\n",
    "    for a in Actions:\n",
    "        q_sa[a.index] = R[a.index, state_id] + gamma * np.dot(P[a.index, state_id], V_star)\n",
    "    action_id = np.argmax(q_sa) # update the action_id based on V_star\n",
    "    \n",
    "    policy.update(state_id, action_id)\n",
    "  \n",
    "  return policy, V_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn0HFryxUd45"
   },
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-7s2t_ttobMJ"
   },
   "outputs": [],
   "source": [
    "def is_same_optimal_value(V1, V2, diff_theta=1e-3):\n",
    "  diff = np.abs(V1 - V2)\n",
    "  return np.all(diff < diff_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DFP678PtlhBI"
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "gamma = 0.9\n",
    "theta = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mokqMJvA7tPW"
   },
   "outputs": [],
   "source": [
    "env = Environment(GRID_WORLD)\n",
    "P, R = env.get_transition_prob_and_expected_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBOiuj8nlYNB"
   },
   "source": [
    "#### Exp 1: Using Policy iteration algorithm find the optimal path from start to goal position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BT6iIL3KPxGM",
    "outputId": "e3e57c4e-c471-4d36-b9ae-3d35eda204ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n"
     ]
    }
   ],
   "source": [
    "# # Start with random choice of init_policy.\n",
    "# One such choice could be: init_policy = np.ones(env.num_states, dtype=int)\n",
    "# Start with your own choice of init_policy\n",
    "init_policy = DeterministicPolicy(actions=np.ones(env.num_states, dtype=int))\n",
    "\n",
    "pitr_policy, pitr_V_star = policy_iteration(P, R, gamma, theta=theta, init_policy=init_policy)\n",
    "pitr_path = env.find_path(pitr_policy)\n",
    "print(pitr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjTyDdIHnXZI"
   },
   "source": [
    "#### Exp 2: Using value iteration algorithm find the optimal path from start to goal position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CrEinOOQcaO",
    "outputId": "6e04b9a5-58ad-41d9-a47c-ac4bcbcad893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n"
     ]
    }
   ],
   "source": [
    "vitr_policy, vitr_V_star = value_iteration(P, R, gamma, theta=theta)\n",
    "vitr_path = env.find_path(vitr_policy)\n",
    "print(vitr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4GU52_tn1Or"
   },
   "source": [
    "#### Exp 3: Compare the optimal value function of policy iteration and value iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNFAuiC2cbgh",
    "outputId": "1eaeb645-6fa3-4a3c-8980-7e86a20885b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same_optimal_value(pitr_V_star, vitr_V_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPAR8sVfpg5x"
   },
   "source": [
    "#### Exp 4: Using initial guess for V as random values, find the optimal value function using policy evaluation and compare it with the optimal value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKhx-N2ipe6M",
    "outputId": "d442901b-322f-4777-db1e-3470e39374fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with random choice of init_V.\n",
    "# One such choice could be: init_V = np.random.randn(env.num_states)\n",
    "# Another choice could be: init_V = 10*np.ones(env.num_states)\n",
    "# Start with your own choice of init_V\n",
    "init_V = 10*np.ones(env.num_states) # your choice\n",
    "\n",
    "V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)\n",
    "is_same_optimal_value(pitr_V_star, V_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "himnwZEWsFgp"
   },
   "source": [
    "#### Exp 5: Using initial guess for V as random values, find the optimal value function using get_optimal_value and compare it with the optimal value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QUhuKKZqp2W",
    "outputId": "5a3a3d09-6e37-41b0-98b5-d39a317ddb87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with random choice.\n",
    "# One such choice could be: init_V = np.random.randn(env.num_states)\n",
    "# Another choice could be: init_V = 10*np.ones(env.num_states)\n",
    "# Start with your own choice of init_V\n",
    "init_V = 100*np.ones(env.num_states)\n",
    "\n",
    "V_star = get_optimal_value(P, R, gamma, theta, init_V)\n",
    "is_same_optimal_value(vitr_V_star, V_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq-42ZJAtOoo"
   },
   "source": [
    "#### Exp Optional: Try changing the grid by adding multiple paths to the goal state and check if our policy_iteration or value_iteration algorithm is able to find optimal path. Redo the above experiments. \n",
    "\n",
    "- 1 way to add another path would be GRID_WORLD[4, 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6ZAaplqMuOUE"
   },
   "outputs": [],
   "source": [
    "GRID_WORLD[4, 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n"
     ]
    }
   ],
   "source": [
    "# # Start with random choice of init_policy.\n",
    "# One such choice could be: init_policy = np.ones(env.num_states, dtype=int)\n",
    "# Start with your own choice of init_policy\n",
    "init_policy = DeterministicPolicy(actions=np.ones(env.num_states, dtype=int))\n",
    "\n",
    "pitr_policy, pitr_V_star = policy_iteration(P, R, gamma, theta=theta, init_policy=init_policy)\n",
    "pitr_path = env.find_path(pitr_policy)\n",
    "print(pitr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RIGHT RIGHT UP UP LEFT LEFT UP UP UP UP UP UP LEFT LEFT DOWN DOWN LEFT LEFT\n"
     ]
    }
   ],
   "source": [
    "vitr_policy, vitr_V_star = value_iteration(P, R, gamma, theta=theta)\n",
    "vitr_path = env.find_path(vitr_policy)\n",
    "print(vitr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same_optimal_value(pitr_V_star, vitr_V_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with random choice of init_V.\n",
    "# One such choice could be: init_V = np.random.randn(env.num_states)\n",
    "# Another choice could be: init_V = 10*np.ones(env.num_states)\n",
    "# Start with your own choice of init_V\n",
    "init_V = 10*np.ones(env.num_states) # your choice\n",
    "\n",
    "V_star = policy_evaluation(P, R, gamma, pitr_policy, theta, init_V)\n",
    "is_same_optimal_value(pitr_V_star, V_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with random choice.\n",
    "# One such choice could be: init_V = np.random.randn(env.num_states)\n",
    "# Another choice could be: init_V = 10*np.ones(env.num_states)\n",
    "# Start with your own choice of init_V\n",
    "init_V = 100*np.ones(env.num_states)\n",
    "\n",
    "V_star = get_optimal_value(P, R, gamma, theta, init_V)\n",
    "is_same_optimal_value(vitr_V_star, V_star)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_4I7x4rMlFgp"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
