{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn7PKu9r0asK"
   },
   "source": [
    "# **Tutorial 5 - DQN and Actor-Critic**\n",
    "\n",
    "Please follow this tutorial to understand the structure (code) of DQNs & get familiar with Actor Critic methods.\n",
    "\n",
    "\n",
    "### References:\n",
    "\n",
    "Please follow [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) for the original publication as well as the psuedocode. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed. \n",
    "\n",
    "\n",
    "## **Part 1: DQN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "azUjb7UK4Yfh",
    "outputId": "d87938c0-21ae-420d-eb91-4248fdbf5e87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (57.4.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-67.4.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 57.4.0\n",
      "    Uninstalling setuptools-57.4.0:\n",
      "      Successfully uninstalled setuptools-57.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
      "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed setuptools-67.4.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (6.0.0)\n",
      "Collecting pygame==2.1.0\n",
      "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.15.0)\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Installing packages for rendering the game on Colab\n",
    "'''\n",
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1\n",
    "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
    "!pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "P_DODRgW_ZKS"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A bunch of imports, you don't have to worry about these\n",
    "'''\n",
    "import scipy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple, deque\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import gym\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "import tensorflow as tf\n",
    "from IPython import display as ipythondisplay\n",
    "from PIL import Image\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fYNA5kiH_esJ",
    "outputId": "045e4536-9dd0-454b-9811-42a8ea228b47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "1\n",
      "----\n",
      "[ 0.01369617 -0.02302133 -0.04590265 -0.04834723]\n",
      "----\n",
      "1\n",
      "----\n",
      "[ 0.01323574  0.17272775 -0.04686959 -0.3551522 ]\n",
      "1.0\n",
      "False\n",
      "{}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Please refer to the first tutorial for more details on the specifics of environments\n",
    "We've only added important commands you might find useful for experiments.\n",
    "'''\n",
    "\n",
    "'''\n",
    "List of example environments\n",
    "(Source - https://gym.openai.com/envs/#classic_control)\n",
    "\n",
    "'Acrobot-v1'\n",
    "'Cartpole-v1'\n",
    "'MountainCar-v0'\n",
    "'''\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(0)\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "no_of_actions = env.action_space.n\n",
    "\n",
    "print(state_shape)\n",
    "print(no_of_actions)\n",
    "print(env.action_space.sample())\n",
    "print(\"----\")\n",
    "\n",
    "'''\n",
    "# Understanding State, Action, Reward Dynamics\n",
    "\n",
    "The agent decides an action to take depending on the state.\n",
    "\n",
    "The Environment keeps a variable specifically for the current state.\n",
    "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
    "- It returns the new current state and reward for the agent to take the next action\n",
    "\n",
    "'''\n",
    "\n",
    "state = env.reset()   \n",
    "''' This returns the initial state (when environment is reset) '''\n",
    "\n",
    "print(state)\n",
    "print(\"----\")\n",
    "\n",
    "action = env.action_space.sample()  \n",
    "''' We take a random action now '''\n",
    "\n",
    "print(action)\n",
    "print(\"----\")\n",
    "\n",
    "next_state, reward, done, info = env.step(action) \n",
    "''' env.step is used to calculate new state and obtain reward based on old state and action taken  ''' \n",
    "\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(info)\n",
    "print(\"----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apuaOxavDXus"
   },
   "source": [
    "## DQN\n",
    "\n",
    "Using NNs as substitutes isn't something new. It has been tried earlier, but the 'human control' paper really popularised using NNs by providing a few stability ideas (Q-Targets, Experience Replay & Truncation). The 'Deep-Q Network' (DQN) Algorithm can be broken down into having the following components. \n",
    "\n",
    "### Q-Network:\n",
    "The neural network used as a function approximator is defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "g4MRC1p2DZbp"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "### Q Network & Some 'hyperparameters'\n",
    "\n",
    "QNetwork1:\n",
    "Input Layer - 4 nodes (State Shape) \\\n",
    "Hidden Layer 1 - 64 nodes \\\n",
    "Hidden Layer 2 - 64 nodes \\\n",
    "Output Layer - 2 nodes (Action Space) \\\n",
    "Optimizer - zero_grad()\n",
    "\n",
    "QNetwork2: Feel free to experiment more\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "'''\n",
    "Bunch of Hyper parameters (Which you might have to tune later **wink wink**)\n",
    "'''\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size \n",
    "BATCH_SIZE = 64         # minibatch size \n",
    "GAMMA = 0.99            # discount factor\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 20       # how often to update the network (When Q target is present) \n",
    "\n",
    "\n",
    "class QNetwork1(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork1, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmv5c0XoK8GA"
   },
   "source": [
    "### Replay Buffer:\n",
    "\n",
    "This is a 'deque' that helps us store experiences. Recall why we use such a technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bh_oghc7Ledh"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8VJYkqoLqlO"
   },
   "source": [
    "## Truncation:\n",
    "We add a line (optionally) in the code to truncate the gradient in hopes that it would help with the stability of the learning process.\n",
    "\n",
    "## Tutorial Agent Code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ok_5eQM7OCTj"
   },
   "outputs": [],
   "source": [
    "class TutorialAgent():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        ''' Agent Environment Interaction '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        ''' Q-Network '''\n",
    "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        ''' Replay memory '''\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        ''' Save experience in replay memory '''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        ''' If enough samples are available in memory, get random subset and learn '''\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
    "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''      \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        ''' Epsilon-greedy action selection (Already Present) '''\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ''' Get max predicted Q values (for next states) from target model'''\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ''' Compute Q targets for current states '''\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        ''' Get expected Q values from local model '''\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ''' Compute loss '''\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        ''' Minimize the loss '''\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        ''' Gradiant Clipping '''\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SQFbRCHWQyO"
   },
   "source": [
    "### Here, we present the DQN algorithm code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "r6A2TdUHWVUN",
    "outputId": "63f5742b-355b-4baa-94d2-ce26915a9a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 38.24\n",
      "Episode 200\tAverage Score: 144.32\n",
      "Episode 231\tAverage Score: 195.80\n",
      "Environment solved in 131 episodes!\tAverage Score: 195.80\n",
      "0:02:04.740842\n"
     ]
    }
   ],
   "source": [
    "''' Defining DQN Algorithm '''\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "def dqn(n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "\n",
    "    scores = []                 \n",
    "    ''' list containing scores from each episode '''\n",
    "\n",
    "    scores_window_printing = deque(maxlen=10) \n",
    "    ''' For printing in the graph '''\n",
    "    \n",
    "    scores_window= deque(maxlen=100)  \n",
    "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
    "\n",
    "    eps = eps_start                    \n",
    "    ''' initialize epsilon '''\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        scores_window.append(score)       \n",
    "        scores_window_printing.append(score)   \n",
    "        ''' save most recent score '''           \n",
    "\n",
    "        eps = max(eps_end, eps_decay*eps) \n",
    "        ''' decrease epsilon '''\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
    "        if i_episode % 10 == 0: \n",
    "            scores.append(np.mean(scores_window_printing))        \n",
    "        if i_episode % 100 == 0: \n",
    "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "           break\n",
    "    return [np.array(scores),i_episode-100]\n",
    "\n",
    "''' Trial run to check if algorithm runs and saves the data '''\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
    "\n",
    "\n",
    "dqn()\n",
    "\n",
    "\n",
    "time_taken = datetime.datetime.now() - begin_time\n",
    "\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL9YMq9yPHLk"
   },
   "source": [
    "### **Task 1a**  \n",
    "Understand the core of the algorithm, follow the flow of data. Identify the exploration strategy used.\n",
    "### **Task 1b**\n",
    "Out of the two exploration strategies discussed in class ($ϵ$-greedy & Softmax). Implement the strategy that's not used here. \n",
    "### **Task 1c**\n",
    "How fast does the agent 'solve' the environment in terms of the number of episodes?\n",
    "(Cartpole-v1 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials)\n",
    "\n",
    "How 'well' does the agent learn? (reward plot?) The above two are some 'evaluation metrics' you can use to comment on the performance of an algorithm.\n",
    "\n",
    "Please compare DQN (using $\\epsilon$-greedy) with DQN (using softmax). Think along the lines of 'no. of episodes', 'reward plots', 'compute time', etc. and add a few comments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LBh6_lOVBdN"
   },
   "source": [
    "#### **Submission Steps**\n",
    "\n",
    "#### Task 1: Add a text cell with the answer.\n",
    "\n",
    "#### Task 2: Add a code cell below task 1 solution and use 'Tutorial Agent Code' to build your new agent (with a different exploration strategy).\n",
    "\n",
    "#### Task 3: Add a code cell below task 2 solution running both the agents to solve the CartPole v-1 environment and add a new text cell below it with your inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m45-HmxzZ2IX"
   },
   "source": [
    "### Task 1\n",
    "The Exploration Strategy used in the algorithm is the **$ϵ$-greedy** strategy. As part of the second task we implement the **softmax** strategy for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoEJMeUfoEkN"
   },
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7rUF-HVTZ1MJ"
   },
   "outputs": [],
   "source": [
    "class CustomAgent():\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        ''' Agent Environment Interaction '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        ''' Q-Network '''\n",
    "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        ''' Replay memory '''\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        ''' Save experience in replay memory '''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        ''' If enough samples are available in memory, get random subset and learn '''\n",
    "        if len(self.memory) >= BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
    "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''      \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "\n",
    "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
    "\n",
    "    def act(self, state, beta=1.):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        ''' Softmax action selection '''\n",
    "        action_values = action_values/beta\n",
    "        smx_probs = scipy.special.softmax(action_values.flatten().cpu().data.numpy())\n",
    "        smx_probs /= sum(smx_probs)\n",
    "        return np.random.choice(np.arange(self.action_size), p = smx_probs)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        ''' Get max predicted Q values (for next states) from target model'''\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        ''' Compute Q targets for current states '''\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        ''' Get expected Q values from local model '''\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "\n",
    "        ''' Compute loss '''\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        ''' Minimize the loss '''\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        ''' Gradiant Clipping '''\n",
    "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        self.optimizer.step()\n",
    "\n",
    "''' Defining DQN Algorithm '''\n",
    "\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "def dqn_soft(n_episodes=10000, max_t=1000, beta_start=5.0, beta_end=0.05, beta_decay=0.995):\n",
    "\n",
    "    scores = []                 \n",
    "    ''' list containing scores from each episode '''\n",
    "\n",
    "    scores_window_printing = deque(maxlen=10) \n",
    "    ''' For printing in the graph '''\n",
    "    \n",
    "    scores_window= deque(maxlen=100)  \n",
    "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
    "\n",
    "    beta = beta_start                    \n",
    "    ''' initialize epsilon '''\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, beta)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        scores_window.append(score)       \n",
    "        scores_window_printing.append(score)   \n",
    "        ''' save most recent score '''           \n",
    "\n",
    "        beta = max(beta_end, beta_decay*beta) \n",
    "        ''' decrease epsilon '''\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
    "        if i_episode % 10 == 0: \n",
    "            scores.append(np.mean(scores_window_printing))        \n",
    "        if i_episode % 100 == 0: \n",
    "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=195.0:\n",
    "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "           break\n",
    "    return [np.array(scores),i_episode-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "BwM14UkrZ1is",
    "outputId": "fd9325a1-5ba9-4562-d18e-deede05c10fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 116.56\n",
      "Episode 200\tAverage Score: 53.77\n",
      "Episode 300\tAverage Score: 10.27\n",
      "Episode 400\tAverage Score: 30.39\n",
      "Episode 500\tAverage Score: 11.96\n",
      "Episode 593\tAverage Score: 196.16\n",
      "Environment solved in 493 episodes!\tAverage Score: 196.16\n",
      "0:02:53.712233\n"
     ]
    }
   ],
   "source": [
    "''' Trial run to check if algorithm runs and saves the data '''\n",
    "begin_time = datetime.datetime.now()\n",
    "agent = CustomAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
    "\n",
    "\n",
    "dqn_soft()\n",
    "\n",
    "\n",
    "time_taken = datetime.datetime.now() - begin_time\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9dmUAbZaKhe"
   },
   "source": [
    "### Task 3\n",
    "\n",
    "Both DQN-epsilon and DQN-softmax perform similarly. However over many runs the main difference noticed was that when the ϵ-greedy strategy is used the training is more jittery. The softmax strategy on the other hand is comparatively much smoother and offers much better localisation of choice on the more probable outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zt6xX0njbJvS"
   },
   "source": [
    "## **Part 2: One-Step Actor-Critic Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9WWiiNxhM7x"
   },
   "source": [
    "**Actor-Critic methods** learn both a policy $\\pi(a|s;\\theta)$ and a state-value function $v(s;w)$ simultaneously. The policy is referred to as the actor that suggests actions given a state. The estimated value function is referred to as the critic. It evaluates actions taken by the actor based on the given policy. In this exercise, both functions are approximated by feedforward neural networks. \n",
    "\n",
    "- The policy network is parametrized by $\\theta$ - it takes a state $s$ as input and outputs the probabilities $\\pi(a|s;\\theta)\\ \\forall\\ a$\n",
    "- The value network is parametrized by $w$ - it takes a state $s$ as input and outputs a scalar value associated with the state, i.e., $v(s;w)$\n",
    "- The single step TD error can be defined as follows:\n",
    "$$\\delta_t  = R_{t+1} + \\gamma v(s_{t+1};w) - v(s_t;w)$$\n",
    "- The loss function to be minimized at every step ($L_{tot}^{(t)}$) is a summation of two terms, as follows:\n",
    "$$L_{tot}^{(t)} = L_{actor}^{(t)} + L_{critic}^{(t)}$$\n",
    "where,\n",
    "$$L_{actor}^{(t)} = -\\log\\pi(a_t|s_t; \\theta)\\delta_t$$\n",
    "$$L_{critic}^{(t)} = \\delta_t^2$$\n",
    "- **NOTE: Here, weights of the first two hidden layers are shared by the policy and the value network**\n",
    "    - First two hidden layer sizes: [1024, 512]\n",
    "    - Output size of policy network: 2 (Softmax activation)\n",
    "    - Output size of value network: 1 (Linear activation)\n",
    "\n",
    "<!-- $$\\pi(a|s;\\theta) = \\phi_{\\theta}(a,s)$$ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eU3_IeYwN3H"
   },
   "source": [
    "### Initializing Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MXFHQcJjVKYu"
   },
   "outputs": [],
   "source": [
    "class ActorCriticModel(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Defining policy and value networkss\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, n_hidden1=1024, n_hidden2=512):\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "\n",
    "        #Hidden Layer 1\n",
    "        self.fc1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
    "        #Hidden Layer 2\n",
    "        self.fc2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
    "        \n",
    "        #Output Layer for policy\n",
    "        self.pi_out = tf.keras.layers.Dense(action_size, activation='softmax')\n",
    "        #Output Layer for state-value\n",
    "        self.v_out = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state):\n",
    "        \"\"\"\n",
    "        Computes policy distribution and state-value for a given state\n",
    "        \"\"\"\n",
    "        layer1 = self.fc1(state)\n",
    "        layer2 = self.fc2(layer1)\n",
    "\n",
    "        pi = self.pi_out(layer2)\n",
    "        v = self.v_out(layer2)\n",
    "\n",
    "        return pi, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40uV1hrewVnA"
   },
   "source": [
    "### Agent Class\n",
    "###**Task 2a:** Write code to compute $\\delta_t$ inside the Agent.learn() function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "9RNpp9WMfkTE"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Agent class\n",
    "    \"\"\"\n",
    "    def __init__(self, action_size, lr=0.001, gamma=0.99, seed = 85):\n",
    "        self.gamma = gamma\n",
    "        self.ac_model = ActorCriticModel(action_size=action_size)\n",
    "        self.ac_model.compile(tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"\n",
    "        Given a state, compute the policy distribution over all actions and sample one action\n",
    "        \"\"\"\n",
    "        pi,_ = self.ac_model(state)\n",
    "\n",
    "        action_probabilities = tfp.distributions.Categorical(probs=pi)\n",
    "        sample = action_probabilities.sample()\n",
    "\n",
    "        return int(sample.numpy()[0])\n",
    "\n",
    "    def actor_loss(self, action, pi, delta):\n",
    "        \"\"\"\n",
    "        Compute Actor Loss\n",
    "        \"\"\"\n",
    "        return -tf.math.log(pi[0,action]) * delta\n",
    "\n",
    "    def critic_loss(self,delta):\n",
    "        \"\"\"\n",
    "        Critic loss aims to minimize TD error\n",
    "        \"\"\"\n",
    "        return delta**2\n",
    "\n",
    "    @tf.function\n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        For a given transition (s,a,s',r) update the paramters by computing the\n",
    "        gradient of the total loss\n",
    "        \"\"\"\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            pi, V_s = self.ac_model(state)\n",
    "            _, V_s_next = self.ac_model(next_state)\n",
    "\n",
    "            V_s = tf.squeeze(V_s)\n",
    "            V_s_next = tf.squeeze(V_s_next)\n",
    "            \n",
    "\n",
    "            #### TO DO: Write the equation for delta (TD error)\n",
    "            ## Write code below\n",
    "            delta = reward + self.gamma*V_s_next - V_s\n",
    "            loss_a = self.actor_loss(action, pi, delta)\n",
    "            loss_c =self.critic_loss(delta)\n",
    "            loss_total = loss_a + loss_c\n",
    "\n",
    "        gradient = tape.gradient(loss_total, self.ac_model.trainable_variables)\n",
    "        self.ac_model.optimizer.apply_gradients(zip(gradient, self.ac_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUJwznIzwBIX"
   },
   "source": [
    "### Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Q0SB0o_OfyGN",
    "outputId": "456d080d-c854-4c27-8058-830fa0a214b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  10 Reward 58.000000 Average Reward 38.300000\n",
      "Episode  20 Reward 81.000000 Average Reward 53.300000\n",
      "Episode  30 Reward 78.000000 Average Reward 64.300000\n",
      "Episode  40 Reward 175.000000 Average Reward 81.100000\n",
      "Episode  50 Reward 66.000000 Average Reward 84.400000\n",
      "Episode  60 Reward 78.000000 Average Reward 98.500000\n",
      "Episode  70 Reward 128.000000 Average Reward 98.800000\n",
      "Episode  80 Reward 72.000000 Average Reward 76.800000\n",
      "Episode  90 Reward 65.000000 Average Reward 82.500000\n",
      "Episode  100 Reward 68.000000 Average Reward 94.100000\n",
      "Episode  110 Reward 46.000000 Average Reward 75.300000\n",
      "Episode  120 Reward 172.000000 Average Reward 140.700000\n",
      "Episode  130 Reward 500.000000 Average Reward 173.400000\n",
      "Episode  140 Reward 332.000000 Average Reward 235.900000\n",
      "Episode  150 Reward 87.000000 Average Reward 88.100000\n",
      "Episode  160 Reward 71.000000 Average Reward 90.900000\n",
      "Episode  170 Reward 228.000000 Average Reward 157.100000\n",
      "Episode  180 Reward 500.000000 Average Reward 500.000000\n",
      "Stopped at Episode  89\n",
      "0:04:44.305476\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "#Initializing Agent\n",
    "agent = Agent(lr=1e-4, action_size=env.action_space.n)\n",
    "#Number of episodes\n",
    "episodes = 1800\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "reward_list = []\n",
    "average_reward_list = []\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "for ep in range(1, episodes + 1):\n",
    "    state = env.reset().reshape(1,-1)\n",
    "    done = False\n",
    "    ep_rew = 0\n",
    "    while not done:\n",
    "        action = agent.sample_action(state) ##Sample Action\n",
    "        next_state, reward, done, info = env.step(action) ##Take action\n",
    "        next_state = next_state.reshape(1,-1)\n",
    "        ep_rew += reward  ##Updating episode reward\n",
    "        agent.learn(state, action, reward, next_state, done) ##Update Parameters\n",
    "        state = next_state ##Updating State\n",
    "    reward_list.append(ep_rew)\n",
    "\n",
    "    if ep % 10 == 0:\n",
    "        avg_rew = np.mean(reward_list[-10:])\n",
    "        print('Episode ', ep, 'Reward %f' % ep_rew, 'Average Reward %f' % avg_rew)\n",
    "\n",
    "    if ep % 100:\n",
    "        avg_100 =  np.mean(reward_list[-100:])\n",
    "        if avg_100 > 195.0:\n",
    "            print('Stopped at Episode ',ep-100)\n",
    "            break\n",
    "\n",
    "time_taken = datetime.datetime.now() - begin_time\n",
    "print(time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twYwsX7Hu1V1"
   },
   "source": [
    "### **Task 2b**: Plot total reward curve\n",
    "In the cell below, write code to plot the total reward averaged over 100 episodes (moving average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "wOZzkLIgvHgS",
    "outputId": "6892ae5a-4af4-4798-c75a-2dca6ad2722d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Plot of total reward vs episode\n",
    "## Write Code Below\n",
    "\n",
    "avg_rew = [np.mean(reward_list[max(0,i-100):i]) for i in range(1,len(reward_list)+1)]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(avg_rew)), avg_rew, 'b')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Moving Avg Reward')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjC1dxbuuNL2"
   },
   "source": [
    "### Code for rendering ([source](https://colab.research.google.com/drive/1D6bvoEVukil7DUaJU465vtfuDgLDbOY7#scrollTo=qbIMMkfmRHyC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "D2cisMBqgg9Y",
    "outputId": "d1b652c8-0586-4647-ba5c-8d8c5c5d146b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Render an episode and save as a GIF file\n",
    "\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "\n",
    "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
    "  screen = env.render(mode='rgb_array')\n",
    "  im = Image.fromarray(screen)\n",
    "\n",
    "  images = [im]\n",
    "  \n",
    "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
    "  for i in range(1, max_steps + 1):\n",
    "    state = tf.expand_dims(state, 0)\n",
    "    action_probs, _ = model(state)\n",
    "    action = np.argmax(np.squeeze(action_probs))\n",
    "    state, _, done, _ = env.step(action)\n",
    "    state = tf.constant(state, dtype=tf.float32)\n",
    "\n",
    "    # Render screen every 10 steps\n",
    "    if i % 10 == 0:\n",
    "      screen = env.render(mode='rgb_array')\n",
    "      images.append(Image.fromarray(screen))\n",
    "  \n",
    "    if done:\n",
    "      break\n",
    "  \n",
    "  return images\n",
    "\n",
    "\n",
    "# Save GIF image\n",
    "images = render_episode(env, agent.ac_model, 200)\n",
    "image_file = 'cartpole-v1.gif'\n",
    "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
    "images[0].save(\n",
    "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "2BT7j10-9zms",
    "outputId": "0ac61f46-b836-433a-f397-5375a1dd26e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/gif;base64,R0lGODlhWAKQAYQAAAAAADMAAAAzADMzADMzM2YzM2ZmM5mZZsyZZplmmWaZmZmZmcyZmczMmf/MmWZmzJlmzGaZzJmZzMzMzP/MzP//zP/M/8z//////wAAAAAAAAAAAAAAAAAAAAAAAAAAACH/C05FVFNDQVBFMi4wAwEAAAAsAAAAAFgCkAEACP8AMQgcSLCgwYMIEypcyLChw4cQI0qcSLGixYsYM2rcyLGjx48gQ4ocSbKkyZMoU6pcybKly5cwY8qcSbOmzZs4c+rcybOnz59AgwodSrSo0aNIkypdyrSp06dQo0qdSrWq1atYs2rdyrWr169gw4odS7as2bNo06pdy7at27dw48qdS7eu3bt48+rdy7ev37+AAwseTLiw4cOIEytezLix48eQI0ueTLmy5cuYM2vezLmz58+gQ4seTbq06dOoU6tezbq169ewY8ueTbu27du4c+vezbu379/AgwsfTry48ePIkytfzry58+fQo0ufTr269evYs2vfzr279+/gw4v/H0++vPnz6NOrX8++vfv38OPLn0+/vv37+PPr38+/v///AAYo4IAEFmjggQgmqOCCDDbo4IMQRijhhBRWaOGFGGao4YYcdujhhyCGKOKIJJZo4okopqjiiiy26OKLMMYo44w01mjjjTjmqOOOPPbo449ABinkkEQWaeSRSCap5JJMNunkk1BGKeWUVFZp5ZVYZqnlllx26eWXYIYp5phklmnmmWimqeaabLbp5ptwxinnnHTWaeedeOap55589unnn4AGKuighBZq6KGIJqrooow26uijkEYq6aSUVmrppZhmqummnHbq6aeghirqqKSWauqpqKaq6qqsturqq7DG/yrrrLTWauutuOaq66689urrr8AGK+ywxBZr7HgIJKvssggcaxiz0DpbGLTMSjtYBdQua61gFmSr7LaBYetts+D+RcG45Jbb1wToquvXueO6uy6zB1Qr714OtHuvXug2sC+/407wb17oUjAwXugKfLBd0NarbAULMzyuwRHThe4FFVs8LsQZy4WuBR17HG/IcdFrL8lv6YuyWyqvvJa43rrcVrfeHiAzWzBne/NaFDicrM/J7qwWvDELjRa7IxttFtE6K21WAy07LRa6DEhdVs0IUGz11A0rq/XWYCUMNtfecjz2VwWfHfbGaqM9Lshtd4Wu2XFrRa3PGNe9VdR6X//Fd99V/Q24VDlTOzhWNCsLdLqHU1V4tI1X1XPSkUvFtOGVT4V00ZlHdTnknUMF9d1Bhw4V6cn6a/pTaa/uVOuuMyV27LKzTftSsN+O1MW6K4U6AnT3XtTHwu9OefFECY78T8ovz9PjJzsfVOI/Ry/9T9Bre31Qn1u//fOLe/+9Tt0rPv5Pmzd9Pk/d+7x+T1S/z1Pu8t/Ub/06mbzs1/jXNHf/OJkdAP1nuwHShHgGPGABExgTrDFQgcsC2gNn0rwJoqSCFixJ9r6VQZdQD3MdZMkGSxfClUxOceIroQYxqMKPlI+DLURJ+kAYQ5Pk63g1HEn8cmgSrPGPhyHRn7L/FAZEHU6siCT5HxJFIsAlfkSJTnzi26IIEihSkSNdU1berohFHHIRIyz8IkTCKMaGjJBxZbTIB0GXRovkLHxtxMgJuxbHi5yxjgIBgB73yMc+9pEA6PKjIP2Yw0Eaco8FCOQhB1nIRQ7SAIp0ZB8bKck+KmsBD5DAAxagrAJUcpI1/KQlEZDJUmoyWQYQ5R4pqUoERAACEdCkLCOAgAGoUo+sFCUsJQDLXZbSk7fM5SdfSUxevlKTtgxmKG8JgFn2MpaZTKYqhVlJWZoyk68UADOpKcliYvOZAdjmMm+JAGuaEwHMBAA3HZksZ0qAlugUZwzTeUkISEABCVBWOteZws5+qtNA/gyoQAdK0IIa9KAITahCF8rQhjr0oRCNqEQX6aGJHpKfE8WoRDUaUY5C1KMPBalDRdpQkjLUpAtFqUJVmlCWItSlB4WpQfFI05ra9KY4zalOd8rTnvr0p0ANqlCHStSiGvWoSE2qUpfK1KY69alQjapUp0rVqlr1qljNqla3ytWuevWrYA2rWMdK1rKa9axoTata18rWtrr1rXCNq1znSte62vWueM2rXvfK17769a+ADaxgB0vYwhq2MgEBACH/C05FVFNDQVBFMi4wAwEAAAAsDwGsAEIAjwCDAAAAmZlmzJlmmZmZZmbMmWbMZpnMmZnM////AAAAAAAAAAAAAAAAAAAAAAAAAAAACP8AEQgcSLBgQQEIEyoUYLChw4cQIxJcSFGixYsYEVBcmLGjx4kbE34cmTGkSJIoI5pEmLKlw5UMXcocCHOmzZo2ZeLM2XInT5Q+f46kGECh0JRBj3ZMqhQj06YWn0JVuXKqR5NFrS4lelLrRaleDYINC9Ik2ahVz1I1qxYi165tG45VO/dsXbJ3w+b1ulfrxqIs475MK1gs4cJlQyI2zHYxzcOO3wZ2LLCvVctTMUPV3JSzUqwxKWuULLoy5MWej6YWuvpna56SQ1N+nZP2zdOIbc/UrRN34b9wI/sWzNtl8Z7D4x5HuhDw5NnJ2y4HGp1udbvX8YbMWhr4c+GNoYf/B6+4e3a95/k2N2p+POr0fuFflp+Z/mb7nUm3Ly96Okn/Q+H3mYCqrRfce+7lRiBrC7rWIGwP1radbOQl5FxpoyX4W4S3aUgch7uB2JtCzlGIIH/ioVhhRftthCGAH8F4lX79iVgQADjmqOOOO8LE448/YgTkkDn6SOSQQh4JpJFK8phkkz0iNAABBhQwQEJQOnlRllFWeUCVBhCAEJc6PkmmAAUYcAABXxbwpQBk5mgml2qyaeeaa8aJ45xZpulnnXjqCQCfUIKJJ5t1Ckpok20CeqeiW+rpJ554pgmpRYIKgCeYaR4Ap56LKokQp3WOCWqkcSY0gJ9XmhpnqILGQTooqrLWauutl0qE662w7gplr74qCWywRA5LLJDGHqslpsq+Smuzvz4LrbDSTltstdYii222y+rKLbXMfnutRQEBACH/C05FVFNDQVBFMi4wAwEAAAAsDgGsADQAjwCEAAAAMwAAADMAMzMAMwAzmZlmzJlmmZmZzJmZzMyZ/8yZZmbMmWbMZpnMmZnMzMzM/8zMzP/M///M/8z/zP//////AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACP8AKwgcSLCgQYEGEipcaOCgw4cQDzKcGLGixYITGV7cWDHjQo4gHXpUGLLkQAkjE5o0OSFlw5UhUaaEGdMlTZAtZ97cKKHAQp8fd16EYFOoRZkjjVqEANSj0opERxZ4GvFBUaoOoybFmrVpRq5ZMzYFezDBVbIDtTpFS9AlArZtU0KAO9BlAroI5eKtMNHrg70u5+J1+XewXsMjIwA+TJehVwMSFnssILhxysiIPU6Q7BGz5ZEUOGf0DLfvws2Zv4qmuFpj66CpWcf2upevTrpI18Z2jfss25wJH7/sbUB48b09b8MFrhtubtV01UKHa1U52+ey4UrPzra66eHafaP/NfuTd3jr48WTdftaYeXSKe/uXvierViFhT97rI+WcPuE/K132X8G5AdfeQqRZh9jB3oU2nzuEahgfymhpl9GD14YXIIE1qaegFtBqFKHq9FGInEhamjedR9yxZxwyG04HVtMSRUjemRttyJa2O1IlnfN0dgiVjrCRp1U4NEo3Fh4KTAkVUDOiJaTOILFnogGBGhlfK01pSVX/mH5JVZhhgTAmWimqaaaLg2w5ptrVgDnnGg6xpCbdMIpZ55wukQAn2/uCSibCR2wgAMLHKAQnoOiKWijZybUAAOTIupAQgFA6qimdR46aQOWLmCAAJwC8CikDlT6qaUNZMrpqY0ediprqA4wQOqrpQJA66yT5grroKlSGiyrrmr6K6CeMpDsp77maoCwrFJqQLOlJkTptZMmRC2nCh1gqaLalnpsruSeOW656Kar7rnqQspuu4O+Cy+f8s5LZ7326pkvufjuq2a//m4asLEDE1xwowAPnHDAC/tbQUAAIf8LTkVUU0NBUEUyLjADAQAAACwOAawANQCPAIQAAAAzAAAAMwCZmWbMmWaZZpmZmZnMmZnMzJn/zJlmZsyZZsxmmcyZmczMzMz/zMzM/8z//8z/zP////8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI/wAnCBxIsKDBgQQSKlxI4KDDhxAdMpwYsaJFgxMZXtxoMeNCjiAfelQYsmTBkQlNqoyAsqFKky1flpQQUyZIlihthqyp8yLOkT030hygkGhCokF98kwK8cFSpg5/eoQa8YHRqVQfSs2Y9aHTnF0PRrjKNazBrxONmj37dC2EtmbRYl07UO7Rj3QHOoAb1i7FvAIR8O3qVyPgCXsXknUJOMHgrImBHm554PCElg4st0ygGWXmyWkVcgY98jPglg86l1btMTVpj6bzhlYYgXXG2HRR255Y+/Xt3Qx7n0bpejjDqxCAL5SgnHbzhMJlo2Tu+3j03MeXPydwfe3jrN+ptv/sbjY8VJQDLF/OqHY3WfXmmcZPOj9o/Z73bW79+3ox47z7GQYYTWABFuBdlhEomYH+4WVgdg7mpeBcAOYnk1UFVpghXYVFSNdb7KV0mF/vjWjhS5FRyOGJKmG4YF4dknRYimU1FqKIgNHIX16ObbiWjgJKN1Jl1TGEAHBXFSekR0cWuZCS2I3UpHGtbYebd7MRACWWqzmp0JbloUReWJhtByaZECaUnJcJndnVeFaqBMCcdNZpp50tBXDnnncKxOefdLYkAKB/+kkon3keuqehiuKZkAEKMFCAAQo12ucEljrKwAINbMqAAgkNmumcjI5KwAIMNKCAqg2oSoCeowKIUGqmqa66qqedxkoqprqiiiqrtjagq6y8xuqpr7U2sMCws1raKbLICqtrs43+am2yzBZr6q+3+kpAtsMmFKytCYGrq0IGsEppudNqO+y78MbbKLXymluvvPTeO2q++lrKb7+K/gswoQIPXKi7BvuLcMIBL8wwwQ4/fLDEmRZM8a4XzxtxxnUGBAAh/wtORVRTQ0FQRTIuMAMBAAAALBABrAA2AI8AhAAAAJmZZsyZZpmZmcyZmczMmf/MmWZmzJlmzGaZzJmZzMzMzP/MzMz/zP//zP/M/8z//////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAj/ACMIHEiwoMGBAhIqXCjgoMOHEB8ynBixosWDExle3Hgx40KOICF6VBiypMGRCU2qFIiy4UqTLV/CRCmzZMyaHB3cxLlxJ0+LEwN8/GlRJ02iFR/4RCrxKNOHRhMK1fhUpNOqBqNmDIDVodKrXQcanUox7EmwZrVmNFvw60i2BB2QXTgVrtilZt16tCtQbVm7DPCGlctwLt8Ietfy9UsVsOCuhN/yDYx2sMe6k4MOtdvgMVbKkh1Xhjz3L1zQezPTbQy38+jPnquiVmx3QeynDEqzZjtbKsnJujefvs3U9uuqBogj7b3brPHQcJNrTsn3eWrR0NkWUE6UuXDthZuH//X+2+72409bEjjcsgB7lAzej3TPd6vCBfI9GsifEX99lPv9N5J/drUUYIHh3cffRPEJ6BGBcBm4IEMQstVSgwgOOCFZGEaIUoVmXTjhQg2MqFCHFn644FwOmJgQiiGq6GBGLc44EYhhtVRjhh7BmON0ApRoI0M+dqWjiwLgaCRKO3o40gMrkohkkVgdOeRCUF6pkJA80ohkll36llCTKY4EAZJkxvgkmkieqRIAcMYp55xzAikAnXjiOVCefMbZUp997glonn8OqqdAhuKp0AAHKHDAAAolSqegkvopQAIIJOCoowlVKielngqQ6aabJnCnp3CCWqmmjba6KQKopoCKaKyZ1srqprECoKqkmN7aKKu57pqoApgSi8CmjQY7K6q2jtqoo8pGkKsApTZ7KqrCGppQr852Gmu2gy5aK6TeYrtsruimq26l4K5rrrTuuttuvJLOS6+h9t4LaL768slvv4fCC/C7A397bsGJ/ouwrAIvjO/BDgcKccR5KrxwQAAh/wtORVRTQ0FQRTIuMAMBAAAALBMBrAA3AI8AhAAAAMyZM5mZZsyZZpmZmczMmf/MmWZmzJlmzGaZzJmZzMzMzP/MzMz/zP//zP/M/8z//////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAj/ACMIHEiwoEGBAxIqXDjgoMOHECEynBixokWHExle3Lgx40KOICN6VBiy5MGRCU2qHIiy4UqVLV/CROlAZsmYNkO2hJATpEcBCWv27MgQqMIHQ4mOTKrUo1CmIlEihRp15FOqGFHyxPqw5VWuBVtOBWswo9EBX8kilKq2LM2tbddajUtQLF2WIwWkVTvx7AAGd+U6DRzBbuCWDQi3BHyYpmKUCx6bRStZoV/Gd71Wzhi58UjMdDV79tg5M0rQcVuWDl10Ieq2qjdPfM0XsmyGtMnGHp0xN9jJCQvcdj1c4erUKA0UT3gc9mnZfoXznr18gHTTI5vXHql8OkPtulFe/2fdOqFvrgZw3l2gnm56lITZW6YY+P3SwPLv32XQPm6B/m3x95NL6wGoloAL+UUYA37Rt5+BZOXn0YIQgsUgfIE5UCFXCJYX34ZYdZgRYRpiuF+DCgYmooN0lbhQAB/hByJVLk6o4oxQ1Tiiin6dBRSJODK1okYZBpnUAw0SeZeOLMY1ZIxLAkcgXQ+UR1KGSULZopFDVWnill+SZ2OUYbblpX5b+qgkdmOCiWZcZ7Yp5o4rAWDnnXjmmSdKAujpp58E/SnonS0NOmighv4pZaJ/IsrongkRcEACCBCg0KN6OoqpnQlRqsABnyqQ0KZ4arrpAAogkECooB4wAKl3mo+KKasKUGqrArDaKeujtaqqKq25ArAro6HaikCvCQQ7bKK0+roqAsoOFOytoBqbbK7LGopqtb+C+iq20uaaEK3efgtrtoMqRICvlo4KrkDBxivvvNHCS++9woaL77zo7stov/4aCnDAgg5MMKD6HkyqwQqXmnDDjzIMcb72ThzxwxYfinHGjW7McaYef4xnQAAh/wtORVRTQ0FQRTIuMAMBAAAALBcBrAA2AI8AhAAAAMyZM5mZZsyZZpmZmczMmf/MmWZmzJlmzGaZzJmZzMzMzP/MzMz/zP//zP/M/8z//////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAj/ACMIHEiwoMEIAxIqXDjgoMOHEB8ynBixosWDExle3Hgx40KOICF6VBiypMGRCU2qFIiy4UqTLV/CROlAZsiYNkG2hJCTo0cBCWv27MgQqMIHQ4mOTKrUo1CmIlEihRp15FOqGFHyxOqw5VWuBFtOBVswo9EBX8kilKq2LM2tbddajTtQLF2WIwWkJTvx7AAGd+U6DWz3bssGhFECNkwz8cgFjvuijZzQ72K6XilPhMx45OW4mTt75IxZsWaGpEEXXfi5bcvUrk2LzghbbcvWtlHW5iu79OqEBU6zFq5wN9iWBognNM71tma/wWdPxM17ZHTfo5UPSC4dtfbrqn///w1sACfdBebjlkcZGL1Cvynvrl961z19ugzSty2gX23+ny6d1x9Z/y3kV2AMwKfRXQV61N6AYDWYEYIKfnSXAxByJeF78QnIHoMZYoXhh/jBdyCIJMY14kIBWOjhfSqGSNWGC9K1ooMM+nUWUIHdOCGKMLblI0V3PVAhSRfKCBWNLsYoWYdxPSAelEIeSaVaQ9YYpZJMZdlkeDjayGVSUqYo5AA7agnmj2KaqVaZQcYWJ0gA1GnnnXjiiZIAefbZ50B+BmpnS4IKCmihfpqlEKJ+HsqongkRcEACCBCw6KN4OoppnQlRqsABnyqQ0KZ3arrpAAogkECooB4wAKl2mo2KKasKUGqrArDWKeujtaqqKq25ArAro6HaikCvCQQ7LKK0+roqAsoKFOytoBqbbK7LFopqtb+C+iq20uaaEK3efgtrtoIqRICvlo4KbgTBxivvvO/Sa6+u4d47L7r6Mspvv4X+C3CgAg/8Z74Gk1pwwqUizPCjCz8srMMSB0xxxQRfjPHB8G6MaMQPBwQAIf8LTkVUU0NBUEUyLjADAQAAACwaAawANgCPAIQAAAAzAAAzMwAzADMzMzOZmWbMmWZmZpmZmZnMzJn/zJlmZsyZZsxmmcyZmczMzMz/zMzM/8z//8z/zP/M//////8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI/wArCBxIsKBBAwgTKjRgsKHDhxAHLpwYsaLFhhMXXtx4MaNCjiAhekwoIaTJgiMRTjjJskJKAyVbmnxJQebMlBBshnwZU+dGjwUQ5vT5M2UEokVHDkVa8eVSpiJTPoDadGHQhE+pYpSqNarSrg9fJgDr0CnZrSPHni2o4OXagg8mXkX4liAEt3UFxk2ZV+Bdvn0hzM3Yt4IEvHn/Khxc+DDgvHtHNkZcdwLlt44l9808+Gphy4M1br78lvRa02dRk1UNlnVXoHQLy/0oO2XPui9X9uVZe6TuvLx3L15YU/hX4x5vl+aKPGPW00Z7WxUqHeHg56mZA8dZfaLa7cfBe//8jpt74OGi8yZwTVVxxgKFI3ss7J5w4NDp62ZGz/D8Y/3sQSWffXntR2Bl09FWYIBMGUhRX5YZgF9sC/5XnmbNPZihdRSKd+CF80mHX2Eu2dbdQr+BmFFxHk6kHHS+nSghQi9mx58BKS43Uo2r4cSijsnJmNBUGyqEXY8JhUZkiwsd2Zp2Kk6kgJAILRnlQlMWOWR3g5H3Vn0a1rWehWuBmd9bA4b5JYNIGRhafGw2BMCcdNZpp50CvHTnnncOxOefdBKgJ6B8+kkon3mmdOiehi5q5wAIIbCAAwcgoJCjdjaK6Zx5TjppAww0kNCmdGq6aQANOOCpqqoiROqcpmKKyiqoDoBqqwGvAhCro7aGGiqrDOD66q6LzuprqguImiuxhwKLLKsOCEsqs4TayuqkrEq7KbWEXuvAta4OK1Cucxrg7aSjilsBuQBEyqql6U47Lrv01muvo9zei2m++i7Kb7+E/gvwnwIPzOi8Bi+LcMLyrsuwug9DHPG2C098aMETYxyxxg9zzHBAACH/C05FVFNDQVBFMi4wAwEAAAAsLwGsAA8AfgCDAAAAmZlmzJlmmZmZZmbMmWbMZpnMmZnM////AAAAAAAAAAAAAAAAAAAAAAAAAAAACP8AESAQQLCgQQECBR5cmHDgQoMNH0JMKLFgxIoIKWK8WJGjRI8PQTLU2JFkwQATFW40GZLlSJUlYX50eTKjzJoERR7UmdJhTJ8zb74EShClRZo9MdokOlQpz6NCd9I0utQpUqhMpUZNulIo1apds3L9aXUr1ooodVJ9mvNqW7NvxZ4Nu/ArW7Bk6erNy9fgWrd4g8qNW3Zw4KJjBRdevFexxLRT5/ZtCfdw08aIJTuefJmzVsNqNVMGDfiuaZd2S6uufJr1VchR/7qeTZo2Y7+JR9/ezFs35syWpQIAoHT48OLGkR8nOICAgQIDChonLuD5gecGCBCcLqCAgQMErhc0uC5g+vfw6MGDn+69/Xn15r2rD39+uvj36dlbD6/eO3f12MlXXnLVuQfedgQKMEB70SEYEAAh/wtORVRTQ0FQRTIuMAMBAAAALBwBrAA0AI8AhAAAADMAADMzADMAM5mZZsyZZpmZmcyZmczMmf/MmWZmzJlmzGaZzJmZzMzMzP/MzMz/zP//zP/M/8z//////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAj/ACkIHEiwoEEKBRIqXFjgoMOHEA8ynBixosWCExle3Fgx40KOIB16VBiy5MAIIxOaNCkhZcOVIVGmhBnTJU2QLWfe3BiBwEKfH3defGBTqEWZI41afADUo9KKREcSeBrRQVGqDqMmxZq1aUauWTM2BXsQwVWyA7U6RUvQ5QG2bVM+gDvQJQK6Al3OxTvRqwO8COUCdvmXr2DDIyEMPkyXodcCERZ7JLC3ccrIiD1KkOwRs+WREzhn9Ay378LNmb+KprhaY+ugqVnH9go48Fa6SNfGdo37LNucCR+/7F1AeHHAPXXSBa4bbm7VdNVCh2tVuXPfaKXLpj5ZJV7tvOGa//0Zni142OKxk3X7WmHl0inv7l74nq1YhYU/e6yPlnD7hPytd9l/BeQHH3kKkWYfYwd6FNp87hGoYH8poaZfRg9eGFyCBNamnoC3aVjegiE2uCFJHeL13HYmsojWiiOSxZxwyJ3oIllMSVWjdeZ9yBWM6LFVXYnZ+YjVeSjSNaRpgOXoWJJwJWAkVUtOx5aUPFI40lsQAkigfCI2FSBY/nVZwJhclVkSAGy26eabbrokAJx00klBnXiy+eRCc+aJ551+0unSAIHWCWihcSpkwAIMLGBAQn0i6uahkgKgkAIMNKCApgwUEEClk4KqZwGZMspoA4x+KioAlEpagKawNnoqawOrsrpqAZvmGiujtbaK6Kubzqprr7dmGqypxhIragHILgDrqcqCmhCsxzZaq63LJiTrqQoUcK2vhS5kAKyPehvttei+CW667LbL7rruggpvvJLOS2+h9t7rZ776/tnvuf/WG7Co/A4cqsECI4xowQozjLDDBlMQEAAh/wtORVRTQ0FQRTIuMAMBAAAALBwBrAA2AI8AgwAAAJmZZsyZZpmZmWZmzJlmzGaZzJmZzP///wAAAAAAAAAAAAAAAAAAAAAAAAAAAAj/ABEIHEiwoEGBAhIqXCjgoMOHEB8ynBixosWDExle3Hgx40KOICF6VBiypMGRCU2qRIhypUqUDV2WhClzZsuaIGni5KhzZ0eGAT76/DlyKFGPRi32TCrxJtOmRZ9CnRhU6lSFQVNaxeh0K8GlXllGDTsQbFizXqkKJSsWKdu2Gd/CpSgX7Va7VvFKzZg15lu9TwEzFZyUsFHDQ9WSrNuVLGKfj3dGxjm55siqjIEu/tv4bOe0n++Gzjt6r+bNbCvLVO2S9UrXL0sHlj1Ysd/UtAvnPrw7cW/IvyWf1sp5LG7jjoNTVm6Z+WqPmIsv7CsXAWyT120i97wddHfR09ce/3crPW5m8uPNl6e7XuN5rKiTfyc933T92fdr59cdPj539PIB+J966bFXoHvtJRTdgdS9Z2CABEL44IATeieghQqKJyGCB2pIIYcbeohhheBdWCJ8xHXoH0gAtOjiizDCCFOMNNI4UI04ujhjjjjeyGONO/4Yo49CypjQAAQYUMAAChU5pEBOGqnkAUoaQEBCUb5IZJYCFGDAAQRQWQCVAmTp4pZRfhnmmmCCaWaLaDrp5ZxqtvkmAHEWWWWbYap5Z55Cilknm39C+eacbbbpZaEI3ClAm1V6eUCZbwL6Y0KRqollpYaaqdAAczK5qZmW3mnqqajmWGqqWa7KqpOuvi0qZKyy8khrrT12iiunje7KqK+/AkuqrsIWeWuxcBKLrK3KLptrr87+eCyyAQEAIf8LTkVUU0NBUEUyLjADAQAAACwfAawANgCPAIQAAAAzAAAAMwAzMwAAADOZmWbMmWaZZpmZmZnMzJn/zJlmZsyZZsxmmcyZmczMzMz/zMzM/8z//8z/zP/M//////8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI/wArCBxIsKBBAwgTKjRgsKHDhxAHLpwYsaLFhhMXXtx4MaNCjiAhekwYsuTBkQYkmFxZASVDliZdToBZ0qVKmiBRFsAZ0iZPjhkLJJz5syPKm0UruoRAIanSo06fjoQQNaJOpFUxLhSakGpWhz6/ah35QOxYj17NEnRZVu1alGndtkTZVu7cqXYFTuSKsK5clwry3vXo1y1gwWwRo0ygGG9eCHs/5k3gUjBklIIfVM6rYLPdywj5ksyrGfNj0RTzStBp2bNc0B4FR3DtFrZCroJXb5X8mbZaCag15p3g26zukbmLiyV+W7hd3cERCq4QffRz5V+ZI28cm3vG6di/sv/2njpvUOt2d/NOb9p8e/YjsRpGSRR+d/fxyS+s/7d5wgLyqSWTfgoFaFZY9mXE33weAUggV3Ex+F+BBHZVIUJM4eeRgWIlpmFGEQpIl3eohXjgiB9OZGKHi11owIrioZjghAYElqJohYk4ko0zLpSjWZ2951Zp29kVZJFyEWlAcK0JqZaS3z0WXlUQVCedlE4a1+CVdkFZ3mtTRnXcfWD6h55bwGUplm1fohmmU9qRWRIAdNZp5512CuASnnzyOVCfgNIZwHkIBRron4byOYB6CSXaJ6KO2klAQggs4MABCDQa6Z2QbgrAoAY0wICopBbqaZ2dbqonqQ5YWuqpqAqMBCudDow6aqu4MjArAKluiiupDPy6a6+ROgCssbY2MKyss9qKq6vBLlvBrsg28Gyrys5KrKMGJPsrAwZIuytC1lpqrqmwbpsopbdmiu6p6u4q77z0HspsvfjGi6+n+u4bab/+JgpwwPZOS7C8Ax/s570Kw8tww5smDHGsBk8c8cMWGyqxxRtP3DHEAQEAIf8LTkVUU0NBUEUyLjADAQAAACw0AawADgB+AIQAAACZmWbMmWaZmZnMmZnMzJn/zJlmZsyZZsxmmcyZmczMzMz/zMzM/8z//8z/zP/M//////8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI/wAjCBRAsKBBAQIjHFyYcOHBhg4LQoyIcCDFigovToy40WFHhhEcXMQ48mMAgwJFaozwoKRFiikNnnz4kmPIiwEEtlypcibIjDBvrtwZ1IFPmQRjDnWpMigDpkcFHNXJ1OVTnlFpXi0acabArQWnRmhgtSzWlWA9RkgbVSBZtGaDLrCaFeVahz6/1pV4d+XclQbi2vwbNPDCvBEI20wLsoBgtY6R2mVMM3LQkQRqOiygeSGDzgc5A5UsYAFogwZOFzQ92mHq1gtZj3x92KDsi59hH7xNkfZF3hFzjwQetqDw36oJHqfYILmA5RGJSzXowDl0h9IPVtdt2/n2kdcJ+odszt24d+fZqTt/0PkoefDnyxNkL1/A+4vfL9LHWzA/RQjxjbQffs5BAAAAtRV0IIIXLTiSgwQNcIACBwyg4IEEJYBAAhRSSJCDG3bYYQICLMjhhCh2iMCCG7Z4YocmhpjiiQsqoKGNCHQ4IYsa9ijiAQ6O6CICJWIoQI8yfmikAAO0aOGHAQEAIf8LTkVUU0NBUEUyLjADAQAAACwiAawANACPAIQAAACZmWbMmWaZmZnMmZnMzJn/zJlmZsyZZsxmmcyZmczMzMz/zMzM/8z//8z/zP/M//////8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI/wAjCBxIsKDBCAISKlwo4KDDhxAPMpwYsaLFghMZXtxYMeNCjiAdelQYsiTBkQlNqkTZUGVJli5foowZEibNjQ5s3ryoc2dHhgE++qyYc+bQiA96HhVpdKnDogmDanT6UCnVgVAzBrh6MGlTrlijegRr0CrXrBnJEvQ6Ui1WqUBTuo2AluJctmPn1p3qloHZqw7giiV59y/VvUL7GnYa+CtZv47BIpY6NwLkhYIrN1i89HJexZHPCrYLuu1czwJGa+Z8FDVpta75ql3AeiiD0bIfayXcF3di2LV90g591UDwnbF/kx1u2q3xiXArM/8MnDjVAsdvJuetFjvm3GC3y/916725WpYEKrMsoB4lg/Yj2c/dnXABfI8G7me0Px9l/v4j8ecWS/8NGJdCAp7nnn4TJUgWgQwy5CBYLL0HoEcTckWfABYaGGCEC3Wo4EgNgKiQiA+ilOFV0C3kgIkJoUihijAK8OKF+9V4o4ceyahhiwmViONEPrKI0o4jYqhjjQ8wKJiQPGZUJFUsIZniSE0OyRCUSWZk5YxYRgjXlz+OBMGSWi6UZZQTkWmkmSYBIOecdNZZJ5AJ2amnnhHs6eecLP35Z5+C7hlooXwiqqdCAxygwAEDKKSonYROCqgACSCQwKOP5mnpnJV+KoCmnHKagACfgpqqnJs66iqnCKx1CkColmpqa6ucykrrpJni6mirusqqQKbDIsCpo8Gueiupjj6abKoCmLosqqvuqmhCvTLraarWIsqorZFu+2m3spZr7rl+kosut+u2q267k74LL6Lyzitovfamm2+5+O5Lqb/VAsyuwJb2S/CsB8ebsKIGExwQACH/C05FVFNDQVBFMi4wAwEAAAAsIwGsADcAjwCEAAAAMwAAADMAMzMAAAAzmZlmzJlmmWaZmZmZzMyZ/8yZZmbMmWbMZpnMmZnMzMzM/8zMzP/M///M/8z/zP//////AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACP8AKwgcSLCgwQoGEipcaOCgw4cQITKcGLGiRYcTGV7cuDHjQo4gI3pUGLLkwZEJJZhcKRBlQ5YrXU6AadKlSpohURbAWdImT5AZCyic+bMjyptFLbqEQCGp0qNOn46EELWiTqRVMTIUqpBq1oc+v2od+UDsWI9ezRZ0WVbtWpRp3bZE2Vbu3Kl2B07kmrCuXZcK8t716FcuYMEI6SJ2mWAxXMQQ9n4UnMAlZMuCH2DOq2Cz3cgK+ZLM7FkuBNEUBUvQeRll65GII5R2C1ryy7yrt07OW9sjYgmoNQqeMFttbtiqi5slvhD174TBEyKuEH00buVimSMXjF1s96/fs7L/drw9b1Dr5psL5+6avG/3GbEaRkk0ffm/UNmPrI8/dHP5bskE30QAqhWWfR7xN59HBRRoFoMJxbWgfwo56N1j+qHVFILxDchQYQFiyOFEIBqo3kIShkiWhyiyqFBjGWaUookrxkhhYDaKVuKDKOE44oeIddZeXpoNaZeQ97lVJHSp8RZeVEu+56SRplUnnWC9ZfTcebfZFaWWWD7p1HFSfqbbbnYBRyVtYiZFJph5aVemSQDUaeedeN4pgEt59tknQX4GWmcAXBogqKCAHtrnAGcqpKifiT56JwEKIbCAAwcg4KikeEbKKQCEGtAAA6OWmtCnd3rK6Z6lOnCpqajakqkqpw6QSqqruDIQa52zSoprqQz8uisAvT7qALDH2trAsMUqaiuurwbL7EDDJtsAtK4uu2uzhxqg7K8MGLottbsmdO2l6J46rkDDVnqrpurGyu2w9NZrL6Lk3qvvvPpyym+/j/4L8KECDxxowQb/mW/C8i7M8KcIPyyrwxIHTHHFBF+M8cEab6wwux5bDHLIhwYEACH/C05FVFNDQVBFMi4wAwEAAAAsJgGsADQAjwCEAAAAmZlmzJlmmZmZzJmZzMyZ/8yZZmbMmWbMZpnMmZnMzMzM/8zMzP/M///M/8z/zP//////AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACP8AIwgcSLCgQQEIEyoUYLChw4cQIyycGLGixYITF17cWDGjQo4gHXpMGLIkwZEITapEyVBlSZYuX6KMGRImzY0ObN68qHNnx4UBPvqsmHPm0IgPeh4VaXSpw6IIg2p0+lAp1YFQMwa42jBpU65Yo3oEe/Ar2awZyRL0OlItVqlAU7qNgJbiXLZj59ad6paBVa4O4Ioleffv1b1C+xqmGtgsWL+OAXuUOjcCZIWCKzdY7PRyXsWRDwu2C7rtXM8CRmvmvBQ1abWu+apdwPoog9GyycaGW/l2aKqxE8+uPdQAcZ/BCbul/dup8Ym85zI3Xfqz2gLHdyaX6xY75tyPs9//9E5dLUsClVkWSI+SAfuR6+dqTbjgvUcD9jPWl48SP/+R+7nFkn8CxkVffhO5959HAZrXH4ILNUgWSwoWCCCEcFXo4IULZqThhCg1AKFCH4LFkoQmGoiQAyMmVCJXJ7a4oowCoAgjSix2mCCGCYmo40IvXsVSjhYySCORG3r0AIKC+Vikh0fSuOSPCjmZZEZIgjjSlE8OJkCWKY4EQZRUJsTllROBeaOYJgHg5ptwxhkndArJaaedEdyp55ss7blnnn7e2WegeBJqZ0IDHKDAAQMkZKicgD7KpwAJIJDAoosiJCmckW4qgKWYYpqAAJu+2amklyqqKqYIlOrmqY9acyprqpi6CgCshlZKq6Kp2ooroQpUGiwCmCrqq62zgqroose6KoCoyZLq6q+BIqSrsppOayuisjaabanU2irureOWa+65eoaL7qPqrktou+76CW+86dIr7rz2Qpqvtvtuim+/rwIs6b8AE9yvwfsinG8EAQEAIf8LTkVUU0NBUEUyLjADAQAAACwmAawANgCPAIQAAADMmTOZmWbMmWaZmZnMzJn/zJlmZsyZZsxmmcyZmczMzMz/zMzM/8z//8z/zP/M//////8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI/wAjCBxIsKDBCAMSKlw44KDDhxAfMpwYsaLFgxMZXtx4MeNCjiAhelQYsqTBkQlNqhSIsuFKky1fwkTpQGbImDZBtoSQk6NHAQlr9uzIEKjCB0OJjkyq1KNQpiJRIoUadeRTqhhR8sTqsOVVrgRbTgVbMKPRAV/JIpSqtizNrW3XWo07UCxdliMFpCU78ewABnflOg1s927LBoRRAjZMM/HIBY77oo2c0O9iul4pT4TMeOTluJk7e+SMWbFmhqRBF134uW3L1K5Ni84IW23L1rZR1uYru/TqhAVOsxaucDfYlgaIJzTO9bZmv8FnT8TNe2R036OVD0guHbX266p///8NbAAn3QXm45ZHGRi9Qr8p765fetc9fboM0rctoF9t/p8undcfWf8t5FdgDMCn0V0FetTegGA1mBGCCn50lwMQciXhe/EJyB6DGWKF4Yf4wXcgiCTGNeJCAVjo4X0qhkjVhgvStaKDDPp1FlCB3TghijC25SNFdz1QIUkXyggVjS7GKFmHcT0gHpRCHkmlWkPWGKWSTGXZZHg42shlUlKmKOQAO2oJ5o9imqlWmUHGFidIANRp55144omSAHn22edAfgZqZ0uCCgpooX6apRCifh7KqJ4JEXBAAggQsOijeDqKaZ0JUarAAZ8qkNCmd2q66QAKIJBAqKAeMACpdpqNiimrClBqqwKw1inro7WqqiqtuQKwK6Oh2opArwkEOyyitPq6KgLKChTsraAam2yuyxaKarW/gvoqttLmmhCt3n4La7aCKkSAr5aOCm4EwcYr77zv0muvruHeOy+6+jLKb7+F/gtwoAIP/Ge+BpNacMKlIszwows/LKzDEgdMccUEX4zxwfBujGjEDwcEACH/C05FVFNDQVBFMi4wAwEAAAAsKQGsADQAjwCDAAAAmZlmzJlmmZmZZmbMmWbMZpnMmZnM////AAAAAAAAAAAAAAAAAAAAAAAAAAAACP8AEQgcSLCgQQEIEyoUYLChw4cQESycGLGixYITF17cWDGjQo4gHXpMGLIkwZEITapEyVBlSZYuX6KMGRImTY42b17MqbPjwgAfe1rkKfQh0aINjyLFOHOp0aZOk3oEGlXkRKApqx6EqnWg0qpfo4Z1Onbp1aBdvXJNWxZp26JvhcbtOVdnRqwt0wqse5MvTb8xAbsUvPInWr2ETSaWOVKv2saOJa7tOpJq5LskL0/WurjmZrCfxYYmaziz484gUeMcbZa1W9dwYcs9m/W0bLq37ebuu/tv78ClayP+PZh4Yci2kQ9XznZq3uUK8UaWzJyyccXXGXucrnojbeHNq3P/zu5ZPGjzotGTVt86OHfyqeGvZv+afmz7s6MfDr9dM37c/+kWIG8D+obZe+75119yC0KXEYIN8vegggjh9ZyEFFGYIYMTcrihgx9iqJGGAkhH4n4cAaDiiiy22CJLLsYYIwIy1rgijDbWSGOOMuLIo4s7/vgiQgMQYEABAyQkJJBLDnnkAUcaQABCTbIYZJUCFGDAAQRAWQCUAlS54pVNbtnlmVxyKaaKZC6p5ZtmprkmAG0KGWWaXZo5Z50/ehknmnvO+WaaaWoZ6JoCpBmllgeEuSafPCK0qJlUPjpnQgO8mWSlYkI656eghiqjp6JWSWqpS56K6o+qrppjq67qHBgrqLDOyqStluLaqa678tpkrb4Cy6uwuhKLa0AAIf8LTkVUU0NBUEUyLjADAQAAACw+AawADAB8AIQAAACZmWbMmWaZmZnMmZnMzJn/zJlmZsyZZsxmmcyZmczMzMz/zMzM/8z//8z/zP/M//////8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI/wAjCBhIsKAAgQYNIkxIcCHDgw8bRhzokGHFhBcVThTgYGPGghcDFOw48YHHjSQHitQ4MSXDACZbqnz4kaBLhjEjOlhpcOXNhDkf/jTIACXPmQSDMhxasGjLowmd6ny4UirBow02WmW4NeHOiV2J9jSYFazWjQu0QhWbkCeDtU3RbjRwdmLaiXTbErwbMWzBAnUjAi4I1S/BwRsjEtiI+KHhgY2v7p0rF2/liHkJF+Tr+PLDzA85c/UseeBjAaKjbizbl7SAo0wJnk5tMPZA2iMDax7IurNMuyg3Kn1dsPfo3xGHkw1e8qXNjRCYJ5f+MPpYggA2Zp+4XcCAAwoODDjAPjABggThww/Mfj59+gQCAKAHTz89AgDn889PL799/fkAKGCegAikBx5+5iXo3gHZvacfAvEFBAAh/wtORVRTQ0FQRTIuMAMBAAAALCoBrAA2AI8AhAAAAMyZM5mZZsyZZpmZmczMmf/MmWZmzJlmzGaZzJmZzMzMzP/MzMz/zP//zP/M/8z//////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAj/ACMIHEiwoMEIAxIqXDjgoMOHEB8ynBixosWDExle3Hgx40KOICF6VBiypMGRCU2qFIiy4UqTLV/CROlAZsiYNkG2hJCTo0cBCWv27MgQqMIHQ4mOTKrUo1CmIlEihRp15FOqGFHyxOqw5VWuBFtOBVswo9EBX8kilKq2LM2tbddajTtQLF2WIwWkJTvx7AAGd+U6DWz3bssGhFECNkwz8cgFjvuijZzQ72K6XilPhMx45OW4mTt75IxZsWaGpEEXXfi5bcvUrk2LzghbbcvWtlHW5iu79OqEBU6zFq5wN9iWBognNM71tma/wWdPxM17ZHTfo5UPSC4dtfbrqn///w1sACfdBebjlkcZGL1Cvynvrl961z19ugzSty2gX23+ny6d1x9Z/y3kV2AMwKfRXQV61N6AYDWYEYIKfnSXAxByJeF78QnIHoMZYoXhh/jBdyCIJMY14kIBWOjhfSqGSNWGC9K1ooMM+nUWUIHdOCGKMLblI0V3PVAhSRfKCBWNLsYoWYdxPSAelEIeSaVaQ9YYpZJMZdlkeDjayGVSUqYo5AA7agnmj2KaqVaZQcYWJ0gA1GnnnXjiiZIAefbZ50B+BmpnS4IKCmihfpqlEKJ+HsqongkRcEACCBCw6KN4OoppnQlRqsABnyqQ0KZ3arrpAAogkECooB4wAKl2mo2KKasKUGqrArDWKeujtaqqKq25ArAro6HaikCvCQQ7LKK0+roqAsoKFOytoBqbbK7LFopqtb+C+iq20uaaEK3efgtrtoIqRICvlo4KbgTBxivvvO/Sa6+u4d47L7r6Mspvv4X+C3CgAg/8Z74Gk1pwwqUizPCjCz8srMMSB0xxxQRfjPHB8G6MaMQPBwQAIf8LTkVUU0NBUEUyLjADAQAAACwtAawANACPAIMAAACZmWbMmWaZmZlmZsyZZsxmmcyZmcz///8AAAAAAAAAAAAAAAAAAAAAAAAAAAAI/wARCBxIsKBBAQgTKhRgsKHDhxARLJwYsaLFghMXXtxYMaNCjiAdekwYsiTBkQhNqkTJUGVJli5foowZEiZNjjZvXsyps+PCAB97WuQp9CHRog2PIsU4c6nRpk6TegQaVeREoCmrHoSqdaDSql+jhnU6dunVoF29ck1bFmnbom+Fxu05V2dGrC3TCqx7ky9NvzEBuxS88idavYRNJpY5Uq/axo4lru06kmrkuyQvT9a6uOZmsJ/FhiZrOLPjziBR4xxtlrVb13Bhyz2b9bRsurft5u67+2/vwKVrI/49mHhhyLaRD1fOdmre5QrxRpbMnLJxxdcZe5yueiNt4c2rc//O7lk8aPOi0ZNW3zo4d/Kp4a9m/5p+bPuzox8Ov10zftz/6RYgbwP6htl77vnXX3ILQpcRgg3y96CCCOH1nIQUUZghgxNyuKGDH2KokYYCSEfifhwBoOKKLLbYIksuxhgjAjLWuCKMNtZIY44y4sijizv++CJCAxBgQAEDJCQkkEsOeeQBRxpAAEJNshhklQIUYMABBEBZAJQCVLnilU1u2eWZXHIppopkLqnlm2amuSYAbQoZZZpdmjlnnT96GSeae875ZpppahnomgKkGaWWB4S5Jp88IrSomVQ+OmdCA7yZZKViQjrnp6CGKqOnolZJaqlLnorqj6qummOrruocGCuosM7KpK2W4tqprrvy2mStvgLLq7C6EotrQAA7\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MTJUJw3wlN92"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = [[1,2,3], [1,2,3], [1,2,3]]\n",
    "arr2 = np.mean(arr, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5LBh6_lOVBdN"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
