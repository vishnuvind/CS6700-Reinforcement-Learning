{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/blaze010/CS6700-Reinforcement_Learning/blob/main/Assignment%201/Programming_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JiAc34_Ixg9"
   },
   "source": [
    "# <center >CS6700: Reinforcement Learning\n",
    "## <center >Programming Assignment 1\n",
    "## <center> TD Learning: SARSA and Q-Learning\n",
    "###  Submitted by: \n",
    "### Gautham Govind A: EE19B022\n",
    "### Vishnu Vinod: CS19B048 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIdfqnCWJV5X"
   },
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pqvk6fcRJvnu"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from math import floor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92G2tnyyKIWE"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTJ_Hxc2UxYR"
   },
   "source": [
    "We require functions capable of converting from row-column based indexing and sequential indexing of grid cells. These are defined below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i3BOKyqxVAer"
   },
   "outputs": [],
   "source": [
    "# Converts row_column format to sequential (state number) format\n",
    "# Input  - 2D array of grid cells in (row, col) format\n",
    "# Output - 1D array of corresponding state numbers\n",
    "def row_col_to_seq(row_col, num_cols): \n",
    "    return row_col[:,0] * num_cols + row_col[:,1]\n",
    "\n",
    "# Converts sequential (state number) format to row_column format \n",
    "# Input  - 1D array of grid cells in state number format\n",
    "# Output - 2D array of grid cells in corresponding (row, col) format \n",
    "def seq_to_col_row(seq, num_cols): \n",
    "    r = floor(seq / num_cols)\n",
    "    c = seq - r * num_cols\n",
    "    return np.array([[r, c]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnJXDjixV8pp"
   },
   "source": [
    "## Defining the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXwJVRiaWA8P"
   },
   "source": [
    "The environment class, the definition of which has already been provided as part of the problem statement, is defined here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uzwVett_WwPW"
   },
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a gridworld object to pass to an RL algorithm.\n",
    "    Parameters:\n",
    "    ----------\n",
    "    num_rows : int\n",
    "        The number of rows in the gridworld.\n",
    "    num_cols : int\n",
    "        The number of cols in the gridworld.\n",
    "    start_state : numpy array of shape (1, 2), np.array([[row, col]])\n",
    "        The start state of the gridworld (can only be one start state)\n",
    "    goal_states : numpy arrany of shape (n, 2)\n",
    "        The goal states for the gridworld where n is the number of goal\n",
    "        states.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_rows, num_cols, start_state, goal_states, wind = False):\n",
    "        self.num_rows = num_rows\n",
    "        self.num_cols = num_cols\n",
    "        self.start_state = start_state\n",
    "        self.goal_states = goal_states\n",
    "        self.obs_states = None\n",
    "        self.bad_states = None\n",
    "        self.num_bad_states = 0\n",
    "        self.p_good_trans = None\n",
    "        self.bias = None\n",
    "        self.r_step = None\n",
    "        self.r_goal = None\n",
    "        self.r_dead = None\n",
    "        self.gamma = 1 # default is no discounting\n",
    "        self.wind = wind\n",
    "    \n",
    "        # added to help naming conventions\n",
    "        self.wind_mode = None\n",
    "        self.start_mode = None\n",
    "        self.stochasticity = None\n",
    "        \n",
    "        # 'windy' if wind=True | 'clear' if wind=False\n",
    "        self.wind_mode = '_windy' if wind else '_clear'\n",
    "        \n",
    "        # 's1' if starting from [0,4] | 's2' if starting from [3,6] | None otherwise\n",
    "        if (start_state == np.array([[0,4]])).all(): self.start_mode = '_s1'\n",
    "        elif (start_state == np.array([[3,6]])).all(): self.start_mode = '_s2'\n",
    "        \n",
    "\n",
    "    def add_obstructions(self, obstructed_states=None, bad_states=None, restart_states=None):\n",
    "\n",
    "        self.obs_states = obstructed_states\n",
    "        self.bad_states = bad_states\n",
    "        if bad_states is not None:\n",
    "            self.num_bad_states = bad_states.shape[0]\n",
    "        else:\n",
    "            self.num_bad_states = 0\n",
    "        self.restart_states = restart_states\n",
    "        if restart_states is not None:\n",
    "            self.num_restart_states = restart_states.shape[0]\n",
    "        else:\n",
    "            self.num_restart_states = 0\n",
    "\n",
    "    def add_transition_probability(self, p_good_transition, bias):\n",
    "\n",
    "        self.p_good_trans = p_good_transition\n",
    "        self.bias = bias\n",
    "        \n",
    "        # 'detrm' if deterministic (p=1) | 'noisy' if stochastic (p!=1)\n",
    "        self.stochasticity = '_detrm' if (p_good_transition == 1.) else '_noisy'\n",
    "\n",
    "    def add_rewards(self, step_reward, goal_reward, bad_state_reward=None, restart_state_reward = None):\n",
    "\n",
    "        self.r_step = step_reward\n",
    "        self.r_goal = goal_reward\n",
    "        self.r_bad = bad_state_reward\n",
    "        self.r_restart = restart_state_reward\n",
    "\n",
    "\n",
    "    def create_gridworld(self):\n",
    "\n",
    "        self.num_actions = 4\n",
    "        self.num_states = self.num_cols * self.num_rows# +1\n",
    "        self.start_state_seq = row_col_to_seq(self.start_state, self.num_cols)\n",
    "        self.goal_states_seq = row_col_to_seq(self.goal_states, self.num_cols)\n",
    "\n",
    "        # rewards structure\n",
    "        self.R = self.r_step * np.ones((self.num_states, 1))\n",
    "        #self.R[self.num_states-1] = 0\n",
    "        self.R[self.goal_states_seq] = self.r_goal\n",
    "        \n",
    "        for i in range(self.num_bad_states):\n",
    "            if self.r_bad is None:\n",
    "                raise Exception(\"Bad state specified but no reward is given\")\n",
    "            bad_state = row_col_to_seq(self.bad_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"bad states\", bad_state)\n",
    "            self.R[bad_state, :] = self.r_bad\n",
    "        for i in range(self.num_restart_states):\n",
    "            if self.r_restart is None:\n",
    "                raise Exception(\"Restart state specified but no reward is given\")\n",
    "            restart_state = row_col_to_seq(self.restart_states[i,:].reshape(1,-1), self.num_cols)\n",
    "            #print(\"restart_state\", restart_state)\n",
    "            self.R[restart_state, :] = self.r_restart\n",
    "\n",
    "        # probability model\n",
    "        if self.p_good_trans == None:\n",
    "            raise Exception(\"Must assign probability and bias terms via the add_transition_probability method.\")\n",
    "\n",
    "        self.P = np.zeros((self.num_states,self.num_states,self.num_actions))\n",
    "        for action in range(self.num_actions):\n",
    "            for state in range(self.num_states):\n",
    "\n",
    "\n",
    "                # check if the state is the goal state or an obstructed state - transition to end\n",
    "                row_col = seq_to_col_row(state, self.num_cols)\n",
    "                if self.obs_states is not None:\n",
    "                    end_states = np.vstack((self.obs_states, self.goal_states))\n",
    "                else:\n",
    "                    end_states = self.goal_states\n",
    "\n",
    "                if any(np.sum(np.abs(end_states-row_col), 1) == 0):\n",
    "                    self.P[state, state, action] = 1\n",
    "\n",
    "                # else consider stochastic effects of action\n",
    "                else:\n",
    "                    for dir in range(-1,2,1):\n",
    "                        \n",
    "                        direction = self._get_direction(action, dir)\n",
    "                        next_state = self._get_state(state, direction)\n",
    "                        if dir == 0:\n",
    "                            prob = self.p_good_trans\n",
    "                        elif dir == -1:\n",
    "                            prob = (1 - self.p_good_trans)*(self.bias)\n",
    "                        elif dir == 1:\n",
    "                            prob = (1 - self.p_good_trans)*(1-self.bias)\n",
    "\n",
    "                        self.P[state, next_state, action] += prob\n",
    "\n",
    "                # make restart states transition back to the start state with\n",
    "                # probability 1\n",
    "                if self.restart_states is not None:\n",
    "                    if any(np.sum(np.abs(self.restart_states-row_col),1)==0):\n",
    "                        next_state = row_col_to_seq(self.start_state, self.num_cols)\n",
    "                        self.P[state,:,:] = 0\n",
    "                        self.P[state,next_state,:] = 1\n",
    "        return self\n",
    "\n",
    "    def _get_direction(self, action, direction):\n",
    "\n",
    "        left = [2,3,1,0]\n",
    "        right = [3,2,0,1]\n",
    "        if direction == 0:\n",
    "            new_direction = action\n",
    "        elif direction == -1:\n",
    "            new_direction = left[action]\n",
    "        elif direction == 1:\n",
    "            new_direction = right[action]\n",
    "        else:\n",
    "            raise Exception(\"getDir received an unspecified case\")\n",
    "        return new_direction\n",
    "\n",
    "    def _get_state(self, state, direction):\n",
    "\n",
    "        row_change = [-1,1,0,0]\n",
    "        col_change = [0,0,-1,1]\n",
    "        row_col = seq_to_col_row(state, self.num_cols)\n",
    "        row_col[0,0] += row_change[direction]\n",
    "        row_col[0,1] += col_change[direction]\n",
    "\n",
    "        # check for invalid states\n",
    "        if self.obs_states is not None:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1) or\n",
    "                np.any(np.sum(abs(self.obs_states - row_col), 1)==0)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "        else:\n",
    "            if (np.any(row_col < 0) or\n",
    "                np.any(row_col[:,0] > self.num_rows-1) or\n",
    "                np.any(row_col[:,1] > self.num_cols-1)):\n",
    "                next_state = state\n",
    "            else:\n",
    "                next_state = row_col_to_seq(row_col, self.num_cols)[0]\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reset(self):\n",
    "        return int(self.start_state_seq)\n",
    "      \n",
    "    def step(self, state, action):\n",
    "        p, r = 0, np.random.random()\n",
    "        for next_state in range(self.num_states):\n",
    "            \n",
    "            p += self.P[state, next_state, action]\n",
    "            \n",
    "            if r <= p:\n",
    "                break\n",
    "\n",
    "        if(self.wind and np.random.random() < 0.4):\n",
    "            arr = self.P[next_state, :, 3]\n",
    "            next_next = np.where(arr == np.amax(arr))\n",
    "            next_next = next_next[0][0]\n",
    "            return next_next, self.R[next_next]\n",
    "        else:\n",
    "            return next_state, self.R[next_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mbej1hYjaKOF"
   },
   "source": [
    "## Action Policy Definitions\n",
    "\n",
    "This part defines the necessary action selection policies like $\\epsilon$-greedy and softmax. They are used to select the next action of the RL agent based on the state and the Q-values of each action.\n",
    "\n",
    "\n",
    "First, we create an abstract BasePolicy() class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "E8wqIujOaWt3"
   },
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'base_policy'\n",
    "\n",
    "    def select_action(self, state_id):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$ - Greedy Policy\n",
    "We first create an epsilon greedy policy which makes use of action value functions as the quantity based on which actions are chosen:\n",
    "\n",
    "\\begin{equation}\n",
    "next\\_action = \n",
    "\\begin{cases} \n",
    "      \\text{argmax}_a(Q(a)) & \\text{with probability }1-\\epsilon \\\\\n",
    "      \\text{random}(a[\\dots]) & \\text{with probability }\\epsilon \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "### Hyperparameter - $\\epsilon$\n",
    "\n",
    "- Gives the ratio of explore to exploit\n",
    "- Higher values mean more exploration of alternate policies\n",
    "- Lower values mean more exploitation of greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Tq9xPdMfdPBJ"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy(BasePolicy):\n",
    "    # epsilon      : epsilon value to be used by epsilon-greedy\n",
    "    # actions      : Possible actions that can be taken up in each state\n",
    "    #              : Default set for the current problem\n",
    "    def __init__(self, epsilon, actions = np.array([0, 1, 2, 3])):\n",
    "        self.eps = epsilon\n",
    "        self.actions = actions\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return '_eps'\n",
    "\n",
    "    # action_values: 2D array containing predicted action values for each (state, action) pair\n",
    "    #              : array size is |S|*4, i.e, total number of states x total number of actions\n",
    "    # state_id     : state for which action is to be determined\n",
    "    def select_action(self, state_id, action_values):\n",
    "        # To explore or not to explore, that is the question\n",
    "        explore_or_exploit = np.random.binomial(1, 1 - self.eps)\n",
    "\n",
    "        # if exploit, select the arm with maximum value of action value function else choose random arm\n",
    "        if explore_or_exploit == 1:\n",
    "            return self.actions[np.argmax(action_values[state_id, :])]\n",
    "        else:\n",
    "            return np.random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s7QBOBQ1MHr"
   },
   "source": [
    "## Softmax Policy\n",
    "Next, we create a SoftMax policy for exploration. For this we select each action with a probability given by the softmax function:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{P}(i) = \\frac{e^{(Q[s,a_i]/\\beta)}}{\\sum_{k=1}^Ne^{(Q[s,a_k]/\\beta)}}\n",
    "\\end{equation}\n",
    "\n",
    "The selection takes place as follows:\n",
    "\\begin{equation}\n",
    "next\\_action = \n",
    "\\begin{cases} \n",
    "      a_1& \\text{with probability }\\mathcal{P}(1) \\\\\n",
    "      a_2& \\text{with probability }\\mathcal{P}(2) \\\\\n",
    "      \\dots& \\dots \\\\\n",
    "      a_N& \\text{with probability }\\mathcal{P}(N) \\\\\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Hyperparameter - $\\beta$\n",
    "\n",
    "- Measure of temperature (higher implies more randomness)\n",
    "- Gives the ratio of explore to exploit\n",
    "- Higher values mean more exploration of alternate policies\n",
    "- Lower values mean more exploitation of greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ihnhSGZMi4P5"
   },
   "outputs": [],
   "source": [
    "class SoftMax(BasePolicy):\n",
    "\n",
    "    # beta      : temperature to be used by softmax function\n",
    "    def __init__(self, beta, actions = np.array([0, 1, 2, 3])):\n",
    "        self.beta = beta\n",
    "        self.actions = actions\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return '_smx'\n",
    "\n",
    "    # action_values: 2D array containing predicted action values for each (state, action) pair\n",
    "    #              : array size is |S|*4, i.e, total number of states x total number of actions\n",
    "    # state_id     : state for which action is to be determined\n",
    "    def select_action(self, state_id, action_values):\n",
    "\n",
    "        # sample according to the softmax distribution to get an arm with beta temperature\n",
    "        return np.random.choice(self.actions, p = scipy.special.softmax(action_values[state_id, :]/self.beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXJu3h8L1h5Z"
   },
   "source": [
    "Finally, we also create a greedy deterministic policy which simply chooses the action with maximum Q-value. Although this is not explicitly asked for in the assignment, this can help us quantify how \"good\" the learned policy is which is necessary to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eKFbwBDW14FU"
   },
   "outputs": [],
   "source": [
    "class Greedy(BasePolicy):\n",
    "    # actions      : Possible actions that can be taken up in each state\n",
    "    #              : Default set for the current problem\n",
    "    def __init__(self,  actions = np.array([0, 1, 2, 3])):\n",
    "        self.actions = actions\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return '_greedy'\n",
    "\n",
    "    # action_values: 2D array containing predicted action values for each (state, action) pair\n",
    "    #              : array size is |S|*4, i.e, total number of states x total number of actions\n",
    "    # state_id     : state for which action is to be determined\n",
    "    def select_action(self, state_id, action_values):\n",
    "\n",
    "        #  select the arm with maximum value of action value function \n",
    "        return self.actions[np.argmax(action_values[state_id, :])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Policy Definitions\n",
    "\n",
    "This part defines the necessary action selection policies like SARSA and QLearning. They are used to update the the Q-values based on the actions of the RL agent and the state and the Q-values of each action.\n",
    "\n",
    "First, we create an abstract BaseUpdate() class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Nyh4ixNDnxP1"
   },
   "outputs": [],
   "source": [
    "class BaseUpdate:\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'base_policy'\n",
    "    \n",
    "    def update(self, state_id):\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVbg8OyUl9ki"
   },
   "source": [
    "## SARSA\n",
    "\n",
    "Recall the update rule for SARSA:\n",
    "\\begin{equation}\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "So we have some hyperparameters for the algorithm:\n",
    "- $\\alpha$: learning rate for the RL agent\n",
    "- $\\gamma$: discount factor for values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV-VfCfknCVt"
   },
   "source": [
    "We first define the SARSA update rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "m8CshjuJnxP2"
   },
   "outputs": [],
   "source": [
    "class SARSA(BaseUpdate):\n",
    "    # alpha - learning rate\n",
    "    # gamma - discount factor\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'sarsa'\n",
    "    \n",
    "    # q_current - Current estimate for a particular (state,action) pair\n",
    "    # q_future - Future estimates for all actions on a particular state\n",
    "    # reward - reward\n",
    "    # next_action - specifies next action\n",
    "    def update(self, q_current, q_future, reward, next_action = None):\n",
    "        return q_current + self.alpha*(reward + self.gamma*q_future[next_action] - q_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq66-ZzBnxP2"
   },
   "source": [
    "## Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2h4yqBQnxP2"
   },
   "source": [
    "Recall the update rule for Q-Learning:\n",
    "\\begin{equation}\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma \\cdot max_a(Q(s_{t+1}, a)) - Q(s_t, a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "So we have some hyperparameters for the algorithm:\n",
    "- $\\alpha$: learning rate for the RL agent\n",
    "- $\\gamma$: discount factor for values\n",
    "\n",
    "Now we define the QLearning update rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "prFswwdNnxP2"
   },
   "outputs": [],
   "source": [
    "class QLearning(BaseUpdate):\n",
    "    # alpha - learning rate\n",
    "    # gamma - discount factor\n",
    "    def __init__(self, alpha, gamma):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    # return name of policy\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'qlrng'\n",
    "    \n",
    "    # q_current - Current estimate for a particular (state,action) pair\n",
    "    # q_future - Future estimates for all actions on a particular state\n",
    "    # reward - reward\n",
    "    # next_action - specifies next action (redundant here)\n",
    "    def update(self, q_current, q_future, reward, next_action = None):\n",
    "        q_future_max = max(q_future)\n",
    "        return q_current + self.alpha*(reward + self.gamma * q_future_max - q_current)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHGltJcEnxP2"
   },
   "source": [
    "## Iterator for Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfT1vO3p2tFB"
   },
   "source": [
    "The following iterator class forms the crux of the learning process. The class essentially has the ability to perform generalized policy iteration using some update policy and exploration policy. It also has several utility methods to help us visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rToJAJQ7nxP2"
   },
   "outputs": [],
   "source": [
    "class TrainingIterator:\n",
    "    def __init__(self, env, update_policy, exploration_policy, episodes):\n",
    "        self.env = env                         # Grid World Env\n",
    "        self.num_eps = episodes                # training episodes\n",
    "        self.update_policy = update_policy     # SARSA vs Q-Learning\n",
    "        self.explore = exploration_policy      # epsilon-greedy vs softmax\n",
    "        \n",
    "        self.steps = np.zeros(episodes)                                # total_steps for each episode\n",
    "        self.rewards = np.zeros(episodes)                              # total_rewards for each episode\n",
    "        self.q_vals = np.zeros((env.num_states, env.num_actions))      # Qvalues for each (state,action) pair\n",
    "        self.hmap_visits = np.zeros(env.num_states)                    # heatmap of state visits (during training)\n",
    "        self.hmap_qvals = np.zeros(env.num_states)                     # heatmap of Qvalues for optimal actions (after training)\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        # iterate over training episodes\n",
    "        for episode in tqdm(range(self.num_eps), desc = 'Training Episodes'):\n",
    "            \n",
    "            # find the current state and select action to take\n",
    "            current_state  = row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "            current_action = self.explore.select_action(current_state, self.q_vals)\n",
    "\n",
    "            # initialise all steps and rewards to 0\n",
    "            self.steps[episode] = 0\n",
    "            self.rewards[episode] = 0\n",
    "            self.hmap_visits[current_state] += 1\n",
    "\n",
    "            # loop until 100 steps are taken or goal state is reached\n",
    "            while((current_state not in list(row_col_to_seq(self.env.goal_states, self.env.num_cols))) and (self.steps[episode] <= 100)):\n",
    "            \n",
    "                # take a step and calculate reward and the next action\n",
    "                next_state, reward = self.env.step(current_state, current_action)\n",
    "                next_action = self.explore.select_action(next_state, self.q_vals)\n",
    "                \n",
    "                # carry out update of action values based on the step taken\n",
    "                self.q_vals[current_state, current_action] = self.update_policy.update(self.q_vals[current_state, current_action], self.q_vals[next_state], reward, next_action)\n",
    "                \n",
    "                # if new state is unique (agent doesnt stay at the same place) record state visit\n",
    "                if current_state != next_state:\n",
    "                    self.hmap_visits[next_state] += 1\n",
    "                    \n",
    "                # update the state and action\n",
    "                current_state = next_state\n",
    "                current_action = next_action\n",
    "                \n",
    "                # update rewards and steps\n",
    "                self.rewards[episode] += reward\n",
    "                self.steps[episode] += 1\n",
    "                \n",
    "            # if end state is not a goal state we set number of steps needed to infinity\n",
    "            if current_state not in list(row_col_to_seq(self.env.goal_states, self.env.num_cols)):\n",
    "                self.steps[episode] = np.inf\n",
    "\n",
    "        # find the max action values for each state\n",
    "        for state in range(self.env.num_states):\n",
    "            self.hmap_qvals[state] = max(self.q_vals[state])\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def plot_learnt_policy(self, verbose = False):\n",
    "        # set name of image\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_lnt_policy.jpg'\n",
    "\n",
    "        # initialise a greedy policy for action selection\n",
    "        greedy_policy = Greedy()\n",
    "        hmap_greedy = np.zeros((self.env.num_rows, self.env.num_cols))\n",
    "        hmap_greedy[self.env.start_state[0][0], self.env.start_state[0][1]] = 1\n",
    "\n",
    "        # find the current state and select action to take\n",
    "        current_state  = row_col_to_seq(self.env.start_state, self.env.num_cols)[0]\n",
    "        current_action = greedy_policy.select_action(current_state, self.q_vals)\n",
    "\n",
    "        # initialise all steps and rewards to 0\n",
    "        steps = 0\n",
    "        reward_greedy = 0\n",
    "\n",
    "        # loop until 100 steps are taken or goal state is reached\n",
    "        while((current_state not in list(row_col_to_seq(self.env.goal_states, self.env.num_cols))) and steps < 100):\n",
    "\n",
    "            # take a step and calculate reward and the next action\n",
    "            next_state, reward = self.env.step(current_state, current_action)\n",
    "            next_action = greedy_policy.select_action(next_state, self.q_vals)   \n",
    "            \n",
    "            # update state\n",
    "            current_state = next_state\n",
    "            current_action = next_action\n",
    "\n",
    "            # calculate reward and update steps and reward\n",
    "            reward_greedy += reward\n",
    "            hmap_greedy[seq_to_col_row(current_state, self.env.num_cols)[0][0], seq_to_col_row(current_state, self.env.num_cols)[0][1]] = 1\n",
    "            steps += 1\n",
    "\n",
    "        # plot the learnt path as a heatmap\n",
    "        if verbose:\n",
    "            plt.title(\"Learnt Policy\")\n",
    "            hmap = sns.heatmap(hmap_greedy, annot = False)\n",
    "            plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "            plt.show()\n",
    "        \n",
    "        # return the reward gathered by the learnt policy\n",
    "        return reward_greedy[0]\n",
    "    \n",
    "    \n",
    "    def plot_reward_curve(self, save = True):\n",
    "        # set name of image\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_plt_rewards.jpg'\n",
    "        \n",
    "        # plot reward curve\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        plt.title(\"Reward Curve: Avg:%0.3f Max:%0.3f\"%(np.mean(self.rewards), np.max(self.rewards)))\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Rewards\")\n",
    "        plt.plot(np.arange(self.num_eps), self.rewards, 'b')\n",
    "        \n",
    "        # save figure\n",
    "        if save: plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "        \n",
    "        plt.show()\n",
    "        return\n",
    "        \n",
    "    def plot_steps(self, save = True):\n",
    "        # set name of image\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_plt_steps.jpg'\n",
    "        \n",
    "        # plot steps to reach goal\n",
    "        sns.set_style(\"darkgrid\")\n",
    "        plt.title(\"Steps till Goal State: Min:%0.3f\"%(np.min(self.steps)))\n",
    "        plt.xlabel(\"Episodes\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.plot(np.arange(self.num_eps), self.steps, 'g')\n",
    "        \n",
    "        # save figure\n",
    "        if save: plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "        plt.show()\n",
    "        return\n",
    "        \n",
    "    def plot_hmap_visits(self, save = True):\n",
    "        # set name of image\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_hmp_visits.jpg'\n",
    "        \n",
    "        # convert to 2D numpy array\n",
    "        n_rows, n_cols = self.env.num_rows, self.env.num_cols\n",
    "        data = np.zeros((n_rows, n_cols))\n",
    "        for i in range(n_rows):\n",
    "            for j in range(n_cols):\n",
    "                data[i,j] = self.hmap_visits[row_col_to_seq(np.array([[i,j]]), n_cols)[0]]\n",
    "           \n",
    "        # plot and save heatmap\n",
    "        plt.title(\"Heatmap of State Visits\")\n",
    "        hmap = sns.heatmap(data, annot = False)\n",
    "        \n",
    "        # save figure\n",
    "        if save: plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "        \n",
    "        plt.show()\n",
    "        return\n",
    "        \n",
    "    def plot_hmap_qvals(self, save = False):\n",
    "        # set name of image\n",
    "        name = self.update_policy.name + self.env.start_mode + self.env.wind_mode + self.env.stochasticity + self.explore.name + '_hmp_qvals.jpg'\n",
    "        \n",
    "        # convert to 2D numpy array\n",
    "        n_rows, n_cols = self.env.num_rows, self.env.num_cols\n",
    "        data = np.zeros((n_rows, n_cols))\n",
    "        for i in range(n_rows):\n",
    "            for j in range(n_cols):\n",
    "                data[i,j] = self.hmap_qvals[row_col_to_seq(np.array([[i,j]]), n_cols)[0]]\n",
    "          \n",
    "        # plot and save heatmap\n",
    "        plt.title(\"Heatmap of Q-Values: Avg:%0.3f Max:%0.3f Min:%0.3f\"%(np.mean(data), np.max(data), np.min(data)))\n",
    "        hmap = sns.heatmap(data, annot = False)\n",
    "        \n",
    "        if save: plt.savefig('./plots/' + name, pad_inches = 0)\n",
    "        plt.show()\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search for Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkk7UDaUDELX"
   },
   "source": [
    "In order to carry out the hyperparameter tuning we have decided tune on the basis of their asymptotic optimality, which can be discerned by using the Q_values learnt by the agent and following a greedy action selection mechanism. \n",
    "\n",
    "Next, we define a grid search function which returns the best hyperparameter set based on asymptotic optimality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QH3D3x5HDDiB"
   },
   "outputs": [],
   "source": [
    "# alphas  : list of alpha values\n",
    "# gammas  : list of gamma values\n",
    "# epsilons: list of epsilon values when epsilon-greedy selection is used\n",
    "# betas   : list of beta values when softmax selection is used\n",
    "def asymptotic_grid_search(env, alphas, gammas, epsilons = None, betas = None, update_rule = 'sarsa'):\n",
    "\n",
    "    # initialise reward and the best hyperparam list\n",
    "    best_reward = - np.inf\n",
    "    best_hyper_params_list = []\n",
    "\n",
    "    # For softmax action selection\n",
    "    if epsilons == None:\n",
    "        # Loop over the alpha, gamma and beta values\n",
    "        for alpha in alphas:\n",
    "            for gamma in gammas:\n",
    "                for beta in betas:\n",
    "                    # print the configuration\n",
    "                    print(\"Current configuration: alpha = {}, gamma = {}, beta = {}\".format(alpha, gamma, beta))\n",
    "                    \n",
    "                    # select the update policy\n",
    "                    if update_rule == 'sarsa': update_policy = SARSA(alpha, gamma = gamma)\n",
    "                    elif update_rule == 'qlrng': update_policy = QLearning(alpha, gamma = gamma)\n",
    "        \n",
    "                    # train an RL agent\n",
    "                    trainer = TrainingIterator(env, update_policy, SoftMax(beta = beta), 10000)\n",
    "                    trainer.train()\n",
    "\n",
    "                    # find the greedy learnt policy\n",
    "                    reward = trainer.plot_learnt_policy()\n",
    "                    print(\"Total Reward:\", reward)\n",
    "\n",
    "                    # update hyperparam list based on values of reward\n",
    "                    if reward > best_reward:\n",
    "                        best_hyper_params_list = []\n",
    "                    if reward >= best_reward:\n",
    "                        best_reward = reward\n",
    "                        best_hyper_params = {}\n",
    "                        best_hyper_params['alpha'] = alpha\n",
    "                        best_hyper_params['gamma'] = gamma\n",
    "                        best_hyper_params['beta'] = beta\n",
    "                        best_hyper_params_list.append(best_hyper_params)\n",
    "\n",
    "    # for epsilon-greedy action selection\n",
    "    else:\n",
    "        # Loop over the alpha, gamma and epsilon values\n",
    "        for alpha in alphas:\n",
    "            for gamma in gammas:\n",
    "                for epsilon in epsilons:\n",
    "                    # print configuration\n",
    "                    print(\"Current configuration: alpha = {}, gamma = {}, epsilon = {}\".format(alpha, gamma, epsilon))\n",
    "                    \n",
    "                    # select the update policy\n",
    "                    if update_rule == 'sarsa': update_policy = SARSA(alpha, gamma = gamma)\n",
    "                    elif update_rule == 'qlrng': update_policy = QLearning(alpha, gamma = gamma)\n",
    "\n",
    "                    # train an RL agent\n",
    "                    trainer = TrainingIterator(env, update_policy, EpsilonGreedy(epsilon = epsilon), 10000)\n",
    "                    trainer.train()\n",
    "\n",
    "                    # find greedy learnt policy\n",
    "                    reward = trainer.plot_learnt_policy()\n",
    "                    print(\"Total Reward:\", reward)\n",
    "\n",
    "                    # update hyperparam list based on values of reward\n",
    "                    if reward > best_reward:\n",
    "                        best_hyper_params_list = []\n",
    "                    if reward >= best_reward:\n",
    "                        best_reward = reward\n",
    "                        best_hyper_params = {}\n",
    "                        best_hyper_params['alpha'] = alpha\n",
    "                        best_hyper_params['gamma'] = gamma\n",
    "                        best_hyper_params['epsilon'] = epsilon\n",
    "                        best_hyper_params_list.append(best_hyper_params)\n",
    "\n",
    "    # print best reward as well as the list of hyperparameter combinations that perform best asymptotically\n",
    "    print()\n",
    "    print(\"Best Reward\", best_reward)\n",
    "    print(\"Asymptotic Best Hyper Parameters List\", best_hyper_params_list)\n",
    "    print()\n",
    "\n",
    "    # return best reward and hparam list\n",
    "    return best_reward, best_hyper_params_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the reward obtained may be equal in the asymptotic cases for many different combinations of hyperparameters. These sets of configurations are then collected and the best out of them is taken by training **5 times** and calculating the average rewards and hence the **average regret** of each configuration with respect to the best achievable reward.\n",
    "\n",
    "This allows us to rule out extremely poor configurations and carry out the regret based estimation (which has multiple training runs within it and takes more time) only for those combinations of hyperparameters that actually show promise in learning the correct(best) path.\n",
    "\n",
    "This regret based grid search is defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry out regret based grid search of the hyperparameter space\n",
    "def regret_grid_search(env, best_reward, best_hyper_params, update_rule = 'sarsa', exploration_rule = 'epsilon'):\n",
    "\n",
    "    # initialise regret\n",
    "    best_regret = np.inf\n",
    "    best_hyper = {}\n",
    "\n",
    "    # for epsilon-greedy action selection policy\n",
    "    if exploration_rule == 'epsilon':\n",
    "        # test every hyperparam configuration selected by asymptotic grid search\n",
    "        for hyper_dict in best_hyper_params:\n",
    "            # print configuration\n",
    "            print(\"Current configuration: alpha = {}, gamma = {}, epsilon = {}\".format(hyper_dict['alpha'], hyper_dict['gamma'], hyper_dict['epsilon']))\n",
    "            \n",
    "            # select update policy\n",
    "            if update_rule == 'sarsa': update_policy = SARSA(alpha = hyper_dict['alpha'], gamma = hyper_dict['gamma'] )\n",
    "            elif update_rule == 'qlrng': update_policy = QLearning(alpha = hyper_dict['alpha'], gamma = hyper_dict['gamma'])\n",
    "\n",
    "            # carry out 5 training runs\n",
    "            rewards = []\n",
    "            for i in range(5):\n",
    "                print(\"Running Experiment:\", i+1)\n",
    "                trainer = TrainingIterator(env, update_policy, EpsilonGreedy(epsilon = hyper_dict['epsilon']), 10000)\n",
    "                trainer.train()\n",
    "                rewards.append(trainer.rewards)\n",
    "                \n",
    "            # calculate mean rewards and regret\n",
    "            rewards = np.mean(np.array(rewards), axis = 0)\n",
    "            regret = np.sum((best_reward - rewards))\n",
    "            print(\"Regret:\", regret)\n",
    "\n",
    "            # update best regret\n",
    "            if regret < best_regret:\n",
    "                best_regret = regret\n",
    "                best_hyper['alpha'] =  hyper_dict['alpha']\n",
    "                best_hyper['gamma'] =  hyper_dict['gamma']\n",
    "                best_hyper['epsilon'] =  hyper_dict['epsilon']\n",
    "\n",
    "    # for softmax action selection policy\n",
    "    else:\n",
    "        # test every hyperparam configuration selected by asymptotic grid search\n",
    "        for hyper_dict in best_hyper_params:\n",
    "            # print configuration\n",
    "            print(\"Current configuration: alpha = {}, gamma = {}, beta = {}\".format(hyper_dict['alpha'], hyper_dict['gamma'], hyper_dict['beta']))\n",
    "            \n",
    "            # select update policy\n",
    "            if update_rule == 'sarsa': update_policy = SARSA(alpha = hyper_dict['alpha'], gamma = hyper_dict['gamma'] )\n",
    "            elif update_rule == 'qlrng': update_policy = QLearning(alpha = hyper_dict['alpha'], gamma = hyper_dict['gamma'])\n",
    "            \n",
    "            # carry out 5 training runs\n",
    "            rewards = []\n",
    "            for i in range(5):\n",
    "                trainer = TrainingIterator(env, update_policy, SoftMax(beta = hyper_dict['beta']), 10000)\n",
    "                trainer.train()\n",
    "                rewards.append(trainer.rewards)\n",
    "               \n",
    "            # calculate mean rewards and regret\n",
    "            rewards = np.mean(np.array(rewards), axis = 0)\n",
    "            regret = np.sum((best_reward - rewards))\n",
    "            print(\"Regret:\", regret)\n",
    "\n",
    "            # update best regret\n",
    "            if regret < best_regret:\n",
    "                best_regret = regret\n",
    "                best_hyper['alpha'] =  hyper_dict['alpha']\n",
    "                best_hyper['gamma'] =  hyper_dict['gamma']\n",
    "                best_hyper['beta'] =  hyper_dict['beta']\n",
    "\n",
    "    # report the best regret and the combination of hyperparameters\n",
    "    print()\n",
    "    print(\"Best Regret:\", best_regret)\n",
    "    print(\"Best Hyperparameters:\", best_hyper)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    # return regret and hparams\n",
    "    return best_regret, best_hyper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLearning - Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry out the Grid Search first with very rough granularity to observe the behaviour of the different configurations of the environment for the various combinations of hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Experiment 1\n",
    "\n",
    "The initial search is conducted on the following grid of values:\n",
    "\n",
    "- $\\alpha$ : 0.001 , 0.01 ,  0.1\n",
    "- $\\gamma$ : 0.8  , 0.9 ,  1.0\n",
    "- $\\beta$ : 0.5, 1.0 , 5.0     (for softmax action selection cases)\n",
    "- $\\epsilon$ : 0.001 , 0.01 ,  0.1  (for $\\epsilon$-greedy action selection cases)\n",
    "\n",
    "There are a total of 27 hyperparameter configurations that are being tested here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./logs/qlrng_log.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [False, True]\n",
    "    transition_prob = [1.0]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['epsilon','softmax']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print()\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.001, 0.01, 0.1]\n",
    "                    gammas = [0.8, 0.9, 1.0]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001, 0.01, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'qlrng')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'qlrng', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['epsilon']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [0.5, 1.0, 5.0]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'qlrng')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'qlrng', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Expt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the plots that the learning is not very successful for the noisy cases. Hence in order to figure out which are better combinations of hyperparameters, we have decided to fine tune them with more intensity.\n",
    "\n",
    "#### For $\\epsilon$-greedy\n",
    "- $\\alpha$ : 0.001 , 0.005, 0.01 , 0.03, 0.05, 0.07, 0.1\n",
    "- $\\gamma$ : 0.9, 1.0\n",
    "- $\\epsilon$ : 0.001 , 0.005, 0.01 , 0.05, 0.1  (for $\\epsilon$-greedy action selection cases)\n",
    "\n",
    "There are a total of 70 hyperparameter configurations being tried out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine tuning for noisy case\n",
    "with open('./logs/qlrng_log1.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [False, True]\n",
    "    transition_prob = [0.7]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['epsilon']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print()\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.001, 0.005, 0.01, 0.03, 0.05, 0.07, 0.1]\n",
    "                    gammas = [0.9, 1.0]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'qlrng')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'qlrng', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['epsilon']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [0.5, 1.0, 5.0]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'qlrng')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'qlrng', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Expt 3\n",
    "\n",
    "It can be seen from the plots that the learning is not very successful for the noisy cases in case of softmax. Hence in order to figure out which are better combinations of hyperparameters, we have decided to fine tune them with more intensity.\n",
    "\n",
    "#### For softmax\n",
    "- $\\alpha$ : 0.001 , 0.005, 0.01 , 0.03, 0.05, 0.07, 0.1\n",
    "- $\\gamma$ : 0.9, 1.0\n",
    "- $\\beta$ : 0.01 , 0.05, 0.10, 0.50, 1.0, 5.0 (for softmax action selection cases)\n",
    "\n",
    "There are a total of 84 hyperparameter configurations being tried out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine tuning for noisy case\n",
    "with open('./logs/qlrng_log2.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [False, True]\n",
    "    transition_prob = [0.7]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['softmax']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print()\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.001, 0.005, 0.01, 0.03, 0.05, 0.07, 0.1]\n",
    "                    gammas = [0.9, 1.0]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'qlrng')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'qlrng', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['epsilon']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [0.01, 0.05, 0.10, 0.50, 1.0, 5.0]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'qlrng')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'qlrng', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Expt 4\n",
    "\n",
    "It can be seen from the plots that the learning is not very successful for the windy cases in case of softmax. Hence in order to figure out which are better combinations of hyperparameters, we have decided to fine tune them with more intensity.\n",
    "\n",
    "#### For softmax\n",
    "- $\\alpha$ : 0.01, 0.1\n",
    "- $\\gamma$ : 0.9, 1.0\n",
    "- $\\beta$ : 0.005, 0.01 , 0.05, 0.10(for softmax action selection cases)\n",
    "\n",
    "There are a total of 16 hyperparameter configurations being tried out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./logs/qlrng_log3.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [True]\n",
    "    transition_prob = [1.0, 0.7]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['softmax']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print('-------------------------------------------------------------------------------------------------------')\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.01, 0.1]\n",
    "                    gammas = [0.9, 1.0]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001 , 0.005, 0.01 , 0.05, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'sarsa', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['epsilon']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [0.005, 0.01 , 0.05, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'sarsa', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, QLearning(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA - Grid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We carry out the Grid Search first with very rough granularity to observe the behaviour of the different configurations of the environment for the various combinations of hyperparameters. \n",
    "\n",
    "### Running Expt 1\n",
    "\n",
    "The initial search is conducted on the following grid of values:\n",
    "\n",
    "- $\\alpha$ : 0.001 , 0.01 ,  0.1\n",
    "- $\\gamma$ : 0.8  , 0.9 ,  1.0\n",
    "- $\\beta$ : 0.5   , 1.0  , 5.0     (for softmax action selection cases)\n",
    "- $\\epsilon$ : 0.001 , 0.01 ,  0.1  (for $\\epsilon$-greedy action selection cases)\n",
    "\n",
    "There are a total of 27 hyperparameter configurations that are being tested here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./logs/sarsa_log.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [False, True]\n",
    "    transition_prob = [1.0, 0.7]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['epsilon','softmax']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print('-------------------------------------------------------------------------------------------------------')\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.001, 0.01, 0.1]\n",
    "                    gammas = [0.8, 0.9, 1.0]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001, 0.01, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'sarsa', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['epsilon']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [0.5, 1.0, 5.0]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'sarsa', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Expt 2\n",
    "\n",
    "It can be seen from the plots that the learning is not very successful for the noisy cases. Hence in order to figure out which are better combinations of hyperparameters, we have decided to fine tune them with more intensity.\n",
    "\n",
    "#### For $\\epsilon$-greedy\n",
    "- $\\alpha$ : 0.001 , 0.005, 0.01 , 0.03, 0.05, 0.07, 0.1\n",
    "- $\\gamma$ : 0.9, 1.0\n",
    "- $\\epsilon$ : 0.001 , 0.005, 0.01 , 0.05, 0.1  (for $\\epsilon$-greedy action selection cases)\n",
    "\n",
    "There are a total of 70 hyperparameter configurations being tried out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./logs/sarsa_log1.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [False, True]\n",
    "    transition_prob = [0.7]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['epsilon']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print('-------------------------------------------------------------------------------------------------------')\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.001 , 0.005, 0.01 , 0.03, 0.05, 0.07, 0.1]\n",
    "                    gammas = [0.9, 1.0]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001 , 0.005, 0.01 , 0.05, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'sarsa', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['epsilon']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [0.5, 1.0, 5.0]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'sarsa', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Expt 3\n",
    "\n",
    "It can be seen from the plots that the learning is not very successful for the noisy cases in case of softmax. Hence in order to figure out which are better combinations of hyperparameters, we have decided to fine tune them with more intensity.\n",
    "\n",
    "#### For softmax\n",
    "- $\\alpha$ : 0.001 , 0.005, 0.01 , 0.03, 0.05, 0.07, 0.1\n",
    "- $\\gamma$ : 0.9, 1.0\n",
    "- $\\beta$ : 0.01 , 0.05, 0.10, 0.50, 1.0, 5.0 (for softmax action selection cases)\n",
    "\n",
    "There are a total of 84 hyperparameter configurations being tried out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./logs/sarsa_log2.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [False, True]\n",
    "    transition_prob = [0.7]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['softmax']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print('-------------------------------------------------------------------------------------------------------')\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.001 , 0.005, 0.01 , 0.03, 0.05, 0.07, 0.1]\n",
    "                    gammas = [0.9, 1.0]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001 , 0.005, 0.01 , 0.05, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'sarsa', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['epsilon']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [0.01 , 0.05, 0.10, 0.50, 1.0, 5.0]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'sarsa', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Expt 4\n",
    "\n",
    "It can be seen from the plots that the learning is not very successful for the windy cases in case of softmax. Hence in order to figure out which are better combinations of hyperparameters, we have decided to fine tune them with more intensity.\n",
    "\n",
    "#### For softmax\n",
    "- $\\alpha$ : 0.001 , 0.01, 0.1\n",
    "- $\\gamma$ : 0.9, 1.0\n",
    "- $\\beta$ : 0.005, 0.01 , 0.05, 0.10(for softmax action selection cases)\n",
    "\n",
    "There are a total of 16 hyperparameter configurations being tried out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('./logs/sarsa_log3.txt', 'w') as f:\n",
    "\n",
    "    sys.stdout = f \n",
    "\n",
    "    wind_states = [True]\n",
    "    transition_prob = [1.0, 0.7]\n",
    "    start_states = [[0, 4], [3, 6]]\n",
    "    epsilon_soft = ['softmax']\n",
    "\n",
    "    \n",
    "\n",
    "    for start in start_states:\n",
    "        for wind_state in wind_states:\n",
    "            for prob in transition_prob:\n",
    "                for exp in epsilon_soft:\n",
    "\n",
    "                    print(\"Running Experiment with parameters: Start State: {}, Wind: {}, Transition Probability: {}, Exploration Policy: {}\".format(start, wind_state, prob, exp))\n",
    "                    print('-------------------------------------------------------------------------------------------------------')\n",
    "                    print()\n",
    "                    num_cols = 10\n",
    "                    num_rows = 10\n",
    "                    obstructions = np.array([[0,7],[1,1],[1,2],[1,3],[1,7],[2,1],[2,3],\n",
    "                                            [2,7],[3,1],[3,3],[3,5],[4,3],[4,5],[4,7],\n",
    "                                            [5,3],[5,7],[5,9],[6,3],[6,9],[7,1],[7,6],\n",
    "                                            [7,7],[7,8],[7,9],[8,1],[8,5],[8,6],[9,1]])\n",
    "\n",
    "                    bad_states = np.array([[1,9],[4,2],[4,4],[7,5],[9,9]])\n",
    "                    restart_states = np.array([[3,7],[8,2]])\n",
    "                    goal_states = np.array([[0,9],[2,2],[8,7]])\n",
    "\n",
    "                    start_state = np.array([start])\n",
    "\n",
    "                    # create model\n",
    "                    gw = GridWorld(num_rows=num_rows,\n",
    "                                    num_cols=num_cols,\n",
    "                                    start_state=start_state,\n",
    "                                    goal_states=goal_states, wind = wind_state)\n",
    "\n",
    "                    gw.add_obstructions(obstructed_states=obstructions,\n",
    "                                        bad_states=bad_states,\n",
    "                                        restart_states=restart_states)\n",
    "\n",
    "                    gw.add_rewards(step_reward=-1,\n",
    "                                    goal_reward=10,\n",
    "                                    bad_state_reward=-6,\n",
    "                                    restart_state_reward=-100)\n",
    "\n",
    "                    gw.add_transition_probability(p_good_transition=prob, bias=0.5)\n",
    "\n",
    "                    env = gw.create_gridworld()\n",
    "\n",
    "                    alphas = [0.001 , 0.01, 0.1]\n",
    "                    gammas = [0.9, 1.0]\n",
    "\n",
    "                    if exp == 'epsilon':\n",
    "                        epsilons = [0.001 , 0.005, 0.01 , 0.05, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, epsilons = epsilons, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list,  update_rule = 'sarsa', exploration_rule = 'epsilon' )\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), EpsilonGreedy(epsilon = best_hyper_params['epsilon']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        betas = [0.005, 0.01 , 0.05, 0.1]\n",
    "                        print(\"Starting Asymptotic Grid Search:\")\n",
    "                        print()\n",
    "                        best_reward, best_hyper_list = asymptotic_grid_search(env, alphas = alphas, gammas = gammas, betas = betas, update_rule = 'sarsa')\n",
    "                        print(\"Starting Regret Grid Search:\")\n",
    "                        print()\n",
    "                        best_regret, best_hyper_params = regret_grid_search(env, best_reward, best_hyper_list, update_rule = 'sarsa', exploration_rule = 'softmax')\n",
    "\n",
    "                        print(\"Creating Required Plots...\")\n",
    "                        print()\n",
    "                        print()\n",
    "                        trainer = TrainingIterator(env, SARSA(alpha = best_hyper_params['alpha'], gamma = best_hyper_params['gamma']), SoftMax(beta = best_hyper_params['beta']), 10000)\n",
    "                        trainer.train()\n",
    "                        trainer.plot_reward_curve()\n",
    "                        trainer.plot_steps()\n",
    "                        trainer.plot_hmap_visits()\n",
    "                        trainer.plot_hmap_qvals()\n",
    "                        trainer.plot_learnt_policy(verbose = True )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ced69849f870846bba8401de8ea27d999c6a9be7d5e22b7f0d1cc9e892f569ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
